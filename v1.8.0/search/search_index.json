{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to EITprocessing","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the documentation of the ALIVE software tool designed to load, analyze, and extract parameters from Electrical Impedance Tomography (EIT) data. This software was designed by a joined effort of the Rotterdam Advanced Respiratory Care research group (ROTARC) of the Intensive Care of the Erasmus Medical Center and the Netherlands e-Science center. Grant ID: NLESC.OEC.2022.002</p> <p>EIT is a bedside non-invasive lung imaging tool: it continuously and real-time visualizes changes in lung volume. Our software tool serves as a comprehensive solution for handling EIT data from multiple leading manufacturers, including Sentec, Dr\u00e4ger, and Timpel.</p> <p>The software tool includes a back-end for researchers that are familair with programming eitprocessing and also a user-friendly dashboard eit_dash for clinical researchers allowing to quickly import datasets from various formats and sources and perform processing and analysis. This documentation page concerns eitprocessing.</p> <p>Our tool offers robust analysis features. From basic filters to advanced signal processing techniques, you can extract meaningful parameters from your EIT data. With our dashboard we aim to provide default analysis pipelines and many opportunities for customization according to the user needs. Visualizations and interactive graphs make it easy to interpret the results and understand the underlying physiological processes.</p> <p>It is important to note that the software tool is a work in progress, so not all fuctionalities are available yet. If you would like to contribute to coding you can reach out to us.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To start using our software you can you use the installation guide to set up the software on your system. Once installed, you can load your first dataset and explore the basic features. We are committed to supporting your journey with EIT data analysis and extraction. If you encounter any issues or have questions you can put a pull request via github or emailadres.</p>"},{"location":"basic_example/","title":"Basic example","text":"<p>Under construction</p>"},{"location":"code_of_conduct_doc/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct_doc/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct_doc/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct_doc/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct_doc/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct_doc/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at d.bodor@esciencecenter.nl. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct_doc/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"contributing_doc/","title":"Contributing guidelines","text":"<p>We welcome any kind of contribution to our software, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation);</li> <li>you want to make a new release of the code base.</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"contributing_doc/#you-have-a-question","title":"You have a question","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"contributing_doc/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:</li> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"contributing_doc/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<p>We welcome all contributions to this open-source project, as long as they follow our code of conduct.</p> <p>Please read out developers documentation if you are interested in contributing to the code base</p>"},{"location":"installation/","title":"Installation","text":"<p>It is advised to install eitprocessing in a dedicated virtual environment. See e.g. Install packages in a virtual environment using pip and venv or Getting started with conda.</p>"},{"location":"installation/#install-from-pypi","title":"Install from PyPi","text":"<p>eitprocessing can be installed from PyPi as follows:</p> <pre><code>pip install eitprocessing\n</code></pre>"},{"location":"installation/#developer-install","title":"Developer install","text":"<p>For full developer options (testing, etc):</p> <pre><code>git clone git@github.com:EIT-ALIVE/eitprocessing.git\ncd eitprocessing\npip install -e \".[dev]\"\n</code></pre>"},{"location":"license_doc/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/</p> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li>Definitions.</li> </ol> <p>\"License\" shall mean the terms and conditions for use, reproduction,    and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by    the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all    other entities that control, are controlled by, or are under common    control with that entity. For the purposes of this definition,    \"control\" means (i) the power, direct or indirect, to cause the    direction or management of such entity, whether by contract or    otherwise, or (ii) ownership of fifty percent (50%) or more of the    outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity    exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,    including but not limited to software source code, documentation    source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical    transformation or translation of a Source form, including but    not limited to compiled object code, generated documentation,    and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or    Object form, made available under the License, as indicated by a    copyright notice that is included in or attached to the work    (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object    form, that is based on (or derived from) the Work and for which the    editorial revisions, annotations, elaborations, or other modifications    represent, as a whole, an original work of authorship. For the purposes    of this License, Derivative Works shall not include works that remain    separable from, or merely link (or bind by name) to the interfaces of,    the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including    the original version of the Work and any modifications or additions    to that Work or Derivative Works thereof, that is intentionally    submitted to Licensor for inclusion in the Work by the copyright owner    or by an individual or Legal Entity authorized to submit on behalf of    the copyright owner. For the purposes of this definition, \"submitted\"    means any form of electronic, verbal, or written communication sent    to the Licensor or its representatives, including but not limited to    communication on electronic mailing lists, source code control systems,    and issue tracking systems that are managed by, or on behalf of, the    Licensor for the purpose of discussing and improving the Work, but    excluding communication that is conspicuously marked or otherwise    designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity    on behalf of whom a Contribution has been received by Licensor and    subsequently incorporated within the Work.</p> <ol> <li> <p>Grant of Copyright License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    copyright license to reproduce, prepare Derivative Works of,    publicly display, publicly perform, sublicense, and distribute the    Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    (except as stated in this section) patent license to make, have made,    use, offer to sell, sell, import, and otherwise transfer the Work,    where such license applies only to those patent claims licensable    by such Contributor that are necessarily infringed by their    Contribution(s) alone or by combination of their Contribution(s)    with the Work to which such Contribution(s) was submitted. If You    institute patent litigation against any entity (including a    cross-claim or counterclaim in a lawsuit) alleging that the Work    or a Contribution incorporated within the Work constitutes direct    or contributory patent infringement, then any patent licenses    granted to You under this License for that Work shall terminate    as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the    Work or Derivative Works thereof in any medium, with or without    modifications, and in Source or Object form, provided that You    meet the following conditions:</p> </li> </ol> <p>(a) You must give any other recipients of the Work or          Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices          stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works          that You distribute, all copyright, patent, trademark, and          attribution notices from the Source form of the Work,          excluding those notices that do not pertain to any part of          the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its          distribution, then any Derivative Works that You distribute must          include a readable copy of the attribution notices contained          within such NOTICE file, excluding those notices that do not          pertain to any part of the Derivative Works, in at least one          of the following places: within a NOTICE text file distributed          as part of the Derivative Works; within the Source form or          documentation, if provided along with the Derivative Works; or,          within a display generated by the Derivative Works, if and          wherever such third-party notices normally appear. The contents          of the NOTICE file are for informational purposes only and          do not modify the License. You may add Your own attribution          notices within Derivative Works that You distribute, alongside          or as an addendum to the NOTICE text from the Work, provided          that such additional attribution notices cannot be construed          as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and    may provide additional or different license terms and conditions    for use, reproduction, or distribution of Your modifications, or    for any such Derivative Works as a whole, provided Your use,    reproduction, and distribution of the Work otherwise complies with    the conditions stated in this License.</p> <ol> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,    any Contribution intentionally submitted for inclusion in the Work    by You to the Licensor shall be under the terms and conditions of    this License, without any additional terms or conditions.    Notwithstanding the above, nothing herein shall supersede or modify    the terms of any separate license agreement you may have executed    with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade    names, trademarks, service marks, or product names of the Licensor,    except as required for reasonable and customary use in describing the    origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or    agreed to in writing, Licensor provides the Work (and each    Contributor provides its Contributions) on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied, including, without limitation, any warranties or conditions    of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A    PARTICULAR PURPOSE. You are solely responsible for determining the    appropriateness of using or redistributing the Work and assume any    risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,    whether in tort (including negligence), contract, or otherwise,    unless required by applicable law (such as deliberate and grossly    negligent acts) or agreed to in writing, shall any Contributor be    liable to You for damages, including any direct, indirect, special,    incidental, or consequential damages of any character arising as a    result of this License or out of the use or inability to use the    Work (including but not limited to damages for loss of goodwill,    work stoppage, computer failure or malfunction, or any and all    other commercial damages or losses), even if such Contributor    has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing    the Work or Derivative Works thereof, You may choose to offer,    and charge a fee for, acceptance of support, warranty, indemnity,    or other liability obligations and/or rights consistent with this    License. However, in accepting such obligations, You may act only    on Your own behalf and on Your sole responsibility, not on behalf    of any other Contributor, and only if You agree to indemnify,    defend, and hold each Contributor harmless for any liability    incurred by, or claims asserted against, such Contributor by reason    of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!)  The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright [yyy][name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"team/","title":"Our Team","text":""},{"location":"team/#escience-center","title":"eScience Center","text":"<ul> <li>Dani Bodor, Lead RSE</li> <li>Walter Baccinelli, RSE</li> <li>Pablo Lopez-Tarifa, Programme Manager</li> </ul>"},{"location":"team/#erasmus-medical-center-rotterdam","title":"Erasmus Medical Center Rotterdam","text":"<ul> <li>Annemijn H. Jonkman, Lead Applicant</li> <li>Peter Somhorst</li> <li>Juliette Francovich</li> <li>Jantine Wisse</li> </ul>"},{"location":"api/categories/","title":"Categories","text":""},{"location":"api/categories/#eitprocessing.categories.Category","title":"eitprocessing.categories.Category","text":"<pre><code>Category(name: str, parent: Self | None = None)\n</code></pre> <p>Data category indicating what type of information is saved in an object.</p> <p>Categories are nested, where more specific categories are nested inside more general categories. The root category is simply named 'category'. Categories have a unique name within the entire tree.</p> <p>To check the existence of a category with name  within a category tree, either as subcategory or subsub(...)category, <code>category.has_subcategory(\"&lt;name&gt;\")</code> can be used. The keyword <code>in</code> can be used as a shorthand. <p>Example: <pre><code>&gt;&gt;&gt; \"tea\" in category  # is the same as:\nTrue\n&gt;&gt;&gt; category.has_subcategory(\"tea\")\nTrue\n</code></pre></p> <p>To select a subcategory, <code>category[\"&lt;name&gt;\"]</code> can be used. You can select multiple categories at once. This will create a new tree with a temporary root, containing only the selected categories.</p> <p>Example: <pre><code>&gt;&gt;&gt; foobar = categories[\"foo\", \"bar\"]\n&gt;&gt;&gt; print(foobar)\nCategory('/temporary root')\n&gt;&gt;&gt; print(foobar.children)\n(Category('/temporary root/foo'), Category('/temporary root/bar'))\n</code></pre></p> <p>Categories can be hand-crafted, created from a dictionary or a YAML string. See <code>anytree.DictionaryImporter</code> documentation for more info on the dictionary format. anytree documentation on YAML import/export shows the relevant structure of a normal YAML string.</p> <p>Categories also supports a compact YAML format, where each category containing a subcategory is a sequence. Categories without subcategories are strings in those sequences.</p> <pre><code>root:\n- sub 1 (without subcategories)\n- sub 2 (with subcategories):\n  - sub a (without subcategories)\n</code></pre> <p>Categories are read-only by default, as they should not be edited by the end-user during runtime. Consider editing the config file instead.</p> <p>Each type of data that is attached to an eitprocessing object should be categorized as one of the available types of data. This allows algorithms to check whether it can apply itself to the provided data, preventing misuse of algorithms.</p> <p>Example: <pre><code>&gt;&gt;&gt; categories = get_default_categories()\n&gt;&gt;&gt; print(categories)\nCategory('/category')\n&gt;&gt;&gt; print(\"pressure\" in categories)\nTrue\n&gt;&gt;&gt; categories[\"pressure\"]\nCategory('/category/physical measurement/pressure')\n</code></pre></p>"},{"location":"api/categories/#eitprocessing.categories.Category.has_subcategory","title":"has_subcategory","text":"<pre><code>has_subcategory(subcategory: str) -&gt; bool\n</code></pre> <p>Check whether this category contains a subcategory.</p> <p>Returns True if the category and subcategory both exist. Returns False if the category exists, but the subcategory does not. Raises a ValueError</p> Attr <p>category: the category to be checked as an ancestor of the subcategory. This category should exist. subcategory: the subcategory to be checked as a descendent of the category.</p> RETURNS DESCRIPTION <code>bool</code> <p>whether subcategory exists as a descendent of category.</p> <p> TYPE: <code>bool</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if category does not exist.</p>"},{"location":"api/categories/#eitprocessing.categories.Category.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(string: str) -&gt; Self\n</code></pre> <p>Load categories from YAML file.</p>"},{"location":"api/categories/#eitprocessing.categories.Category.from_compact_yaml","title":"from_compact_yaml  <code>classmethod</code>","text":"<pre><code>from_compact_yaml(string: str) -&gt; Self\n</code></pre> <p>Load categories from compact YAML file.</p>"},{"location":"api/categories/#eitprocessing.categories.Category.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(dictionary: dict) -&gt; Self\n</code></pre> <p>Create categories from dictionary.</p>"},{"location":"api/loading/","title":"Loading functions","text":""},{"location":"api/loading/#eitprocessing.datahandling.loading.load_eit_data","title":"eitprocessing.datahandling.loading.load_eit_data","text":"<pre><code>load_eit_data(\n    path: str | Path | list[str | Path],\n    vendor: Vendor | str,\n    label: str | None = None,\n    name: str | None = None,\n    description: str = \"\",\n    sample_frequency: float | None = None,\n    first_frame: int = 0,\n    max_frames: int | None = None,\n) -&gt; Sequence\n</code></pre> <p>Load EIT data from path(s).</p> PARAMETER DESCRIPTION <code>path</code> <p>relative or absolute path(s) to data file.</p> <p> TYPE: <code>str | Path | list[str | Path]</code> </p> <code>vendor</code> <p>vendor indicating the device used. Note: for load functions of specific vendors (e.g. <code>load_draeger_data</code>), this argument is defaulted to the correct vendor.</p> <p> TYPE: <code>Vendor | str</code> </p> <code>label</code> <p>short description of sequence for computer interpretation. Defaults to \"Sequence_\". <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>short description of sequence for human interpretation. Defaults to the same value as label.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>long description of sequence for human interpretation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>sample_frequency</code> <p>sample frequency at which the data was recorded. No default for Draeger. Will be autodetected. Warns if autodetected differs from provided. Default for Timpel: 50 Default for Sentec: 50.2</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>first_frame</code> <p>index of first frame to load. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>max_frames</code> <p>maximum number of frames to load. The actual number of frames can be lower than this if this would surpass the final frame.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>is raised when there is no loading method for</p> RETURNS DESCRIPTION <code>Sequence</code> <p>a Sequence with the given label, name and description, containing the loaded data.</p> <p> TYPE: <code>Sequence</code> </p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(\n...     [\"path/to/file1\", \"path/to/file2\"],\n...     vendor=\"sentec\",\n...     label=\"initial_measurement\"\n... )\n&gt;&gt;&gt; pixel_impedance = sequence.eit_data[\"raw\"].pixel_impedance\n</code></pre></p>"},{"location":"api/mixins/","title":"Mixins","text":""},{"location":"api/mixins/#eitprocessing.datahandling.mixins.equality.Equivalence","title":"eitprocessing.datahandling.mixins.equality.Equivalence","text":"<p>Mixin class that adds an equality and equivalence check.</p>"},{"location":"api/mixins/#eitprocessing.datahandling.mixins.equality.Equivalence.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> PARAMETER DESCRIPTION <code>other</code> <p>object that will be compared to self.</p> <p> TYPE: <code>Self</code> </p> <code>raise_</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> RETURNS DESCRIPTION <code>bool</code> <p>bool describing result of equivalence comparison.</p>"},{"location":"api/mixins/#eitprocessing.datahandling.mixins.slicing.SelectByIndex","title":"eitprocessing.datahandling.mixins.slicing.SelectByIndex","text":"<p>Adds slicing functionality to subclass by implementing <code>__getitem__</code>.</p> <p>Subclasses must implement a <code>_sliced_copy</code> function that defines what should happen when the object is sliced. This class ensures that when calling a slice between square brackets (as e.g. done for lists) then return the expected sliced object.</p>"},{"location":"api/mixins/#eitprocessing.datahandling.mixins.slicing.SelectByIndex.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p>"},{"location":"api/mixins/#eitprocessing.datahandling.mixins.slicing.SelectByTime","title":"eitprocessing.datahandling.mixins.slicing.SelectByTime","text":"<p>Adds methods for slicing by time rather than index.</p>"},{"location":"api/mixins/#eitprocessing.datahandling.mixins.slicing.SelectByTime.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/mixins/#eitprocessing.datahandling.mixins.slicing.SelectByTime.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>first time point to include. Defaults to first frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>last time point. Defaults to last frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>start_inclusive</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <p> TYPE: <code>default</code> DEFAULT: <code>False</code> </p> <code>label</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> RETURNS DESCRIPTION <code>Self</code> <p>A shortened copy of the object.</p>"},{"location":"api/mixins/#eitprocessing.datahandling.mixins.slicing.TimeIndexer","title":"eitprocessing.datahandling.mixins.slicing.TimeIndexer  <code>dataclass</code>","text":"<pre><code>TimeIndexer(obj: T)\n</code></pre> <p>Helper class for slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/parameters/","title":"Parameters","text":""},{"location":"api/parameters/#eitprocessing.parameters.eeli.EELI","title":"eitprocessing.parameters.eeli.EELI  <code>dataclass</code>","text":"<pre><code>EELI(\n    breath_detection: BreathDetection = _sentinel_breath_detection(),\n    method: Literal[\"breath_detection\"] = \"breath_detection\",\n    breath_detection_kwargs: InitVar[dict | None] = None,\n)\n</code></pre> <p>Compute the end-expiratory lung impedance (EELI) per breath.</p>"},{"location":"api/parameters/#eitprocessing.parameters.eeli.EELI.compute_parameter","title":"compute_parameter","text":"<pre><code>compute_parameter(\n    continuous_data: ContinuousData,\n    sequence: Sequence | None = None,\n    store: bool | None = None,\n    result_label: str = \"continuous_eelis\",\n) -&gt; SparseData\n</code></pre> <p>Compute the EELI for each breath in the impedance data.</p> <p>Example: <pre><code>&gt;&gt;&gt; global_impedance = sequence.continuous_data[\"global_impedance_(raw)\"]\n&gt;&gt;&gt; eeli_data = EELI().compute_parameter(global_impedance)\n</code></pre></p> PARAMETER DESCRIPTION <code>continuous_data</code> <p>a ContinuousData object containing impedance data.</p> <p> TYPE: <code>ContinuousData</code> </p> <code>sequence</code> <p>optional, Sequence to store the result in.</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> <code>store</code> <p>whether to store the result in the sequence, defaults to <code>True</code> if a Sequence if provided.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>result_label</code> <p>label of the returned SparseData object, defaults to <code>'continuous_eelis'</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'continuous_eelis'</code> </p> RETURNS DESCRIPTION <code>SparseData</code> <p>A SparseData object with the end-expiratory values of all breaths in the impedance data.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If store is set to true but no sequence is provided.</p> <code>ValueError</code> <p>If the provided sequence is not an instance of the Sequence dataclass.</p> <code>ValueError</code> <p>If tiv_method is not one of 'inspiratory', 'expiratory', or 'mean'.</p>"},{"location":"api/parameters/#eitprocessing.parameters.tidal_impedance_variation.TIV","title":"eitprocessing.parameters.tidal_impedance_variation.TIV  <code>dataclass</code>","text":"<pre><code>TIV(\n    method: Literal[\"extremes\"] = \"extremes\",\n    breath_detection: BreathDetection = _sentinel_breath_detection(),\n    breath_detection_kwargs: InitVar[dict | None] = None,\n    pixel_breath: PixelBreath = _sentinel_pixel_breath(),\n)\n</code></pre> <p>Compute the tidal impedance variation (TIV) per breath.</p>"},{"location":"api/parameters/#eitprocessing.parameters.tidal_impedance_variation.TIV.compute_parameter","title":"compute_parameter","text":"<pre><code>compute_parameter(data: ContinuousData | EITData) -&gt; NoReturn\n</code></pre> <p>Compute the tidal impedance variation per breath on either ContinuousData or EITData, depending on the input.</p> PARAMETER DESCRIPTION <code>data</code> <p>either continuous_data or eit_data to compute TIV on.</p> <p> TYPE: <code>ContinuousData | EITData</code> </p>"},{"location":"api/parameters/#eitprocessing.parameters.tidal_impedance_variation.TIV.compute_continuous_parameter","title":"compute_continuous_parameter","text":"<pre><code>compute_continuous_parameter(\n    continuous_data: ContinuousData,\n    tiv_method: Literal[\"inspiratory\", \"expiratory\", \"mean\"] = \"inspiratory\",\n    sequence: Sequence | None = None,\n    store: bool | None = None,\n    result_label: str = \"continuous_tivs\",\n) -&gt; SparseData\n</code></pre> <p>Compute the tidal impedance variation per breath.</p> PARAMETER DESCRIPTION <code>continuous_data</code> <p>The ContinuousData to compute the TIV on.</p> <p> TYPE: <code>ContinuousData</code> </p> <code>tiv_method</code> <p>The label of which part of the breath the TIV should be determined on (inspiratory, expiratory, or mean). Defaults to 'inspiratory'.</p> <p> TYPE: <code>Literal['inspiratory', 'expiratory', 'mean']</code> DEFAULT: <code>'inspiratory'</code> </p> <code>sequence</code> <p>optional, Sequence that contains the object to detect TIV on,</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> <code>store</code> <p>whether to store the result in the sequence, defaults to <code>True</code> if a Sequence if provided.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>result_label</code> <p>label of the returned SparseData object, defaults to <code>'continuous_tivs'</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'continuous_tivs'</code> </p> RETURNS DESCRIPTION <code>SparseData</code> <p>A SparseData object with the computed TIV values.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If store is set to true but no sequence is provided.</p> <code>ValueError</code> <p>If the provided sequence is not an instance of the Sequence dataclass.</p> <code>ValueError</code> <p>If tiv_method is not one of 'inspiratory', 'expiratory', or 'mean'.</p>"},{"location":"api/parameters/#eitprocessing.parameters.tidal_impedance_variation.TIV.compute_pixel_parameter","title":"compute_pixel_parameter","text":"<pre><code>compute_pixel_parameter(\n    eit_data: EITData,\n    continuous_data: ContinuousData,\n    sequence: Sequence,\n    tiv_method: Literal[\"inspiratory\", \"expiratory\", \"mean\"] = \"inspiratory\",\n    tiv_timing: Literal[\"pixel\", \"continuous\"] = \"pixel\",\n    store: bool | None = None,\n    result_label: str = \"pixel_tivs\",\n) -&gt; SparseData\n</code></pre> <p>Compute the tidal impedance variation per breath on pixel level.</p> PARAMETER DESCRIPTION <code>sequence</code> <p>The sequence containing the data.</p> <p> TYPE: <code>Sequence</code> </p> <code>eit_data</code> <p>The eit pixel level data to determine the TIV of.</p> <p> TYPE: <code>EITData</code> </p> <code>continuous_data</code> <p>The continuous data to determine the continuous data breaths or pixel level breaths.</p> <p> TYPE: <code>ContinuousData</code> </p> <code>tiv_method</code> <p>The label of which part of the breath the TIV should be determined on         (inspiratory, expiratory or mean). Defaults to 'inspiratory'.</p> <p> TYPE: <code>Literal['inspiratory', 'expiratory', 'mean']</code> DEFAULT: <code>'inspiratory'</code> </p> <code>tiv_timing</code> <p>The label of which timing should be used to compute the TIV, either based on breaths         detected in continuous data ('continuous') or based on pixel breaths ('pixel').         Defaults to 'pixel'.</p> <p> TYPE: <code>Literal['pixel', 'continuous']</code> DEFAULT: <code>'pixel'</code> </p> <code>result_label</code> <p>label of the returned IntervalData object, defaults to <code>'pixel_tivs'</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pixel_tivs'</code> </p> <code>store</code> <p>whether to store the result in the sequence, defaults to <code>True</code> if a Sequence if provided.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SparseData</code> <p>A SparseData object with the computed TIV values.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If store is set to true but no sequence is provided.</p> <code>ValueError</code> <p>If the provided sequence is not an instance of the Sequence dataclass.</p> <code>ValueError</code> <p>If tiv_method is not one of 'inspiratory', 'expiratory', or 'mean'.</p> <code>ValueError</code> <p>If tiv_timing is not one of 'continuous' or 'pixel'.</p>"},{"location":"api/datacontainers/breath/","title":"Breath","text":""},{"location":"api/datacontainers/breath/#eitprocessing.datahandling.breath","title":"eitprocessing.datahandling.breath","text":""},{"location":"api/datacontainers/breath/#eitprocessing.datahandling.breath.Breath","title":"Breath  <code>dataclass</code>","text":"<pre><code>Breath(start_time: float, middle_time: float, end_time: float)\n</code></pre> <p>Represents a breath with a start, middle and end time.</p>"},{"location":"api/datacontainers/continuousdata/","title":"ContinuousData","text":""},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata","title":"eitprocessing.datahandling.continuousdata","text":""},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData","title":"ContinuousData  <code>dataclass</code>","text":"<pre><code>ContinuousData(\n    label: str,\n    name: str,\n    unit: str,\n    category: str,\n    description: str = \"\",\n    parameters: dict[str, Any] = dict(),\n    derived_from: Any | list[Any] = list(),\n    *,\n    time: ndarray,\n    values: ndarray,\n    sample_frequency: float | None = None\n)\n</code></pre> <p>Container for data with a continuous time axis.</p> <p>Continuous data is assumed to be sequential (i.e. a single data point at each time point, sorted by time) and continuously measured/created at a fixed sampling frequency. However, a fixed interval between consecutive time points is not enforced to account for floating point arithmetic, devices with imperfect sampling frequencies, and other sources of variation.</p> PARAMETER DESCRIPTION <code>label</code> <p>Computer readable naming of the instance.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Human readable naming of the instance.</p> <p> TYPE: <code>str</code> </p> <code>unit</code> <p>Unit of the data, if applicable.</p> <p> TYPE: <code>str</code> </p> <code>category</code> <p>Category the data falls into, e.g. 'airway pressure'.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Human readable extended description of the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>parameters</code> <p>Parameters used to derive this data.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>dict()</code> </p> <code>derived_from</code> <p>Traceback of intermediates from which the current data was derived.</p> <p> TYPE: <code>Any | list[Any]</code> DEFAULT: <code>list()</code> </p> <code>values</code> <p>Data points.</p> <p> TYPE: <code>ndarray</code> </p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.locked","title":"locked  <code>property</code>","text":"<pre><code>locked: bool\n</code></pre> <p>Return whether the values attribute is locked.</p> <p>See lock().</p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.loaded","title":"loaded  <code>property</code>","text":"<pre><code>loaded: bool\n</code></pre> <p>Return whether the data was loaded from disk, or derived from elsewhere.</p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.copy","title":"copy","text":"<pre><code>copy(\n    label: str,\n    *,\n    name: str | None = None,\n    unit: str | None = None,\n    description: str | None = None,\n    parameters: dict | None = None\n) -&gt; Self\n</code></pre> <p>Create a copy.</p> <p>Whenever data is altered, it should probably be copied first. The alterations should then be made in the copy.</p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.derive","title":"derive","text":"<pre><code>derive(\n    label: str, function: Callable, func_args: dict | None = None, **kwargs\n) -&gt; Self\n</code></pre> <p>Create a copy deriving data from values attribute.</p> PARAMETER DESCRIPTION <code>label</code> <p>New label for the derived object.</p> <p> TYPE: <code>str</code> </p> <code>function</code> <p>Function that takes the values and returns the derived values.</p> <p> TYPE: <code>Callable</code> </p> <code>func_args</code> <p>Arguments to pass to function, if any.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Values for changed attributes of derived object.</p> <p> DEFAULT: <code>{}</code> </p> <p>Example: <pre><code>def convert_data(x, add=None, subtract=None, multiply=None, divide=None):\n    if add:\n        x += add\n    if subtract:\n        x -= subtract\n    if multiply:\n        x *= multiply\n    if divide:\n        x /= divide\n    return x\n\n\ndata = ContinuousData(\n    name=\"Lung volume (in mL)\", label=\"volume_mL\", unit=\"mL\", category=\"volume\", values=some_loaded_data\n)\nderived = data.derive(\"volume_L\", convert_data, {\"divide\": 1000}, name=\"Lung volume (in L)\", unit=\"L\")\n</code></pre></p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.lock","title":"lock","text":"<pre><code>lock(*attr: str) -&gt; None\n</code></pre> <p>Lock attributes, essentially rendering them read-only.</p> <p>Locked attributes cannot be overwritten. Attributes can be unlocked using <code>unlock()</code>.</p> PARAMETER DESCRIPTION <code>*attr</code> <p>any number of attributes can be passed here, all of which will be locked. Defaults to \"values\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # lock the `values` attribute of `data`\n&gt;&gt;&gt; data.lock()\n&gt;&gt;&gt; data.values = [1, 2, 3] # will result in an AttributeError\n&gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n</code></pre>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.unlock","title":"unlock","text":"<pre><code>unlock(*attr: str) -&gt; None\n</code></pre> <p>Unlock attributes, rendering them editable.</p> <p>Locked attributes cannot be overwritten, but can be unlocked with this function to make them editable.</p> PARAMETER DESCRIPTION <code>*attr</code> <p>any number of attributes can be passed here, all of which will be unlocked. Defaults to \"values\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # lock the `values` attribute of `data`\n&gt;&gt;&gt; data.lock()\n&gt;&gt;&gt; data.values = [1, 2, 3] # will result in an AttributeError\n&gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n&gt;&gt;&gt; data.unlock()\n&gt;&gt;&gt; data.values = [1, 2, 3]\n&gt;&gt;&gt; print(data.values)\n[1,2,3]\n&gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n&gt;&gt;&gt; print(data.values)\n1\n</code></pre>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>first time point to include. Defaults to first frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>last time point. Defaults to last frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>start_inclusive</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <p> TYPE: <code>default</code> DEFAULT: <code>False</code> </p> <code>label</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> RETURNS DESCRIPTION <code>Self</code> <p>A shortened copy of the object.</p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> PARAMETER DESCRIPTION <code>other</code> <p>object that will be compared to self.</p> <p> TYPE: <code>Self</code> </p> <code>raise_</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> RETURNS DESCRIPTION <code>bool</code> <p>bool describing result of equivalence comparison.</p>"},{"location":"api/datacontainers/continuousdata/#eitprocessing.datahandling.continuousdata.ContinuousData.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy() -&gt; Self\n</code></pre> <p>Return a deep copy of the object.</p>"},{"location":"api/datacontainers/datacollection/","title":"DataCollection","text":""},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection","title":"eitprocessing.datahandling.datacollection","text":""},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection","title":"DataCollection","text":"<pre><code>DataCollection(data_type: type[V], *args, **kwargs)\n</code></pre> <p>A collection of a single type of data with unique labels.</p> <p>A DataCollection functions largely as a dictionary, but requires a data_type argument, which must be one of the data containers existing in this package. When adding an item to the collection, the type of the value must match the data_type of the collection. Furthermore, the key has to match the attribute 'label' attached to the value.</p> <p>The convenience method <code>add()</code> adds an item by setting the key to <code>value.label</code>.</p> PARAMETER DESCRIPTION <code>data_type</code> <p>the data container stored in this collection.</p> <p> TYPE: <code>type[V]</code> </p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.add","title":"add","text":"<pre><code>add(*item: V, overwrite: bool = False) -&gt; None\n</code></pre> <p>Add one or multiple item(s) to the collection using the item label as the key.</p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.get_loaded_data","title":"get_loaded_data","text":"<pre><code>get_loaded_data() -&gt; dict[str, V]\n</code></pre> <p>Return all data that was directly loaded from disk.</p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.get_data_derived_from","title":"get_data_derived_from","text":"<pre><code>get_data_derived_from(obj: V) -&gt; dict[str, V]\n</code></pre> <p>Return all data that was derived from a specific source.</p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.get_derived_data","title":"get_derived_data","text":"<pre><code>get_derived_data() -&gt; dict[str, V]\n</code></pre> <p>Return all data that was derived from any source.</p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.concatenate","title":"concatenate","text":"<pre><code>concatenate(other: Self) -&gt; Self\n</code></pre> <p>Concatenate this collection with an equivalent collection.</p> <p>Each item of self of concatenated with the item of other with the same key.</p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(\n    start_time: float | None,\n    end_time: float | None,\n    start_inclusive: bool = True,\n    end_inclusive: bool = False,\n) -&gt; DataCollection\n</code></pre> <p>Return a DataCollection containing sliced copies of the items.</p>"},{"location":"api/datacontainers/datacollection/#eitprocessing.datahandling.datacollection.DataCollection.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> PARAMETER DESCRIPTION <code>other</code> <p>object that will be compared to self.</p> <p> TYPE: <code>Self</code> </p> <code>raise_</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> RETURNS DESCRIPTION <code>bool</code> <p>bool describing result of equivalence comparison.</p>"},{"location":"api/datacontainers/eitdata/","title":"EITData","text":""},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata","title":"eitprocessing.datahandling.eitdata","text":""},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData","title":"EITData  <code>dataclass</code>","text":"<pre><code>EITData(\n    path: str | Path | list[Path | str],\n    nframes: int,\n    time: ndarray,\n    sample_frequency: float,\n    vendor: Vendor,\n    label: str | None = None,\n    description: str = \"\",\n    name: str | None = None,\n    suppress_simulated_warning: InitVar[bool] = False,\n    *,\n    pixel_impedance: ndarray\n)\n</code></pre> <p>Container for EIT impedance data.</p> <p>This class holds the pixel impedance from an EIT measurement, as well as metadata describing the measurement. The class is meant to hold data from (part of) a singular continuous measurement.</p> <p>This class can't be initialized directly. Instead, use <code>load_eit_data(&lt;path&gt;, vendor=&lt;vendor&gt;)</code> to load data from disk.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path of list of paths of the source from which data was derived.</p> <p> TYPE: <code>str | Path | list[Path | str]</code> </p> <code>nframes</code> <p>Number of frames.</p> <p> TYPE: <code>int</code> </p> <code>time</code> <p>The time of each frame (since start measurement).</p> <p> TYPE: <code>ndarray</code> </p> <code>sample_frequency</code> <p>The (average) frequency at which the frames are collected, in Hz.</p> <p> TYPE: <code>float</code> </p> <code>vendor</code> <p>The vendor of the device the data was collected with.</p> <p> TYPE: <code>Vendor</code> </p> <code>label</code> <p>Computer readable label identifying this dataset.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Human readable name for the data.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>pixel_impedance</code> <p>Impedance values for each pixel at each frame.</p> <p> TYPE: <code>ndarray</code> </p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.framerate","title":"framerate  <code>property</code>","text":"<pre><code>framerate: float\n</code></pre> <p>Deprecated alias to <code>sample_frequency</code>.</p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.ensure_path_list","title":"ensure_path_list  <code>staticmethod</code>","text":"<pre><code>ensure_path_list(path: str | Path | list[str | Path]) -&gt; list[Path]\n</code></pre> <p>Return the path or paths as a list.</p> <p>The path of any EITData object can be a single str/Path or a list of str/Path objects. This method returns a list of Path objects given either a str/Path or list of str/Paths.</p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.get_summed_impedance","title":"get_summed_impedance","text":"<pre><code>get_summed_impedance(\n    *, return_label: str | None = None, **return_kwargs\n) -&gt; ContinuousData\n</code></pre> <p>Return a ContinuousData-object with the same time axis and summed pixel values over time.</p> PARAMETER DESCRIPTION <code>return_label</code> <p>The label of the returned object; defaults to 'summed ' where '' is the label of <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>**return_kwargs</code> <p>Keyword arguments for the creation of the returned object.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.calculate_global_impedance","title":"calculate_global_impedance","text":"<pre><code>calculate_global_impedance() -&gt; ndarray\n</code></pre> <p>Return the global impedance, i.e. the sum of all included pixels at each frame.</p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>first time point to include. Defaults to first frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>last time point. Defaults to last frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>start_inclusive</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <p> TYPE: <code>default</code> DEFAULT: <code>False</code> </p> <code>label</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> RETURNS DESCRIPTION <code>Self</code> <p>A shortened copy of the object.</p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> PARAMETER DESCRIPTION <code>other</code> <p>object that will be compared to self.</p> <p> TYPE: <code>Self</code> </p> <code>raise_</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> RETURNS DESCRIPTION <code>bool</code> <p>bool describing result of equivalence comparison.</p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.EITData.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy() -&gt; Self\n</code></pre> <p>Return a deep copy of the object.</p>"},{"location":"api/datacontainers/eitdata/#eitprocessing.datahandling.eitdata.Vendor","title":"Vendor","text":"<p>Enum indicating the vendor (manufacturer) of the source EIT device.</p>"},{"location":"api/datacontainers/event/","title":"Event","text":""},{"location":"api/datacontainers/event/#eitprocessing.datahandling.event","title":"eitprocessing.datahandling.event","text":""},{"location":"api/datacontainers/event/#eitprocessing.datahandling.event.Event","title":"Event  <code>dataclass</code>","text":"<pre><code>Event(marker: int, text: str)\n</code></pre> <p>Single time point event registered during an EIT measurement.</p>"},{"location":"api/datacontainers/intervaldata/","title":"IntervalData","text":""},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata","title":"eitprocessing.datahandling.intervaldata","text":""},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.Interval","title":"Interval","text":"<p>A tuple containing the start time and end time of an interval.</p>"},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.IntervalData","title":"IntervalData  <code>dataclass</code>","text":"<pre><code>IntervalData(\n    label: str,\n    name: str,\n    unit: str | None,\n    category: str,\n    intervals: list[Interval | tuple[float, float]],\n    values: list[Any] | None = None,\n    parameters: dict[str, Any] = dict(),\n    derived_from: list[Any] = list(),\n    description: str = \"\",\n    default_partial_inclusion: bool = False,\n)\n</code></pre> <p>Container for interval data existing over a period of time.</p> <p>Interval data is data that consists for a given time interval. Examples are a ventilator setting (e.g. end-expiratory pressure), the position of a patient, a maneuver (end-expiratory hold) being performed, detected periods in the data, etc.</p> <p>Interval data consists of a number of intervals that may or may not have values associated with them.</p> <p>Examples of IntervalData with associated values are certain ventilator settings (e.g. end-expiratory pressure) and the position of a patient. Examples of IntervalData without associated values are indicators of maneouvres (e.g. a breath hold) or detected occurences (e.g. a breath).</p> <p>Interval data can be selected by time through the <code>select_by_time(start_time, end_time)</code> method. Alternatively, <code>t[start_time:end_time]</code> can be used.</p> PARAMETER DESCRIPTION <code>label</code> <p>Computer readable label identifying this dataset.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Human readable name for the data.</p> <p> TYPE: <code>str</code> </p> <code>unit</code> <p>The unit of the data, if applicable.</p> <p> TYPE: <code>str | None</code> </p> <code>category</code> <p>Category the data falls into, e.g. 'breath'.</p> <p> TYPE: <code>str</code> </p> <code>intervals</code> <p>A list of intervals (tuples containing a start time and end time).</p> <p> TYPE: <code>list[Interval | tuple[float, float]]</code> </p> <code>values</code> <p>An optional list of values associated with each interval.</p> <p> TYPE: <code>list[Any] | None</code> DEFAULT: <code>None</code> </p> <code>parameters</code> <p>Parameters used to derive the data.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>dict()</code> </p> <code>derived_from</code> <p>Traceback of intermediates from which the current data was derived.</p> <p> TYPE: <code>list[Any]</code> DEFAULT: <code>list()</code> </p> <code>description</code> <p>Extended human readible description of the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>default_partial_inclusion</code> <p>Whether to include a trimmed version of an interval when selecting data</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.IntervalData.has_values","title":"has_values  <code>property</code>","text":"<pre><code>has_values: bool\n</code></pre> <p>True if the IntervalData has values, False otherwise.</p>"},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.IntervalData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.IntervalData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(\n    start_time: float | None = None,\n    end_time: float | None = None,\n    partial_inclusion: bool | None = None,\n    newlabel: str | None = None,\n) -&gt; Self\n</code></pre> <p>Create a new copy of the object, selecting data between start_time and end_time.</p> <p>This function returns a shortened copy of the object, containing data from the specified start_time to end_time.</p> <p>If <code>partial_inclusion</code> is set to <code>True</code>, any intervals that overlap with the start_time or end_time are included in the selection, and their times are adjusted to fit within the specified range. If <code>partial_inclusion</code> is <code>False</code>, intervals that overlap the start or end times are excluded from the selection.</p> <p>For example: - Set <code>partial_inclusion</code> to <code>True</code> for cases like \"set_driving_pressure\" where you want to include settings that were active before the start_time. - Set <code>partial_inclusion</code> to <code>False</code> for cases like \"detected_breaths\" where you want to exclude partial data that doesn't fully fit within the time range.</p> <p>Note that the end_time is always included in the selection if it is present in the original object.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>The earliest time point to include in the copy.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>The latest time point to include in the copy.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>partial_inclusion</code> <p>Whether to include intervals that overlap with the start_time or end_time.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>newlabel</code> <p>A new label for the copied object.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.IntervalData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p>"},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.IntervalData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> PARAMETER DESCRIPTION <code>other</code> <p>object that will be compared to self.</p> <p> TYPE: <code>Self</code> </p> <code>raise_</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> RETURNS DESCRIPTION <code>bool</code> <p>bool describing result of equivalence comparison.</p>"},{"location":"api/datacontainers/intervaldata/#eitprocessing.datahandling.intervaldata.IntervalData.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy() -&gt; Self\n</code></pre> <p>Return a deep copy of the object.</p>"},{"location":"api/datacontainers/pixelmap/","title":"PixelMap","text":""},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap","title":"eitprocessing.datahandling.pixelmap","text":"<p>Handling and visualizing pixel-based data maps in EIT analysis.</p> <p>This module provides classes for working with pixel-based data representations of EIT data. The main class hierarchy consists of:</p> <ul> <li>PixelMap: Base class for representing 2D pixel data with visualization and manipulation capabilities.</li> <li>TIVMap: Specialized map for Tidal Impedance Variation (TIV) data.</li> <li>ODCLMap: Specialized map for Overdistention and Collapse (ODCL) analysis.</li> <li>DifferenceMap: Specialized map for visualizing differences between pixel maps.</li> <li>PerfusionMap: Specialized map for perfusion analysis.</li> <li>PendelluftMap: Specialized map for positive-only pendelluft values (severity, no direction).</li> <li>SignedPendelluftMap: Specialized map for signed pendelluft values (severity and direction/phase).</li> </ul> <p>Plotting configurations for each map are managed via a class inheriting from <code>PixelMapPlotConfig</code>, which allows for flexible configuration of colormap, normalization, colorbar, and other display options. These can be customized per map type or per instance. <code>PixelMapPlotConfig</code> is found in <code>eitprocessing.plotting.pixelmap</code>.</p> <p>All classes are immutable to ensure data integrity during analysis pipelines.</p> <p><code>PixelMap</code> provides a <code>replace</code> method, which allows you to create a copy of the object with one or more attributes replaced, similar to <code>dataclasses.replace()</code> (or `copy.replace() in Python \u22653.13). This is useful for updating data or configuration in an immutable and chainable way, and supports partial updates of nested configuration (e.g., updating only a single plotting configuration).</p> Mathematical Operations <p><code>PixelMap</code> instances support basic mathematical operations (+, -, *, /) with other <code>PixelMap</code> instances, arrays, or scalar values. The operations are applied element-wise to the underlying values.</p> <ul> <li> <p>Addition (+): Returns a <code>PixelMap</code> with values added element-wise.</p> </li> <li> <p>Subtraction (-): Returns a <code>DifferenceMap</code> with values subtracted element-wise.</p> </li> <li> <p>Multiplication (*): Returns a <code>PixelMap</code> with values multiplied element-wise.</p> </li> <li> <p>Division (/): Returns a <code>PixelMap</code> with values divided element-wise. Division by zero results in NaN values with a   warning.</p> </li> </ul> <p>When operating with another <code>PixelMap</code> of any type, operations typically return the base PixelMap type, except for subtraction which returns a DifferenceMap. When operating with scalars or arrays, operations return the same type as the original <code>PixelMap</code>.</p> <p>Note: Some <code>PixelMap</code> subclasses (like <code>TIVMap</code> and <code>PerfusionMap</code>) do not allow negative values. Operations that might produce negative values with these maps will display appropriate warnings.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap","title":"PixelMap  <code>dataclass</code>","text":"<pre><code>PixelMap(\n    values: ArrayLike,\n    *,\n    label: str | None = None,\n    suppress_negative_warning: bool = False,\n    suppress_all_nan_warning: bool = False,\n    plot_config: PixelMapPlotConfig | dict | None = None\n)\n</code></pre> <p>Map representing a single value for each pixel.</p> <p>At initialization, values are conveted to a 2D numpy array of floats. The values are immutable after initialization, meaning that the <code>values</code> attribute cannot be changed directly. Instead, use the <code>replace(...)</code> method to create a copy with new values or label.</p> <p>For many common cases, specific classes with default plot configurations are available.</p> PARAMETER DESCRIPTION <code>values</code> <p>2D array of pixel values.</p> <p> TYPE: <code>ndarray</code> </p> <code>label</code> <p>Label for the pixel map.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>plot_config</code> <p>Plotting configuration controlling colormap, normalization, colorbar, and other display options. Accepts both a PixelMapPlotConfig instance or a dict, which is converted to PixelMapPlotConfig during initialization. Subclasses provide their own defaults.</p> <p> TYPE: <code>dict | PixelMapPlotConfig | None</code> DEFAULT: <code>None</code> </p> ATTRIBUTE DESCRIPTION <code>allow_negative_values</code> <p>Whether negative values are allowed in the pixel map.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PixelMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap","title":"DifferenceMap  <code>dataclass</code>","text":"<pre><code>DifferenceMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map representing the difference between two pixel maps.</p> <p>Values are centered around zero, with positive values indicating an increase and negative values indicating a decrease in the pixel values compared to a reference map. If the values are all expected to be positive, consider converting to a normal <code>PixelMap</code> or other subclass instead.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.DifferenceMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap","title":"TIVMap  <code>dataclass</code>","text":"<pre><code>TIVMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map representing the tidal impedance variation.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.TIVMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap","title":"AmplitudeMap  <code>dataclass</code>","text":"<pre><code>AmplitudeMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map representing the amplitude.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.AmplitudeMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap","title":"ODCLMap  <code>dataclass</code>","text":"<pre><code>ODCLMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map representing normalized overdistention and collapse.</p> <p>Values between -1 and 0 (-100% to 0%) represent compliance change due to collapse , while values between 0 and 1 (0% to 100%) represent compliance change due to overdistention.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.ODCLMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap","title":"PerfusionMap  <code>dataclass</code>","text":"<pre><code>PerfusionMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map representing perfusion values.</p> <p>Values represent perfusion, where higher values indicate more perfusion. The values are expected to be non-negative, with 0 representing no perfusion.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PerfusionMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap","title":"PendelluftMap  <code>dataclass</code>","text":"<pre><code>PendelluftMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map representing positive-only pendelluft values.</p> <p>Values represent pendelluft severity as positive values. There is no distinction between pixels with early inflation and pixels with late inflation. Alternatively, you can use a SignedPendelluftMap for signed data.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.PendelluftMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap","title":"SignedPendelluftMap  <code>dataclass</code>","text":"<pre><code>SignedPendelluftMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map representing pendelluft values as signed values.</p> <p>Values represent pendelluft severity. Negative values indicate pixels that have early inflation (before the global inflation starts), while negative values indicate pixels that have late inflation (after the global inflation starts).</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.normalize","title":"normalize","text":"<pre><code>normalize(\n    *,\n    mode: Literal[\n        \"zero-based\", \"symmetric\", \"maximum\", \"reference\"\n    ] = \"zero-based\",\n    reference: float | None = None,\n    **kwargs\n) -&gt; Self\n</code></pre> <p>Normalize the pixel map values.</p> <p>Creates a copy of the pixel map with normalized values. Four normalization modes are available.</p> <ul> <li>\"zero-based\" (default): normalizes the values to the range [0, 1] by subtracting the minimum value and   dividing by the new maximum value. This ensures the lowest resulting value is 0 and the highest resulting   value is 1.</li> <li>\"symmetric\": divides the values by the maximum absolute value, resulting in a range of [-1, 1].</li> <li>\"maximum\": divides the values by the maximum value. If all values are positive, this normalizes them to the   range [0, 1] without shifting the minimum value to zero. The sign of values does not change. If negative   values are present, the result may extend below -1.</li> <li>\"reference\": is similar to \"maximum\", except it divides by a user-defined reference value. A reference value   must be provided. Resulting values can fall outside the range [-1, 1].</li> </ul> <p>NaN values are ignored when normalizing. All-NaN pixel maps results in a ValueError.</p> <p>Examples: <pre><code>&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize()  # Default is zero-based normalization\nPixelMap(values=array([[0. , 0.5, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[1, 3, 5]]).normalize(mode=\"maximum\")\nPixelMap(values=array([[0.2, 0.6, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize()\nPixelMap(values=array([[0. , 0.7, 1. ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"symmetric\")\nPixelMap(values=array([[-1.   , -0.125,  0.25 ]]), ...)\n\n&gt;&gt;&gt; PixelMap([[-8, -1, 2]]).normalize(mode=\"reference\", reference=4)\nPixelMap(values=array([[-2.  , -0.25,  0.5 ]]), ...)\n</code></pre></p> PARAMETER DESCRIPTION <code>mode</code> <p>The normalization mode to use. Defaults to \"zero-based\".</p> <p> TYPE: <code>Literal['zero-based', 'symmetric', 'maximum', 'reference']</code> DEFAULT: <code>'zero-based'</code> </p> <code>reference</code> <p>The reference value to use for normalization in \"reference\" mode.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the new PixelMap instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If an invalid normalization mode is specified.</p> <code>ValueError</code> <p>If no reference value is provided in \"reference\" mode.</p> <code>ValueError</code> <p>If a reference value is provided with a mode other than \"reference\".</p> <code>TypeError</code> <p>If the reference value is not a number.</p> <code>ZeroDivisionError</code> <p>If normalization by zero is attempted (either <code>reference=0</code>, or the maximum (absolute) value in the values is 0).</p> <code>ValueError</code> <p>If normalization by NaN is attempted (either <code>reference=np.nan</code>, or all values are NaN).</p> WARNS DESCRIPTION <code>UserWarning</code> <p>If normalization by a negative number is attempted (either <code>reference</code> is negative, or all values are negative). This results in inverting the sign of the values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.SignedPendelluftMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap","title":"IntegerMap  <code>dataclass</code>","text":"<pre><code>IntegerMap(\n    values: ndarray,\n    *,\n    label: str | None = None,\n    plot_config: InitVar[PixelMapPlotConfig]\n)\n</code></pre> <p>Pixel map with integer values.</p> <p>This class is a wrapper around PixelMap that ensures the values are integers. It is useful for pixel maps that represent labels or discrete values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Get the shape of the pixel map values.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>A utility class for plotting the pixel map with the specified configuration.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.normalize","title":"normalize","text":"<pre><code>normalize(*args, **kwargs) -&gt; NoReturn\n</code></pre> <p>Normalization is not supported for IntegerMap.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.create_mask_from_threshold","title":"create_mask_from_threshold","text":"<pre><code>create_mask_from_threshold(\n    threshold: float,\n    *,\n    comparator: Callable = greater_equal,\n    use_magnitude: bool = False,\n    fraction_of_max: bool = False,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Create a pixel mask from the pixel map based on threshold values.</p> <p>The values of the pixel map are compared to the threshold values. By default, the comparator is <code>&gt;=</code> (<code>np.greater_equal</code>), such that the resulting mask is 1.0 where the map values are at least the threshold values, and NaN elsewhere. The comparator can be set to any comparison function, e.g.<code>np.less</code>, a function from the <code>operator</code> module or custom function which takes pixel map values array and threshold as arguments, and returns a boolean array with the same shape as the array.</p> <p>If <code>use_magnitude</code> is True, absolute values are compared to the threshold.</p> <p>If <code>fraction_of_max</code> is True, the threshold is interpreted as a fraction of the maximum value in the map. For example, a threshold of 0.2 with <code>fraction_of_max=True</code> will create a mask where values are at least 20% of the maximum value.</p> <p>The shape of the pixel mask is the same as the shape of the pixel map.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>The threshold value or fraction, depending on <code>fraction_of_max</code> argument.</p> <p> TYPE: <code>float</code> </p> <code>comparator</code> <p>A function that compares pixel values against the threshold.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>greater_equal</code> </p> <code>use_magnitude</code> <p>If True, apply the threshold to the absolute values of the pixel map.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fraction_of_max</code> <p>If True, interpret threshold as a fraction of the maximum value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A PixelMask instance with values 1.0 where comparison is true, and NaN elsewhere.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>threshold</code> is not a float or <code>comparator</code> is not callable.</p> <p>Examples:</p> <p>pm = PixelMap([[0.1, 0.5, 0.9]]) mask = pm.create_mask_from_threshold(0.5)  # Absolute threshold of 0.5 PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, fraction_of_max=True)  # 50% of max value (=0.45) PixelMask(mask=array([[nan,  1.,  1.]]))</p> <p>mask = pm.create_mask_from_threshold(0.5, comparator=np.less) PixelMask(mask=array([[ 1., nan, nan]]))</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.convert_to","title":"convert_to","text":"<pre><code>convert_to(\n    target_type: type[PixelMapT], *, keep_attrs: bool = False, **kwargs: dict\n) -&gt; PixelMapT\n</code></pre> <p>Convert the pixel map to (a different subclass of) PixelMap.</p> <p>This method allows for converting the pixel map to a <code>PixelMap</code> or different subclass of <code>PixelMap</code>. The <code>label</code> attribute is copied by default, but a new label can be provided. Other attributes are not copied by default, but can be retained by setting <code>keep_attrs</code> to True. Additional keyword arguments can be passed to the new instance.</p> PARAMETER DESCRIPTION <code>target_type</code> <p>The target subclass to convert to.</p> <p> TYPE: <code>type[T]</code> </p> <code>keep_attrs</code> <p>If True, retains the attributes of the original pixel map in the new instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the new instance.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the target type with the same values and attributes.</p> <p> TYPE: <code>PixelMapT</code> </p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.from_aggregate","title":"from_aggregate  <code>classmethod</code>","text":"<pre><code>from_aggregate(\n    maps: Sequence[ArrayLike | PixelMap],\n    aggregator: Callable[..., ndarray],\n    **return_attrs\n) -&gt; Self\n</code></pre> <p>Get a pixel map by aggregating several pixel maps with a specified function.</p> <p>The maps can be 2D numpy arrays, sequences that can be converted to 2D numpy arrays (e.g., nested lists), or PixelMap objects. The aggregation is performed using the provided function along the first axis of the stacked maps. NaN values are typically ignored by using numpy's nan-aware functions (np.nanmean, np.nanmedian, etc.).</p> <p>Returns the same class as this function was called from. Keyword arguments are passed to the initializer of that object.</p> PARAMETER DESCRIPTION <code>maps</code> <p>list/tuple of maps to be aggregated</p> <p> TYPE: <code>Sequence[ArrayLike | PixelMap]</code> </p> <code>aggregator</code> <p>Function to aggregate the maps along the first axis. Should accept an array and axis parameter.</p> <p> TYPE: <code>Callable[..., ndarray]</code> </p> <code>**return_attrs</code> <p>Keyword arguments to be passed to the initializer of the return object</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new instance with the aggregated values of the pixel maps.</p> <p> TYPE: <code>Self</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; maps = [PixelMap([[1, 2]]), PixelMap([[3, 4]]), PixelMap([[5, 6]])]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmean)  # Mean: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmedian)  # Median: [[3, 4]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmax)  # Maximum: [[5, 6]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, np.nanmin)  # Minimum: [[1, 2]]\n&gt;&gt;&gt; TIVMap.from_aggregate(maps, lambda x, axis: np.nanpercentile(x, 75, axis=axis))  # 75th percentile\n</code></pre>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.to_boolean_array","title":"to_boolean_array","text":"<pre><code>to_boolean_array(*, zero: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a boolean array.</p> <p>NaN values are replaced with False, and the resulting array is cast to boolean. 0-values are converted to False by default. Provided <code>zero=True</code> in case you want to treat 0-values as True.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of booleans, where NaN values are replaced with False.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.to_integer_array","title":"to_integer_array","text":"<pre><code>to_integer_array() -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to an integer array.</p> <p>NaN values are replaced with 0, and the resulting array is cast to integers.</p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array of integers, where NaN values are replaced with 0.</p>"},{"location":"api/datacontainers/pixelmap/#eitprocessing.datahandling.pixelmap.IntegerMap.to_non_nan_array","title":"to_non_nan_array","text":"<pre><code>to_non_nan_array(*, nan: float = 0.0, dtype: type | dtype = float) -&gt; ndarray\n</code></pre> <p>Convert the pixel map values to a numpy array, replacing NaN with a fill value.</p> PARAMETER DESCRIPTION <code>nan</code> <p>The value to replace NaN values with. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dtype</code> <p>The data type of the resulting array. Defaults to float.</p> <p> TYPE: <code>type | dtype</code> DEFAULT: <code>float</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: A 2D numpy array with NaN values replaced by <code>nan</code> and cast to the specified <code>dtype</code>.</p>"},{"location":"api/datacontainers/sequence/","title":"Sequence","text":""},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence","title":"eitprocessing.datahandling.sequence","text":""},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence","title":"Sequence  <code>dataclass</code>","text":"<pre><code>Sequence(\n    label: str | None = None,\n    name: str | None = None,\n    description: str = \"\",\n    eit_data: DataCollection[EITData] = (lambda: DataCollection(EITData))(),\n    continuous_data: DataCollection[ContinuousData] = (\n        lambda: DataCollection(ContinuousData)\n    )(),\n    sparse_data: DataCollection[SparseData] = (\n        lambda: DataCollection(SparseData)\n    )(),\n    interval_data: DataCollection[IntervalData] = (\n        lambda: DataCollection(IntervalData)\n    )(),\n)\n</code></pre> <p>Sequence of timepoints containing respiratory data.</p> <p>A Sequence object is a representation of data points over time. These data can consist of any combination of EIT frames (<code>EITData</code>), waveform data (<code>ContinuousData</code>) from different sources, or individual events (<code>SparseData</code>) occurring at any given timepoint. A Sequence can consist of an entire measurement, a section of a measurement, a single breath, or even a portion of a breath. A Sequence can consist of multiple sets of each type of data from the same time-points or can be a single measurement from just one source.</p> <p>A Sequence can be split up into separate sections of a measurement or multiple (similar) Sequence objects can be merged together to form a single Sequence.</p> PARAMETER DESCRIPTION <code>label</code> <p>Computer readable naming of the instance.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Human readable naming of the instance.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>Human readable extended description of the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>eit_data</code> <p>Collection of one or more sets of EIT data frames.</p> <p> TYPE: <code>DataCollection[EITData]</code> DEFAULT: <code>(lambda: DataCollection(EITData))()</code> </p> <code>continuous_data</code> <p>Collection of one or more sets of continuous data points.</p> <p> TYPE: <code>DataCollection[ContinuousData]</code> DEFAULT: <code>(lambda: DataCollection(ContinuousData))()</code> </p> <code>sparse_data</code> <p>Collection of one or more sets of individual data points.</p> <p> TYPE: <code>DataCollection[SparseData]</code> DEFAULT: <code>(lambda: DataCollection(SparseData))()</code> </p>"},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence.time","title":"time  <code>property</code>","text":"<pre><code>time: ndarray\n</code></pre> <p>Time axis from either EITData or ContinuousData.</p>"},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence.data","title":"data  <code>property</code>","text":"<pre><code>data: _DataAccess\n</code></pre> <p>Shortcut access to data stored in collections inside a sequence.</p> <p>This allows all data objects stored in a collection inside a sequence to be accessed. Instead of <code>sequence.continuous_data[\"global_impedance\"]</code> you can use <code>sequence.data[\"global_impedance\"]</code>. This works for getting (<code>sequence.data[\"label\"]</code> or <code>sequence.data.get(\"label\")</code>) and adding data (<code>sequence.data[\"label\"] = obj</code> or <code>sequence.data.add(obj)</code>).</p> <p>Other dict-like behaviour is also supported:</p> <ul> <li><code>label in sequence.data</code> to check whether an object with a label exists;</li> <li><code>del sequence.data[label]</code> to remove an object from the sequence based on the label;</li> <li><code>for label in sequence.data</code> to iterate over the labels;</li> <li><code>sequence.data.items()</code> to retrieve a list of (label, object) pairs, especially useful for iteration;</li> <li><code>sequence.data.labels()</code> or <code>sequence.data.keys()</code> to get a list of data labels;</li> <li><code>sequence.data.objects()</code> or <code>sequence.data.values()</code> to get a list of data objects.</li> </ul> <p>This interface only works if the labels are unique among the data collections. An attempt to add a data object with an exiting label will result in a KeyError.</p>"},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence.concatenate","title":"concatenate  <code>classmethod</code>","text":"<pre><code>concatenate(a: Sequence, b: Sequence, newlabel: str | None = None) -&gt; Sequence\n</code></pre> <p>Create a merge of two Sequence objects.</p>"},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = True,\n    end_inclusive: bool = False,\n    label: str | None = None,\n    name: str | None = None,\n    description: str = \"\",\n) -&gt; Self\n</code></pre> <p>Return a sliced version of the Sequence.</p> <p>See <code>SelectByTime.select_by_time()</code>.</p>"},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p>"},{"location":"api/datacontainers/sequence/#eitprocessing.datahandling.sequence.Sequence.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> PARAMETER DESCRIPTION <code>other</code> <p>object that will be compared to self.</p> <p> TYPE: <code>Self</code> </p> <code>raise_</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> RETURNS DESCRIPTION <code>bool</code> <p>bool describing result of equivalence comparison.</p>"},{"location":"api/datacontainers/sparsedata/","title":"SparseData","text":""},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata","title":"eitprocessing.datahandling.sparsedata","text":""},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata.SparseData","title":"SparseData  <code>dataclass</code>","text":"<pre><code>SparseData(\n    label: str,\n    name: str,\n    unit: str | None,\n    category: str,\n    time: ndarray,\n    description: str = \"\",\n    parameters: dict[str, Any] = dict(),\n    derived_from: list[Any] = list(),\n    values: Any | None = None,\n)\n</code></pre> <p>Container for data related to individual time points.</p> <p>Sparse data is data for which the time points are not necessarily evenly spaced. Data can consist time-value pairs or only time points.</p> <p>Sparse data differs from interval data in that each data points is associated with a single time point rather than a time range.</p> <p>Examples are data points at end of inspiration/end of expiration (e.g. tidal volume, end-expiratoy lung impedance) or detected time points (e.g. QRS complexes).</p> PARAMETER DESCRIPTION <code>label</code> <p>Computer readable name.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Human readable name.</p> <p> TYPE: <code>str</code> </p> <code>unit</code> <p>Unit of the data, if applicable.</p> <p> TYPE: <code>str | None</code> </p> <code>category</code> <p>Category the data falls into, e.g. 'detected r peak'.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Human readable extended description of the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>parameters</code> <p>Parameters used to derive the data.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>dict()</code> </p> <code>derived_from</code> <p>Traceback of intermediates from which the current data was derived.</p> <p> TYPE: <code>list[Any]</code> DEFAULT: <code>list()</code> </p> <code>values</code> <p>List or array of values. These van be numeric data, text or Python objects.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata.SparseData.has_values","title":"has_values  <code>property</code>","text":"<pre><code>has_values: bool\n</code></pre> <p>True if the SparseData has values, False otherwise.</p>"},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata.SparseData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata.SparseData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> PARAMETER DESCRIPTION <code>start_time</code> <p>first time point to include. Defaults to first frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>end_time</code> <p>last time point. Defaults to last frame of sequence.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>start_inclusive</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <p> TYPE: <code>default</code> DEFAULT: <code>False</code> </p> <code>label</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> RETURNS DESCRIPTION <code>Self</code> <p>A shortened copy of the object.</p>"},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata.SparseData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p>"},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata.SparseData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> PARAMETER DESCRIPTION <code>other</code> <p>object that will be compared to self.</p> <p> TYPE: <code>Self</code> </p> <code>raise_</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> RETURNS DESCRIPTION <code>bool</code> <p>bool describing result of equivalence comparison.</p>"},{"location":"api/datacontainers/sparsedata/#eitprocessing.datahandling.sparsedata.SparseData.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy() -&gt; Self\n</code></pre> <p>Return a deep copy of the object.</p>"},{"location":"api/features/breath_detection/","title":"Breath Detection","text":""},{"location":"api/features/breath_detection/#eitprocessing.features.breath_detection.BreathDetection","title":"eitprocessing.features.breath_detection.BreathDetection  <code>dataclass</code>","text":"<pre><code>BreathDetection(\n    *,\n    minimum_duration: float = 2 / 3,\n    averaging_window_duration: float = 15,\n    averaging_window_function: Callable[[int], ArrayLike] | None = blackman,\n    amplitude_cutoff_fraction: float | None = 0.25,\n    invalid_data_removal_window_length: float = 0.5,\n    invalid_data_removal_percentile: int = 5,\n    invalid_data_removal_multiplier: int = 4\n)\n</code></pre> <p>Algorithm for detecting breaths in data representing respiration.</p> <p>This algorithm detects the position of breaths in data by detecting valleys (local minimum values) and peaks (local maximum values) in data. BreathDetection has a default minimum duration of breaths to be detected. The minimum duration should be short enough to include the shortest expected breath in the data. The minimum duration is implemented as the minimum time between peaks and between valleys.</p> <p>Examples: <pre><code>&gt;&gt;&gt; bd = BreathDetection(minimum_duration=0.5)\n&gt;&gt;&gt; breaths = bd.find_breaths(\n...     sequency=seq,\n...     continuousdata_label=\"global_impedance_(raw)\"\n... )\n</code></pre></p> <pre><code>&gt;&gt;&gt; global_impedance = seq.continuous_data[\"global_impedance_(raw)\"]\n&gt;&gt;&gt; breaths = bd.find_breaths(continuous_data=global_impedance)\n</code></pre> PARAMETER DESCRIPTION <code>minimum_duration</code> <p>minimum expected duration of breaths, defaults to 2/3 of a second</p> <p> TYPE: <code>float</code> DEFAULT: <code>2 / 3</code> </p> <code>averaging_window_duration</code> <p>duration of window used for averaging the data, defaults to 15 seconds</p> <p> TYPE: <code>float</code> DEFAULT: <code>15</code> </p> <code>averaging_window_function</code> <p>function used to create a window for averaging the data, defaults to np.blackman</p> <p> TYPE: <code>Callable[[int], ArrayLike] | None</code> DEFAULT: <code>blackman</code> </p> <code>amplitude_cutoff_fraction</code> <p>fraction of the median amplitude below which breaths are removed, defaults to 0.25</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>0.25</code> </p> <code>invalid_data_removal_window_length</code> <p>window around invalid data in which breaths are removed, defaults to 0.5</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>invalid_data_removal_percentile</code> <p>the nth percentile of values used to remove outliers, defaults to 5</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>invalid_data_removal_multiplier</code> <p>the multiplier used to remove outliers, defaults to 4</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p>"},{"location":"api/features/breath_detection/#eitprocessing.features.breath_detection.BreathDetection.find_breaths","title":"find_breaths","text":"<pre><code>find_breaths(\n    continuous_data: ContinuousData,\n    result_label: str = \"breaths\",\n    sequence: Sequence | None = None,\n    store: bool | None = None,\n) -&gt; IntervalData\n</code></pre> <p>Find breaths based on peaks and valleys, removing edge cases and breaths during invalid data.</p> <p>First, it naively finds any peaks that are a certain distance apart and higher than the moving average, and similarly valleys that are a certain distance apart and below the moving average.</p> <p>Next, valleys at the start and end of the signal are removed to ensure the first and last valleys are actual valleys, and not just the start or end of the signal. Peaks before the first or after the last valley are removed, to ensure peaks always fall between two valleys.</p> <p>At this point, it is possible multiple peaks exist between two valleys. Lower peaks are removed leaving only the highest peak between two valleys. Similarly, multiple valleys between two peaks are reduced to only the lowest valley.</p> <p>As a last step, breaths with a low amplitude (the average between the inspiratory and expiratory amplitudes) are removed.</p> <p>Breaths are constructed as a valley-peak-valley combination, representing the start of inspiration, the end of inspiration/start of expiration, and end of expiration.</p> PARAMETER DESCRIPTION <code>continuous_data</code> <p>optional, a ContinuousData object that contains the data</p> <p> TYPE: <code>ContinuousData</code> </p> <code>result_label</code> <p>label of the returned IntervalData object, defaults to <code>'breaths'</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'breaths'</code> </p> <code>sequence</code> <p>optional, Sequence that contains the object to detect breaths in, and/or to store the result in</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> <code>store</code> <p>whether to store the result in the sequence, defaults to <code>True</code> if a Sequence if provided.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>IntervalData</code> <p>An IntervalData object containing Breath objects.</p>"},{"location":"api/features/moving_average/","title":"Moving Average","text":""},{"location":"api/features/moving_average/#eitprocessing.features.moving_average.MovingAverage","title":"eitprocessing.features.moving_average.MovingAverage  <code>dataclass</code>","text":"<pre><code>MovingAverage(\n    window_size: int,\n    window_function: Callable | None = None,\n    padding_type: str = \"edge\",\n)\n</code></pre> <p>Algorithm for calculating the moving average of the data.</p> <p>This class provides a method for calculating of the moving average of a 1D signal by convolution with a window with a given size. If not window function is provided, all samples within that window contribute equally to the moving average. If a window function is provided, the samples are weighed according to the values in the window function.</p> <p>Before convolution the data is padded. The padding type is 'edge' by default. See <code>np.pad()</code> for more information. Padding adds values at the start and end with the first/last value, to more accurately determine the average at the boundaries of the data.</p> PARAMETER DESCRIPTION <code>window_size</code> <p>the number of data points in the averaging window. Should be odd; is increased by 1 if even.</p> <p> TYPE: <code>int</code> </p> <code>window_function</code> <p>window function, e.g. np.blackman.</p> <p> TYPE: <code>Callable | None</code> DEFAULT: <code>None</code> </p> <code>padding_type</code> <p>see <code>np.pad()</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'edge'</code> </p> RETURNS DESCRIPTION <p>np.ndarray: moving average of data with the same shape as <code>data</code>.</p>"},{"location":"api/features/moving_average/#eitprocessing.features.moving_average.MovingAverage.apply","title":"apply","text":"<pre><code>apply(data: ndarray) -&gt; ndarray\n</code></pre> <p>Apply the moving average on the data.</p> PARAMETER DESCRIPTION <code>data</code> <p>input data as 1D array</p> <p> TYPE: <code>NDArray</code> </p>"},{"location":"api/features/pixel_breath/","title":"Pixel Breath","text":""},{"location":"api/features/pixel_breath/#eitprocessing.features.pixel_breath.PixelBreath","title":"eitprocessing.features.pixel_breath.PixelBreath  <code>dataclass</code>","text":"<pre><code>PixelBreath(\n    *,\n    breath_detection: BreathDetection = _return_sentinel_breath_detection(),\n    breath_detection_kwargs: InitVar[dict | None] = None,\n    phase_correction_mode: (\n        Literal[\"negative amplitude\", \"phase shift\", \"none\"] | None\n    ) = \"negative amplitude\"\n)\n</code></pre> <p>Algorithm for detecting timing of pixel breaths in pixel impedance data.</p> <p>This algorithm detects the position of start of inspiration, end of inspiration and end of expiration in pixel impedance data. It uses BreathDetection to find the global start and end of inspiration and expiration. These points are then used to find the start/end of pixel inspiration/expiration in pixel impedance data.</p> <p>Since this algorithm uses the previous and next global breath to determine the start and end of a pixel breath, the first and last global breaths can not be used to determine pixel breaths. They are always set to <code>None</code> in the return list. Breaths that could not properly be detected are set to <code>None</code> as well.</p> <p>Some pixel breaths may be phase shifted (inflation starts and ends later compared to others, e.g., due to pendelluft or late airway opening). Other pixel breaths may have a negative amplitude (impedance decreases during inspiration, e.g., due to pleural effusion or reconstruction artifacts). It is not always possible to determine whether a pixel is out of phase or has a negative amplitude. PixelBreath has three different phase correction modes. In 'negative amplitude' mode (default), pixels that have a decrease in amplitude between the start and end of globally defined inspiration, will have a negative amplitude and smaller phase shift. In 'phase shift' mode, all pixel breaths will have positive amplitudes, but can have large phase shifts. In 'none'/<code>None</code> mode, all pixels are assumed to be within rouglhy -90 to 90 degrees of phase. Note that the 'none' mode can lead to unexpected results, such as ultra-short (down to 2 frames) or very long breaths.</p> <p>Example: <pre><code>&gt;&gt;&gt; pi = PixelBreath()\n&gt;&gt;&gt; eit_data = sequence.eit_data['raw']\n&gt;&gt;&gt; continuous_data = sequence.continuous_data['global_impedance_(raw)']\n&gt;&gt;&gt; pixel_breaths = pi.find_pixel_breaths(eit_data, continuous_data, sequence)\n</code></pre></p> PARAMETER DESCRIPTION <code>breath_detection</code> <p>BreathDetection object to use for detecting breaths.</p> <p> TYPE: <code>BreathDetection</code> DEFAULT: <code>_return_sentinel_breath_detection()</code> </p> <code>phase_correction_mode</code> <p>How to resolve pixels that are out-of-phase. Defaults to \"negative amplitude\".</p> <p> TYPE: <code>Literal['negative amplitude', 'phase shift', 'none'] | None</code> DEFAULT: <code>'negative amplitude'</code> </p>"},{"location":"api/features/pixel_breath/#eitprocessing.features.pixel_breath.PixelBreath.find_pixel_breaths","title":"find_pixel_breaths","text":"<pre><code>find_pixel_breaths(\n    eit_data: EITData,\n    continuous_data: ContinuousData,\n    sequence: Sequence | None = None,\n    store: bool | None = None,\n    result_label: str = \"pixel_breaths\",\n) -&gt; IntervalData\n</code></pre> <p>Find pixel breaths in the data.</p> <p>This method finds the pixel start/end of inspiration/expiration based on the start/end of inspiration/expiration as detected in the continuous data.</p> <p>For most pixels, the start of a breath (start inspiration) is the valley between the middles (start of expiration) of the globally defined breaths on either side. The end of a pixel breath is the start of the next pixel breath. The middle of the pixel breath is the peak between the start and end of the pixel breath.</p> <p>If the pixel is out of phase or has negative amplitude, the definition of the breath depends on the phase correction mode. In 'negative amplitude' mode, the start of a breath is the peak between the middles of the globally defined breaths on either side, while the middle of the pixel breath is the valley of the start and end of the pixel breath. In 'phase shift' mode, first the phase shift between the pixel impedance and global impedance is determined as the highest crosscorrelation between the signals near a phase shift of 0. The start of breath is the valley between the phase shifted middles of the globally defined breaths on either side.</p> <p>Pixel breaths are constructed as a valley-peak-valley combination, representing the start of inspiration, the end of inspiration/start of expiration, and end of expiration.</p> PARAMETER DESCRIPTION <code>eit_data</code> <p>EITData to apply the algorithm to.</p> <p> TYPE: <code>EITData</code> </p> <code>continuous_data</code> <p>ContinuousData to use for global breath detection.</p> <p> TYPE: <code>ContinuousData</code> </p> <code>result_label</code> <p>label of the returned IntervalData object, defaults to <code>'pixel_breaths'</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pixel_breaths'</code> </p> <code>sequence</code> <p>optional, Sequence that contains the object to detect pixel breaths in, and/or to store the result</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> <code>store</code> <p>whether to store the result in the sequence, defaults to <code>True</code> if a Sequence if provided.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>IntervalData</code> <p>An IntervalData object containing Breath objects.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If store is set to true but no sequence is provided.</p> <code>ValueError</code> <p>If the provided sequence is not an instance of the Sequence dataclass.</p>"},{"location":"api/features/rate_detection/","title":"Rate Detection","text":""},{"location":"api/features/rate_detection/#eitprocessing.features.rate_detection.RateDetection","title":"eitprocessing.features.rate_detection.RateDetection  <code>dataclass</code>","text":"<pre><code>RateDetection(\n    subject_type: Literal[\"adult\", \"neonate\"],\n    *,\n    welch_window: float = 30.0,\n    welch_overlap: float = 0.5,\n    min_heart_rate: float | None = None,\n    max_heart_rate: float | None = None,\n    min_respiratory_rate: float | None = None,\n    max_respiratory_rate: float | None = None,\n    refine_estimated_frequency: bool = True\n)\n</code></pre> <p>Detect the respiratory and heart rate from EIT pixel data.</p> <p>This algorithm attempts to detect the respiratory and heart rate from EIT pixel data. It is based on the observation that many high-amplitude pixels have the respiratory rate as the main frequency, while in low-amplitude pixels the power of the heart rate is relatively high. The algorithm uses Welch's method to estimate the power spectrum of the summed pixel data and individual pixels. It then identifies the respiratory rate as the frequency with the highest power for the summed pixels within the specified range. The power spectra of the individual pixels are normalized and averaged. The normalized power spectrum of the summed pixels is subtracted from the average of the normalized individual power spectra. The frequency with the highest relative power in this difference within the specified range is taken as the heart rate.</p> <p>If either rate is variable, the algorithm will in most cases return an average frequency. If there are multiple distinct frequencies, e.g., due to a change in the controlled respiratory rate, multiple peaks might be visible in the power spectrum. The algorithm will only return the frequency with the highest power in the specified range.</p> <p>The algorithm can't distinguish between the respiratory and heart rate if they are too close together, especially in very short signals (or when the Welch window is short). The algorithm can distinguish between both rates if the heart rate is at one of the harmonics of the respiratory rate.</p> <p>If the <code>refine_estimated_frequency</code> attribute is set to False, the estimated frequency is simply the location of the peak of the power Welch spectrum. Since Welch's method results in a limited number of frequency bins, this can lead to inaccuracies, especially with short or low-sample frequency data. If <code>refine_estimated_frequency</code> is set to True (default), the estimated frequency is refined using parabolic interpolation, which often yields more accurate results, even with short signals, low signal-to-noise ratio and very similar (but distinct) respiratory and heart rates. See e.g. Quadratic Interpolation of Spectral Peaks.</p> <p>The respiratory and heart rate limits can be set when initializing this algorithm. Default values for adults and neonates are set in <code>DEFAULT_RATE_LIMITS</code>.</p> <p>Although the algorithm might perform reasonably on data as short as 10 seconds (200 samples), it is recommended to use at least 30 seconds of data for reliable results. Longer data wil yield more reliable results, but only if the respiratory and heart rates are stable.</p> <p>Note that the algorithm performs best if no large changes in end-expiratory impedance occur in the data, the data is unfiltered, and the respiratory and heart rates are relatively stable.</p> ATTRIBUTE DESCRIPTION <code>subject_type</code> <p>The type of subject, either \"adult\" or \"neonate\". This affects the default settings for the minimum and maximum heart and respiratory rates.</p> <p> TYPE: <code>Literal['adult', 'neonate']</code> </p> <code>welch_window</code> <p>The length of the Welch window in seconds.</p> <p> TYPE: <code>float</code> </p> <code>welch_overlap</code> <p>The fraction overlap between Welch windows (e.g., 0.5 = 50% overlap).</p> <p> TYPE: <code>float</code> </p> <code>min_heart_rate</code> <p>The minimum heart rate in Hz. If None, the default value for the subject type is used.</p> <p> TYPE: <code>float</code> </p> <code>max_heart_rate</code> <p>The maximum heart rate in Hz. If None, the default value for the subject type is used.</p> <p> TYPE: <code>float</code> </p> <code>min_respiratory_rate</code> <p>The minimum respiratory rate in Hz. If None, the default value for the subject type is used.</p> <p> TYPE: <code>float</code> </p> <code>max_respiratory_rate</code> <p>The maximum respiratory rate in Hz. If None, the default value for the subject type is used.</p> <p> TYPE: <code>float</code> </p> <code>refine_estimated_frequency</code> <p>If True, the estimated frequency is refined using parabolic interpolation. If False, the frequency with the highest power is used as the estimated frequency.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/features/rate_detection/#eitprocessing.features.rate_detection.RateDetection.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: RateDetectionPlotting\n</code></pre> <p>A utility class for plotting the the results of the RateDetection algorithm.</p> <p>The <code>plotting.plot(**captures)</code> method can be used to plot the results of the algorithm. It takes the captured variables from the <code>apply</code> method as keyword arguments.</p> <p>Example: <pre><code>&gt;&gt;&gt; rd = RateDetection(\"adult\")\n&gt;&gt;&gt; captures = {}\n&gt;&gt;&gt; estimated_respiratory_rate, estimated_heart_rate = rd.detect_respiratory_heart_rate(eit_data, captures)\n&gt;&gt;&gt; fig = rd.plotting(**captures)\n&gt;&gt;&gt; fig.savefig(...)\n</code></pre></p>"},{"location":"api/features/rate_detection/#eitprocessing.features.rate_detection.RateDetection.apply","title":"apply","text":"<pre><code>apply(\n    eit_data: EITData,\n    *,\n    captures: dict | None = None,\n    suppress_length_warnings: bool = False,\n    suppress_edge_case_warning: bool = False\n) -&gt; tuple[float, float]\n</code></pre> <p>Detect respiratory and heart rate based on pixel data.</p> <p>NB: the respiratory and heart rate are returned in Hz. Multiply by 60 to convert to breaths/beats per minute.</p> PARAMETER DESCRIPTION <code>eit_data</code> <p>EITData object containing pixel impedance data and sample frequency.</p> <p> TYPE: <code>EITData</code> </p> <code>captures</code> <p>Optional dictionary to capture additional information during processing. Can be used for plotting or debugging purposes.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>suppress_length_warnings</code> <p>If True, suppress warnings about segment length being larger than the data length or overlap being larger than segment length. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>suppress_edge_case_warning</code> <p>If True, suppress warnings about the maximum power being at the edge of the frequency range, which prevents frequency refinement. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[float, float]</code> <p>A tuple containing the estimated respiratory rate and heart rate in Hz.</p>"},{"location":"api/filters/butterworth/","title":"Filters","text":""},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.LowPassFilter","title":"eitprocessing.filters.butterworth_filters.LowPassFilter  <code>dataclass</code>","text":"<pre><code>LowPassFilter(\n    *,\n    filter_type: Literal[\"lowpass\"] = \"lowpass\",\n    cutoff_frequency: float | tuple[float],\n    order: int,\n    sample_frequency: float,\n    ignore_max_order: InitVar[bool] = False\n)\n</code></pre> <p>Low-pass Butterworth filter for filtering in the time domain.</p> <p><code>LowPassFilter</code> is a convenience class similar to <code>ButterworthFilter</code>, where the <code>filter_type</code> is set to \"lowpass\".</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.LowPassFilter.apply","title":"apply","text":"<pre><code>apply(\n    input_data: ndarray, axis: int = -1, captures: dict | None = None\n) -&gt; ndarray\n</code></pre> <p>Apply the filter to the input data.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>Data to be filtered. If the input data has more than one axis, the filter is applied to the last axis.</p> <p> TYPE: <code>ndarray</code> </p> <code>axis</code> <p>Data axis the filter should be applied to. This defaults to the last axis, assuming this to be the time axis of the input data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> <code>captures</code> <p>Optional. A dictionary to capture intermediate date, useful for plotting and debugging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The filtered output with the same shape as the input data.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.LowPassFilter.apply_filter","title":"apply_filter","text":"<pre><code>apply_filter(*args, **kwargs) -&gt; ndarray\n</code></pre> <p>Deprecated method. Use <code>apply()</code> instead.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.HighPassFilter","title":"eitprocessing.filters.butterworth_filters.HighPassFilter  <code>dataclass</code>","text":"<pre><code>HighPassFilter(\n    *,\n    filter_type: Literal[\"highpass\"] = \"highpass\",\n    cutoff_frequency: float | tuple[float],\n    order: int,\n    sample_frequency: float,\n    ignore_max_order: InitVar[bool] = False\n)\n</code></pre> <p>High-pass Butterworth filter for filtering in the time domain.</p> <p><code>HighPassFilter</code> is a convenience class similar to <code>ButterworthFilter</code>, where the <code>filter_type</code> is set to \"highpass\".</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.HighPassFilter.apply","title":"apply","text":"<pre><code>apply(\n    input_data: ndarray, axis: int = -1, captures: dict | None = None\n) -&gt; ndarray\n</code></pre> <p>Apply the filter to the input data.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>Data to be filtered. If the input data has more than one axis, the filter is applied to the last axis.</p> <p> TYPE: <code>ndarray</code> </p> <code>axis</code> <p>Data axis the filter should be applied to. This defaults to the last axis, assuming this to be the time axis of the input data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> <code>captures</code> <p>Optional. A dictionary to capture intermediate date, useful for plotting and debugging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The filtered output with the same shape as the input data.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.HighPassFilter.apply_filter","title":"apply_filter","text":"<pre><code>apply_filter(*args, **kwargs) -&gt; ndarray\n</code></pre> <p>Deprecated method. Use <code>apply()</code> instead.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.BandStopFilter","title":"eitprocessing.filters.butterworth_filters.BandStopFilter  <code>dataclass</code>","text":"<pre><code>BandStopFilter(\n    *,\n    filter_type: Literal[\"bandstop\"] = \"bandstop\",\n    cutoff_frequency: float | tuple[float],\n    order: int,\n    sample_frequency: float,\n    ignore_max_order: InitVar[bool] = False\n)\n</code></pre> <p>Band-stop Butterworth filter for filtering in the time domain.</p> <p><code>BandStopFilter</code> is a convenience class similar to <code>ButterworthFilter</code>, where the <code>filter_type</code> is set to \"bandstop\".</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.BandStopFilter.apply","title":"apply","text":"<pre><code>apply(\n    input_data: ndarray, axis: int = -1, captures: dict | None = None\n) -&gt; ndarray\n</code></pre> <p>Apply the filter to the input data.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>Data to be filtered. If the input data has more than one axis, the filter is applied to the last axis.</p> <p> TYPE: <code>ndarray</code> </p> <code>axis</code> <p>Data axis the filter should be applied to. This defaults to the last axis, assuming this to be the time axis of the input data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> <code>captures</code> <p>Optional. A dictionary to capture intermediate date, useful for plotting and debugging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The filtered output with the same shape as the input data.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.BandStopFilter.apply_filter","title":"apply_filter","text":"<pre><code>apply_filter(*args, **kwargs) -&gt; ndarray\n</code></pre> <p>Deprecated method. Use <code>apply()</code> instead.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.BandPassFilter","title":"eitprocessing.filters.butterworth_filters.BandPassFilter  <code>dataclass</code>","text":"<pre><code>BandPassFilter(\n    *,\n    filter_type: Literal[\"bandpass\"] = \"bandpass\",\n    cutoff_frequency: float | tuple[float],\n    order: int,\n    sample_frequency: float,\n    ignore_max_order: InitVar[bool] = False\n)\n</code></pre> <p>Band-pass Butterworth filter for filtering in the time domain.</p> <p><code>BandPassFilter</code> is a convenience class similar to <code>ButterworthFilter</code>, where the <code>filter_type</code> is set to \"bandpass\".</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.BandPassFilter.apply","title":"apply","text":"<pre><code>apply(\n    input_data: ndarray, axis: int = -1, captures: dict | None = None\n) -&gt; ndarray\n</code></pre> <p>Apply the filter to the input data.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>Data to be filtered. If the input data has more than one axis, the filter is applied to the last axis.</p> <p> TYPE: <code>ndarray</code> </p> <code>axis</code> <p>Data axis the filter should be applied to. This defaults to the last axis, assuming this to be the time axis of the input data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> <code>captures</code> <p>Optional. A dictionary to capture intermediate date, useful for plotting and debugging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The filtered output with the same shape as the input data.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.BandPassFilter.apply_filter","title":"apply_filter","text":"<pre><code>apply_filter(*args, **kwargs) -&gt; ndarray\n</code></pre> <p>Deprecated method. Use <code>apply()</code> instead.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.ButterworthFilter","title":"eitprocessing.filters.butterworth_filters.ButterworthFilter  <code>dataclass</code>","text":"<pre><code>ButterworthFilter(\n    *,\n    filter_type: Literal[\"lowpass\", \"highpass\", \"bandpass\", \"bandstop\"],\n    cutoff_frequency: float | tuple[float],\n    order: int,\n    sample_frequency: float,\n    ignore_max_order: InitVar[bool] = False\n)\n</code></pre> <p>Butterworth filter for filtering in the time domain.</p> <p>Generates a low-pass, high-pass, band-pass or band-stop digital Butterworth filter of order <code>order</code>. Filters are created using cascaded second-order sections representation, providing better stability compared to the traditionally used transfer function (numerator/denominator or b/a representation).</p> <p>The <code>apply_filter()</code> method applies the filter to the provided data using forward-backward filtering. This minimizes the phase shift, and effectively doubles the order of the filter.</p> <p><code>ButterworthFilter</code> is a wrapper of the <code>scipy.butter()</code> and <code>scipy.filtfilt()</code> functions:     - https://docs.scipy.org/doc/scipy-1.10.1/reference/generated/scipy.signal.butter.html     - https://docs.scipy.org/doc/scipy-1.10.1/reference/generated/scipy.signal.filtfilt.html</p> PARAMETER DESCRIPTION <code>filter_type</code> <p>The type of filter to create: a low pass, high pass, band pass or band stop filter.</p> <p> TYPE: <code>Literal['lowpass', 'highpass', 'bandpass', 'bandstop']</code> </p> <code>cutoff_frequency</code> <p>Cutoff frequency or frequencies (in Hz). For low pass or high pass filters, <code>cutoff_frequency</code> is a scalar. For band pass or band stop filters, <code>cutoff_frequency</code> is a sequence containing two frequencies.</p> <p> TYPE: <code>float | tuple[float]</code> </p> <code>order</code> <p>Order of the filter. The effective order size is twice the given order, due to forward-backward filtering. Higher orders improve the effectiveness of a filter, but can result in unstable or incorrect filtering.</p> <p> TYPE: <code>int</code> </p> <code>sample_frequency</code> <p>The sample frequency of the data to be filtered (in Hz).</p> <p> TYPE: <code>float</code> </p> <code>ignore_max_order</code> <p>Whether to raise an exception if the order is larger than the maximum of 10. Defaults to False.</p> <p> TYPE: <code>InitVar[bool]</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = np.arange(0, 100, 0.1)\n&gt;&gt;&gt; signal = np.sin(t) + 0.1 * np.sin(10 * t)\n&gt;&gt;&gt; lowpass_filter = ButterworthFilter(\n...     filter_type='lowpass',\n...     cutoff_frequenct=45,\n...     order=4,\n...     sample_frequency=250\n... )\n&gt;&gt;&gt; filtered_signal = lowpass_filter.apply_filter(signal)\n</code></pre>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.ButterworthFilter.apply_filter","title":"apply_filter","text":"<pre><code>apply_filter(*args, **kwargs) -&gt; ndarray\n</code></pre> <p>Deprecated method. Use <code>apply()</code> instead.</p>"},{"location":"api/filters/butterworth/#eitprocessing.filters.butterworth_filters.ButterworthFilter.apply","title":"apply","text":"<pre><code>apply(\n    input_data: ndarray, axis: int = -1, captures: dict | None = None\n) -&gt; ndarray\n</code></pre> <p>Apply the filter to the input data.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>Data to be filtered. If the input data has more than one axis, the filter is applied to the last axis.</p> <p> TYPE: <code>ndarray</code> </p> <code>axis</code> <p>Data axis the filter should be applied to. This defaults to the last axis, assuming this to be the time axis of the input data.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> <code>captures</code> <p>Optional. A dictionary to capture intermediate date, useful for plotting and debugging.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The filtered output with the same shape as the input data.</p>"},{"location":"api/filters/mdn/","title":"MDN filter","text":""},{"location":"api/filters/mdn/#eitprocessing.filters.mdn.MDNFilter","title":"eitprocessing.filters.mdn.MDNFilter  <code>dataclass</code>","text":"<pre><code>MDNFilter(\n    *,\n    respiratory_rate: float,\n    heart_rate: float,\n    noise_frequency_limit: float = 220 / MINUTE,\n    notch_distance: float = 10 / MINUTE,\n    order: int = 10\n)\n</code></pre> <p>Multiple Digital Notch filter.</p> <p>This filter is used to remove heart rate noise from EIT data. A band stop filter removes heart rate \u00b1 the notch distance. This is repeated for every harmonic of the heart rate below the noise frequency limit. Lastly, a low pass filter removes noise above the noise frequency limit.</p> <p>By default, the notch distance is set to 0.166... Hz (10 BPM), and the noise frequency limit is set to 3.66... Hz (220 BPM).</p> Warning <p>The respiratory and heart rate should be in provided Hz, not BPM. We recommend defining <code>MINUTE = 60</code> and using, e.g., <code>heart_rate=80 / MINUTE</code> to manually set the heart rate to 80 BPM.</p> Warning <p>This filter was designed to remove heart rate noise from EIT data, and testing in a limited number of cases. The filter may not work as expected for other data types, different cohorts or non-traditional ventilation modes. Use at your own discretion.</p> PARAMETER DESCRIPTION <code>respiratory_rate</code> <p>the respiratory rate of the subject in Hz</p> <p> TYPE: <code>float</code> </p> <code>heart_rate</code> <p>the heart rate of the subject in Hz</p> <p> TYPE: <code>float</code> </p> <code>noise_frequency_limit</code> <p>the highest frequency to filter in Hz</p> <p> TYPE: <code>float</code> DEFAULT: <code>220 / MINUTE</code> </p> <code>notch_distance</code> <p>the half width of the band stop filter's frequency range</p> <p> TYPE: <code>float</code> DEFAULT: <code>10 / MINUTE</code> </p>"},{"location":"api/filters/mdn/#eitprocessing.filters.mdn.MDNFilter.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: FilterPlotting\n</code></pre> <p>Return the plotting class for this filter.</p>"},{"location":"api/filters/mdn/#eitprocessing.filters.mdn.MDNFilter.apply","title":"apply","text":"<pre><code>apply(\n    input_data: ndarray,\n    sample_frequency: float,\n    axis: int = 0,\n    captures: dict | None = None,\n) -&gt; ndarray\n</code></pre><pre><code>apply(\n    input_data: ContinuousData, captures: dict | None = None, **kwargs\n) -&gt; ContinuousData\n</code></pre><pre><code>apply(input_data: EITData, captures: dict | None = None, **kwargs) -&gt; EITData\n</code></pre> <pre><code>apply(\n    input_data: T,\n    sample_frequency: float | object = MISSING,\n    axis: int | object = MISSING,\n    captures: dict | None = None,\n    **kwargs\n) -&gt; T\n</code></pre> <p>Filter data using multiple digital notch filters.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>The data to filter. Can be a numpy array, ContinuousData, or EITData.</p> <p> TYPE: <code>T</code> </p> <code>sample_frequency</code> <p>The sample frequency of the data. Should be provided when using a numpy array. If using ContinuousData or EITData, this will be taken from the data object.</p> <p> TYPE: <code>float | object</code> DEFAULT: <code>MISSING</code> </p> <code>axis</code> <p>The axis along which to apply the filter. Should only be provided when using a numpy array. Defaults to the first axis (0).</p> <p> TYPE: <code>int | object</code> DEFAULT: <code>MISSING</code> </p> <code>captures</code> <p>A dictionary to capture intermediate results for debugging or analysis. If provided, it will store the number of harmonics and the frequency bands used for filtering.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the ContinuousData or EITData object (e.g., <code>label</code>).</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"api/plotting/filter/","title":"Filter","text":""},{"location":"api/plotting/filter/#eitprocessing.plotting.filter.FilterPlotting","title":"eitprocessing.plotting.filter.FilterPlotting  <code>dataclass</code>","text":"<pre><code>FilterPlotting()\n</code></pre> <p>Utility class for plotting the effects of frequency filtering.</p>"},{"location":"api/plotting/filter/#eitprocessing.plotting.filter.FilterPlotting.plot_results","title":"plot_results  <code>classmethod</code>","text":"<pre><code>plot_results(\n    *,\n    unfiltered_data: T,\n    filtered_data: T,\n    ax: Axes | None = None,\n    sample_frequency: float | object = MISSING,\n    high_pass_frequency: float | None = None,\n    low_pass_frequency: float | None = None,\n    frequency_bands: list[tuple[float, float]] | None = None,\n    xlim_to_max_filter_freq: float | None = 2,\n    **kwargs\n) -&gt; Axes\n</code></pre> <p>Plot frequency filtering results.</p> <p>This function is designed to work with the captures from a ButterworthFilter or MDNFilter.</p> <p>The plot shows the power spectra for the unfiltered and filtered data. If a high pass frequency or low pass frequency is provided, vertical lines are drawn at these frequencies. If frequency bands are provided, shaded regions are drawn for these bands.</p> <p>The provided data can be either numpy arrays, ContinuousData, or EITData, and must be the same for the unfiltered and filtered data. If numpy arrays are used, the sample frequency must be provided. If ContinuousData or EITData are used, the sample frequency is taken from the data itself.</p> <p>If an Axes instance is provided, the plot is drawn on that Axes. If not, a new figure and axes are created.</p> Axes scaling <p>The x-axis and y-axis can be scaled linearly or logarithmically using the <code>xscale</code> and <code>yscale</code> arguments. If not provided, <code>xscale</code> is set to \"linear\" and <code>yscale</code> is set to \"log\". If the x-scale is logarithmic, the default limits are used.</p> <p>If the x-scale is linear, the upper limit is determined by the highest filter frequency or the sample frequency. If xlim_to_max_filter_freq is None, the x-axis ranges from 0 to half the sample frequency. If a value is provided, the x-axis limit is set to the maximum frequency of all filters multiplied by this value. This enables focussing on the filtered frequencies. Custom limits can be set using the returned axes object.</p> <p>If the y-scale is linear, the default limits are used. If the y-scale is logarithmic, the axes are scaled such that the upper 95% of the data falls within the middle 80% of the axes. This ensures that very small outliers don't disproportionally scale the y-axis.</p> Warning <p>Although this function plots with the x-axis in beats per minute (BPM), the frequencies provided to it should be in Hz.</p> <p>Example: <pre><code>&gt;&gt;&gt; mdn = MDNFilter(...)\n&gt;&gt;&gt; captures = {}\n&gt;&gt;&gt; filtered_data = mdn.apply(impedance_data, captures=captures)\n&gt;&gt;&gt; ax = FilterResultsPlotting().plot(**captures)\n</code></pre></p> PARAMETER DESCRIPTION <code>unfiltered_data</code> <p>The original data before filtering. Can be a numpy array, ContinuousData, or EITData.</p> <p> TYPE: <code>T</code> </p> <code>filtered_data</code> <p>The data after filtering. Must be the same type as unfiltered_data.</p> <p> TYPE: <code>T</code> </p> <code>ax</code> <p>Optional. A matplotlib Axes instance to plot on. If None, a new figure and axes will be created.</p> <p> TYPE: <code>Axes | None</code> DEFAULT: <code>None</code> </p> <code>sample_frequency</code> <p>The sample frequency of the data. Must be provided only if input data are numpy arrays.</p> <p> TYPE: <code>float | object</code> DEFAULT: <code>MISSING</code> </p> <code>frequency_bands</code> <p>Optional. A list of tuples defining frequency bands to highlight in the plot. Each tuple contains (low, high) frequencies in Hz.</p> <p> TYPE: <code>list[tuple[float, float]] | None</code> DEFAULT: <code>None</code> </p> <code>high_pass_frequency</code> <p>Optional. The critical frequency for a high pass filter in Hz.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>low_pass_frequency</code> <p>Optional. The critical frequency for a low pass filter in Hz.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>xlim_to_max_filter_freq</code> <p>Optional. If provided, the upper x-axis limit will be set to the maximum frequency of all filters, times this value. If None, the x-axis is limited to half the sample frequency.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>2</code> </p> <code>**kwargs</code> <p>Extra keyword arguments to pass to the Axes.set() method, such as <code>xlabel</code>, <code>ylabel</code>, <code>title</code>, <code>yscale</code>, and <code>xscale</code>.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Axes</code> <p>A matplotlib Axes instance with the plot of the frequency filtering results.</p>"},{"location":"api/plotting/pixelmap/","title":"PixelMap","text":""},{"location":"api/plotting/pixelmap/#eitprocessing.plotting.pixelmap.PixelMapPlotting","title":"eitprocessing.plotting.pixelmap.PixelMapPlotting  <code>dataclass</code>","text":"<pre><code>PixelMapPlotting(pixel_map: PixelMap | PixelMask)\n</code></pre> <p>Utility class for plotting pixel maps and masks.</p>"},{"location":"api/plotting/pixelmap/#eitprocessing.plotting.pixelmap.PixelMapPlotting.config","title":"config  <code>property</code>","text":"<pre><code>config: PixelMapPlotConfig\n</code></pre> <p>Plotting configuration for pixel maps and pixel masks.</p>"},{"location":"api/plotting/pixelmap/#eitprocessing.plotting.pixelmap.PixelMapPlotting.imshow","title":"imshow","text":"<pre><code>imshow(\n    colorbar: bool | None = None,\n    normalize: bool | None = None,\n    percentage: bool | None = None,\n    absolute: bool | None = None,\n    colorbar_kwargs: dict | None = None,\n    facecolor: ColorType | None = None,\n    hide_axes: bool | None = None,\n    **kwargs\n) -&gt; AxesImage\n</code></pre> <p>Display the pixel map or mask using <code>imshow</code>.</p> <p>This method is a wrapper around <code>matplotlib.pyplot.imshow</code> that provides convenient defaults and formatting options for displaying pixel maps and masks.</p> <p>Plotting configuration is taken from <code>plot_config</code>, unless overridden by explicit arguments. Any additional keyword arguments are merged with <code>plot_config.extra_kwargs</code> and passed to <code>matplotlib.pyplot.imshow</code>.</p> <p>If <code>colorbar</code> is True, a colorbar is added to the axes. If <code>normalize</code> is True, the pixel values are scaled by their maximum value before plotting. The appearance of the colorbar can be modified using <code>percentage</code> and <code>absolute</code> flags:</p> <ul> <li><code>percentage=True</code> displays the colorbar in percentage units (where 1.0 \u2192 100%).</li> <li><code>absolute=True</code> uses the absolute value of the data for color scaling and labeling.</li> </ul> <p>If the colormap has underflow or overflow colors (e.g., for negative values), the colorbar will extend accordingly. Additional arguments can be passed to control or override the appearance of the colorbar via <code>colorbar_kwargs</code>, which are passed directly to <code>matplotlib.pyplot.colorbar</code>.</p> <p>If <code>hide_axes</code> is True, the axis ticks and labels are hidden (but the axes remain visible to retain background styling such as facecolor).</p> <p>Additional keyword arguments are passed directly to <code>imshow</code>, allowing for full control over image rendering. Notably, you can pass an existing matplotlib Axes object using the <code>ax</code> keyword argument.</p> PARAMETER DESCRIPTION <code>colorbar</code> <p>Whether to display a colorbar.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>normalize</code> <p>Whether to scale by the maximum value.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>percentage</code> <p>Whether to display the colorbar values as a percentage.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>absolute</code> <p>Whether to display the colorbar using absolute values.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>colorbar_kwargs</code> <p>Additional arguments passed to <code>matplotlib.pyplot.colorbar</code>.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>facecolor</code> <p>Background color for the axes. If None, uses the facecolor of the PixelMap.</p> <p> TYPE: <code>ColorType | None</code> DEFAULT: <code>None</code> </p> <code>hide_axes</code> <p>Whether to hide the axes ticks and labels.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>ax</code> <p>Axes to plot on. If not provided, uses the current axes.</p> <p> TYPE: <code>Axes</code> </p> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>matplotlib.pyplot.imshow</code>.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>AxesImage</code> <p>The image object created by imshow.</p> <p> TYPE: <code>AxesImage</code> </p>"},{"location":"api/plotting/pixelmap/#eitprocessing.plotting.pixelmap.PixelMapPlotting.contour","title":"contour","text":"<pre><code>contour(**kwargs) -&gt; ContourSet\n</code></pre> <p>Create a contour plot of the pixel map or mask.</p> <p>This method uses <code>matplotlib.pyplot.contour</code> to create a contour plot of the pixel map values.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>Additional keyword arguments passed to <code>matplotlib.pyplot.contour</code>.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>QuadContourSet</code> <p>The contour set created by the contour function.</p> <p> TYPE: <code>ContourSet</code> </p>"},{"location":"api/plotting/pixelmap/#eitprocessing.plotting.pixelmap.PixelMapPlotting.surface","title":"surface","text":"<pre><code>surface(**kwargs) -&gt; Poly3DCollection\n</code></pre> <p>Create a 3D surface plot of the pixel map or mask.</p> <p>This method uses <code>matplotlib.pyplot.axes3d.plot_surface</code> to create a 3D surface plot of the pixel map values.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>Additional keyword arguments passed to <code>matplotlib.pyplot.axes3d.plot_surface</code>.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Axes3D</code> <p>The 3D axes object containing the surface plot.</p> <p> TYPE: <code>Poly3DCollection</code> </p>"},{"location":"api/plotting/pixelmap/#eitprocessing.plotting.pixelmap.PixelMapPlotting.add_region_markers","title":"add_region_markers","text":"<pre><code>add_region_markers(\n    ax: Axes, label_map: dict | None = None, **kwargs\n) -&gt; list[Text]\n</code></pre> <p>Add markers to an integer map plot.</p> <p>This methods adds text labels at the center of all pixels with the same label, assuming continuous regions where the center of mass is inside the region itself. If the regions are not continuous, the label is placed in the largest region. If the center of mass of the region is outside the region, the label position is determined by repeatedly eroding the region (using <code>scipy.ndimage.erosion</code>).</p> <p>By default, the text labels are the integer values converted to strings. You can customize labels by providing a dictionary <code>label_map</code>, mapping the integer values to custom labels.</p> Center of mass <p>For any regular shape, the center of mass is near the geometrical center of a region. However, for e.g. a C-shaped region, the center of mass might fall outside the region itself.</p> PARAMETER DESCRIPTION <code>ax</code> <p>The axes to add the markers to.</p> <p> TYPE: <code>Axes</code> </p> <code>label_map</code> <p>A dictionary mapping integer labels to string labels. If None, the integer label itself is used.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional keyword arguments passed to the text function, such as <code>color</code> and <code>fontsize</code>.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>list[Text]</code> <p>list[mpl.text.Text]: A list of text objects created for the labels.</p>"},{"location":"api/plotting/rate_detection/","title":"Rate Detection","text":""},{"location":"api/plotting/rate_detection/#eitprocessing.plotting.rate_detection.RateDetectionPlotting","title":"eitprocessing.plotting.rate_detection.RateDetectionPlotting","text":"<pre><code>RateDetectionPlotting(obj: RateDetection)\n</code></pre> <p>Utility class to plot the results of the RateDetection algorithm.</p> <p>This class can be accessed through the <code>plotting</code> attribute of the RateDetection class.</p> <p>Example: <pre><code>&gt;&gt;&gt; rd = RateDetection(\"adult\")\n&gt;&gt;&gt; captures = {}\n&gt;&gt;&gt; rr, hr = rd.apply(eit_data, captures=captures)\n&gt;&gt;&gt; fig = rd.plotting(**captures)\n&gt;&gt;&gt; fig.savefig(\"rate_detection.png\")  # Save the figure to a file\n</code></pre></p>"},{"location":"api/plotting/rate_detection/#eitprocessing.plotting.rate_detection.RateDetectionPlotting.plot","title":"plot","text":"<pre><code>plot(\n    *,\n    frequencies: ndarray,\n    normalized_total_power: ndarray,\n    average_normalized_pixel_power: ndarray,\n    diff_total_averaged_power: ndarray,\n    estimated_respiratory_rate: float,\n    estimated_heart_rate: float,\n    fig: None\n) -&gt; Figure\n</code></pre><pre><code>plot(\n    *,\n    frequencies: ndarray,\n    normalized_total_power: ndarray,\n    average_normalized_pixel_power: ndarray,\n    diff_total_averaged_power: ndarray,\n    estimated_respiratory_rate: float,\n    estimated_heart_rate: float,\n    fig: SubFigure\n) -&gt; SubFigure\n</code></pre><pre><code>plot(\n    *,\n    frequencies: ndarray,\n    normalized_total_power: ndarray,\n    average_normalized_pixel_power: ndarray,\n    diff_total_averaged_power: ndarray,\n    estimated_respiratory_rate: float,\n    estimated_heart_rate: float,\n    fig: Figure\n) -&gt; Figure\n</code></pre> <pre><code>plot(\n    *,\n    frequencies: ndarray,\n    normalized_total_power: ndarray,\n    average_normalized_pixel_power: ndarray,\n    diff_total_averaged_power: ndarray,\n    estimated_respiratory_rate: float,\n    estimated_heart_rate: float,\n    fig: Figure | SubFigure | None = None\n) -&gt; Figure | SubFigure\n</code></pre> <p>Plot the results of the RateDetection algorithm.</p> <p>This method is intended to be used after running the <code>detect_respiratory_heart_rate</code> method of the <code>RateDetection</code> class. It visualizes the frequency analysis results, including the normalized power spectrum, average normalized pixel spectrum, and the difference between the two.</p> <p>NB: Although the frequencies, rate limits and estimated rates are provided in Hz, they are visualized in breaths/beats per minute (bpm).</p> PARAMETER DESCRIPTION <code>frequencies</code> <p>Frequencies used in the analysis, in Hz.</p> <p> TYPE: <code>ndarray</code> </p> <code>normalized_total_power</code> <p>Normalized total power spectrum.</p> <p> TYPE: <code>ndarray</code> </p> <code>average_normalized_pixel_power</code> <p>Average normalized pixel power spectrum.</p> <p> TYPE: <code>ndarray</code> </p> <code>diff_total_averaged_power</code> <p>Difference between the normalized summed power spectra and the normalized total power.</p> <p> TYPE: <code>ndarray</code> </p> <code>estimated_respiratory_rate</code> <p>Estimated respiratory rate in Hz.</p> <p> TYPE: <code>float</code> </p> <code>estimated_heart_rate</code> <p>Estimated heart rate in Hz.</p> <p> TYPE: <code>float</code> </p> <code>fig</code> <p>Optional matplotlib Figure or SubFigure to plot on. If None, a new figure is created.</p> <p> TYPE: <code>Figure | SubFigure | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Figure | SubFigure</code> <p>A matplotlib figure or subfigure (if provided) containing the plots of the frequency analysis.</p>"},{"location":"api/roi/filterbysize/","title":"FilterROIBySize","text":""},{"location":"api/roi/filterbysize/#eitprocessing.roi.filter_by_size","title":"eitprocessing.roi.filter_by_size","text":""},{"location":"api/roi/filterbysize/#eitprocessing.roi.filter_by_size.FilterROIBySize","title":"FilterROIBySize  <code>dataclass</code>","text":"<pre><code>FilterROIBySize(\n    *,\n    min_region_size: int = DEFAULT_MIN_REGION_SIZE,\n    connectivity: Literal[1, 2] | ndarray = 1\n)\n</code></pre> <p>Class for labeling and selecting connected regions in a PixelMask.</p> <p>This dataclass identifies and labels regions of interest (ROIs) in a PixelMask. You can specify the minimum region size and the connectivity structure.</p> Connectivity <p>For 2D images, connectivity determines which pixels are considered neighbors when labeling regions. - 1-connectivity (also called 4-connectivity in image processing):         Only directly adjacent pixels (up, down, left, right) are considered neighbors. - 2-connectivity (also called 8-connectivity in image processing):         Both directly adjacent and diagonal pixels are considered neighbors.</p> <p>The default value is 1-connectivity.</p> <p>If a custom array is provided, it must be a boolean or integer array specifying the neighborhood structure. See the documentation for <code>scipy.ndimage.label</code>: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.label.html</p> PARAMETER DESCRIPTION <code>min_region_size</code> <p>Minimum number of pixels in a region for it to be considered an ROI. Defaults to 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_MIN_REGION_SIZE</code> </p> <code>connectivity</code> <p>Connectivity type or custom array. Defaults to 1.</p> <p> TYPE: <code>Literal[1, 2] | ndarray</code> DEFAULT: <code>1</code> </p>"},{"location":"api/roi/filterbysize/#eitprocessing.roi.filter_by_size.FilterROIBySize.apply","title":"apply","text":"<pre><code>apply(mask: PixelMask) -&gt; PixelMask\n</code></pre> <p>Identify connected regions in a PixelMask, filter them by size, and return a combined mask.</p> <p>This method:</p> <ol> <li>Converts the input PixelMask into a binary representation where all non-NaN values     are treated as part of a region and NaNs are excluded.</li> <li>Labels connected components using the specified connectivity structure.</li> <li>Keeps only those connected regions whose pixel count is greater than or equal     to <code>self.min_region_size</code>.</li> <li>Combines the remaining regions into a single PixelMask.</li> </ol> PARAMETER DESCRIPTION <code>mask</code> <p>Input mask where non-NaN pixels are considered valid region pixels. NaNs are treated as excluded/background.</p> <p> TYPE: <code>PixelMask</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A new PixelMask representing the union of all regions that meet the <code>min_region_size</code> criterion.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If no connected regions meet the size threshold (e.g., mask is empty, all regions are too small, or connectivity is too restrictive).</p>"},{"location":"api/roi/pixelmaskcollection/","title":"PixelMaskCollection","text":""},{"location":"api/roi/pixelmaskcollection/#eitprocessing.roi.pixelmaskcollection","title":"eitprocessing.roi.pixelmaskcollection","text":""},{"location":"api/roi/pixelmaskcollection/#eitprocessing.roi.pixelmaskcollection.PixelMaskCollection","title":"PixelMaskCollection  <code>dataclass</code>","text":"<pre><code>PixelMaskCollection(\n    masks: (\n        dict[Hashable, PixelMask]\n        | frozendict[Hashable, PixelMask]\n        | list[PixelMask]\n        | None\n    ) = None,\n    *,\n    label: str | None = None\n)\n</code></pre> <p>A collection of pixel masks, each representing a specific region of interest (ROI) in the EIT data.</p> <p>This class allows for the application of multiple masks to numpy arrays, EIT data or pixel maps, enabling the extraction of specific regions based on the defined masks.</p> <p>PixelMasks can be labelled with strings, or anonymous (<code>label=None</code>). Either all or none of the masks should have a label; it is not possible to mix labelled and unlabelled masks.</p> <p>At initialization, masks can be provided as a list or a dictionary. If provided as a list, the masks are converted to a dictionary with either their labels as keys (if all masks have labels) or their indices as keys (if none have labels). If provided as a dictionary, the keys must match the labels of the masks.</p> <p>When the <code>apply</code> method is called, it applies each mask to the provided data and returns a dictionary mapping each mask's key (label or index) to the resulting masked data.</p> <p>Example: <pre><code>&gt;&gt;&gt; mask_collection = PixelMaskCollection(\n        [\n            PixelMask(right_lung_data, label=\"right lung\"),\n            PixelMask(left_lung_data, label=\"left lung\"),\n        ],\n        label=\"lung masks\",\n    )\n</code></pre></p> PARAMETER DESCRIPTION <code>masks</code> <p>A dictionary mapping mask names to their corresponding pixel masks or a list of masks.</p> <p> TYPE: <code>dict | list</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>An optional label for the collection of masks.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> ATTRIBUTE DESCRIPTION <code>is_anonymous</code> <p>A boolean indicating whether all masks in the collection are anonymous (i.e., have no label).</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/roi/pixelmaskcollection/#eitprocessing.roi.pixelmaskcollection.PixelMaskCollection.is_anonymous","title":"is_anonymous  <code>property</code>","text":"<pre><code>is_anonymous: bool\n</code></pre> <p>Check if all masks in the collection are anonymous (i.e., have no label).</p>"},{"location":"api/roi/pixelmaskcollection/#eitprocessing.roi.pixelmaskcollection.PixelMaskCollection.add","title":"add","text":"<pre><code>add(*mask: PixelMask, overwrite: bool = False, **kwargs) -&gt; Self\n</code></pre> <p>Return a new collection with one or more masks added.</p> <p>Masks can be added as positional arguments or keyword arguments (non-anonymous masks only).</p> <p>Example: <pre><code>&gt;&gt;&gt; left_lung_mask = PixelMask(data, label=\"left_lung\")\n&gt;&gt;&gt; mask_collection.add(left_lung_mask)  # equals mask_collection.add(left_lung=left_lung_mask)\n</code></pre></p> PARAMETER DESCRIPTION <code>*mask</code> <p>One or more <code>PixelMask</code> instances to add to the collection.</p> <p> TYPE: <code>PixelMask</code> DEFAULT: <code>()</code> </p> <code>overwrite</code> <p>If True, allows overwriting existing masks with the same label or index.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Keyword arguments where the key is the label of the mask and the value is the mask itself.</p> <p> TYPE: <code>PixelMask</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new <code>PixelMaskCollection</code> instance with the added masks.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If no masks are provided, if trying to mix labelled and unlabelled masks, or if provided keys don't match the masks label.</p> <code>KeyError</code> <p>If trying to overwrite an existing mask without setting <code>overwrite</code>.</p>"},{"location":"api/roi/pixelmaskcollection/#eitprocessing.roi.pixelmaskcollection.PixelMaskCollection.combine","title":"combine","text":"<pre><code>combine(\n    method: Literal[\"sum\", \"product\"] = \"sum\", label: str | None = None\n) -&gt; PixelMask\n</code></pre> <p>Combine all masks in the collection into a single mask using the specified method.</p> <p>The method can be either \"sum\" or \"product\". If \"sum\" is used, the masks are summed, generally resulting in the union of the represented ROIs. If \"product\" is used, the masks are multiplied, generally resulting in the intersection of the ROIs.</p> <p>Example: <pre><code>&gt;&gt;&gt; pm1 = PixelMask([[True, False], [False, True]])\n&gt;&gt;&gt; pm2 = PixelMask([[True, True], [False, False]])\n&gt;&gt;&gt; collection = PixelMaskCollection([pm1, pm2])\n&gt;&gt;&gt; summed_mask = collection.combine(method=\"sum\")\nPixelMask(mask=array([[ 1.,  1.], [nan,  1.]]))\n</code></pre></p> PARAMETER DESCRIPTION <code>method</code> <p>The method to combine the masks. Defaults to \"sum\".</p> <p> TYPE: <code>Literal['sum', 'product']</code> DEFAULT: <code>'sum'</code> </p> <code>label</code> <p>The label for the combined mask.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A new <code>PixelMask</code> instance representing the combined mask.</p> <p> TYPE: <code>PixelMask</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the PixelMaskCollection is empty.</p> <code>ValueError</code> <p>If an unsupported method is provided.</p>"},{"location":"api/roi/pixelmaskcollection/#eitprocessing.roi.pixelmaskcollection.PixelMaskCollection.apply","title":"apply","text":"<pre><code>apply(data: ndarray) -&gt; dict[Hashable, ndarray]\n</code></pre><pre><code>apply(\n    data: EITData, *, label_format: str | None = None, **kwargs\n) -&gt; dict[Hashable, EITData]\n</code></pre><pre><code>apply(\n    data: PixelMap, *, label_format: str | None = None, **kwargs\n) -&gt; dict[Hashable, PixelMap]\n</code></pre> <pre><code>apply(data, *, label_format: str | None = None, **kwargs)\n</code></pre> <p>Apply the masks to the provided data.</p> <p>The input data can be a numpy array, EITData, or PixelMap. The method applies each mask in the collection to the data and returns a dictionary mapping each mask's key (label or index) to the resulting masked data.</p> <p>If <code>label_format</code> is provided, it should be a format string that includes <code>{mask_label}</code>. This label will be passed to the resulting objects, with the appropriate mask label applied. If <code>label_format</code> is not provided, no label will be provided.</p> <p>Additional keyword arguments are passed to the <code>update</code> of the EITData or PixelMap, if applicable. If the input data is a numpy array, <code>label_format</code> and additional keyword arguments are not applicable and will raise a <code>ValueError</code>.</p> <p>Example: <pre><code>&gt;&gt;&gt; mask_collection = PixelMaskCollection([\n        PixelMask(data, label=\"right lung\"),\n        PixelMask(data, label=\"left lung\")\n    ])\n&gt;&gt;&gt; mask_collection.apply(eit_data, label_format=\"masked {mask_label}\")\n{\"right lung\": EITData(label=\"masked right lung\"), \"left lung\": EITData(label=\"masked left lung\")}\n</code></pre></p> PARAMETER DESCRIPTION <code>data</code> <p>The data to which the masks will be applied.</p> <p> TYPE: <code>ndarray | EITData | PixelMap</code> </p> <code>label_format</code> <p>A format string to create the label of the returned EITData or PixelMap objects.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>update</code> method of the returned EITData or PixelMap objects.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <p>A dictionary mapping each mask's key (label or index) to the resulting masked data.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the PixelMaskCollection is empty.</p> <code>ValueError</code> <p>If a label is passed as a keyword argument.</p> <code>ValueError</code> <p>If label_format or additional keyword arguments are provided when the input data is a numpy array.</p> <code>ValueError</code> <p>If provided label format does not contain '{mask_label}'.</p> <code>TypeError</code> <p>If provided data is not an array, EITData, or PixelMap.</p>"},{"location":"api/roi/roi/","title":"Regions of Interest","text":""},{"location":"api/roi/roi/#eitprocessing.roi","title":"eitprocessing.roi","text":"<p>Region of Interest selection and pixel masking.</p> <p>This module contains tools for the selection of regions of interest and masking pixel data. The central class of this module is <code>PixelMask</code>. Any type of region of interest selection results in a <code>PixelMask</code> object. A mask can be applied to any pixel dataset (EITData, PixelMap) with the same shape.</p> <p>Several default masks have been predefined. NB: the right side of the patient is to the left side of the EIT image and vice versa.</p> <ul> <li><code>VENTRAL_MASK</code> includes only the first 16 rows;</li> <li><code>DORSAL_MASK</code> includes only the last 16 rows;</li> <li><code>ANATOMICAL_RIGHT_MASK</code> includes only the first 16 columns;</li> <li><code>ANATOMICAL_LEFT_MASK</code> includes only the last 16 columns;</li> <li><code>QUADRANT_1_MASK</code> includes the top right quadrant;</li> <li><code>QUADRANT_2_MASK</code> includes the top left quadrant;</li> <li><code>QUADRANT_3_MASK</code> includes the bottom right quadrant;</li> <li><code>QUADRANT_4_MASK</code> includes the bottom left quadrant;</li> <li><code>LAYER_1_MASK</code> includes only the first 8 rows;</li> <li><code>LAYER_2_MASK</code> includes only the second set of 8 rows;</li> <li><code>LAYER_3_MASK</code> includes only the third set of 8 rows;</li> <li><code>LAYER_4_MASK</code> includes only the last 8 rows.</li> </ul>"},{"location":"api/roi/roi/#eitprocessing.roi.PixelMask","title":"PixelMask  <code>dataclass</code>","text":"<pre><code>PixelMask(\n    mask: list | ndarray,\n    *,\n    label: str | None = None,\n    keep_zeros: bool = False,\n    suppress_value_range_error: bool = False,\n    suppress_zero_conversion_warning: bool = False,\n    suppress_all_nan_warning: bool = False,\n    plot_config: PixelMapPlotConfig | dict | None = None\n)\n</code></pre> <p>Mask pixels by selecting or weighing them individually.</p> <p>A mask is a 2D array with a value for each pixel. Most often, this value is NaN (<code>np.nan</code>, 'not a number') or 1, and less commonly a value in the range 0 to 1. NaN values indicate the pixel is not part of the region of interest, e.g., falls outside the functional lung space or is not part of the ventral region of the lung. A value of 1 indicates the pixel is included in the region of interest. A value between 0 and 1 indicates that the pixel is part of the region of interest, but is weighted, e.g., for a weighted summation of pixel values, or because the pixel is considered part of multiple regions of interest.</p> <p>You can initialize a mask using an array or nested list. At initialization, the mask is converted to a floating point numpy array.</p> <p>By default, 0-values are converted tot NaN. You can override this behaviour with <code>keep_zeros=True</code>. You can therefore create a mask by supplying boolean values, where <code>True</code> indicates the pixel is part of the region of interest (<code>True</code> equals 1), and <code>False</code> indicates it is not (<code>False</code> equals 0, and will by default be converted to NaN).</p> <p>Since masking is not intended for other operations, masking values that are negative or higher than 1 will result in a <code>ValueError</code>. You can override this check with <code>suppress_value_range_error=True</code>.</p> <p>A mask can be applied to any pixel dataset, such as an <code>EITData</code> object or a <code>PixelMap</code> object. The mask is applied to the last two dimensions of the data, which must match the shape of the mask. The mask is applied by multiplying each pixel in the dataset by the corresponding masking value. Multiplication by NaN always results in NaN.</p> <p>Adding, subtracting and multiplying:     Masks can be combined by adding, subtracting or multiplying them.</p> <pre><code>Adding masks results in a mask that includes\nall pixels that are in either mask. For non-weighted masks this is similar to a union of sets. Weighted pixels\nare added and clipped at 1.\n\nSubtracting masks results in the pixels that are part of the second mask being removed from the first mask. For\nnon-weighted masks this is similar to a set difference. For weighted masks, pixel values are clipped at 0.\n\nMultiplying masks\nresults in a mask that includes only pixels that are in both masks. For non-weighted masks this is similar to an\nintersection of sets.\n</code></pre> <p>Example: <pre><code>&gt;&gt;&gt; assert VENTRAL_MASK * ANATOMICAL_RIGHT_MASK == QUADRANT_1_MASK\nTrue  # quadrant 1 is the ventral part of the right lung\n&gt;&gt;&gt; assert DORSAL_MASK * ANATOMICAL_LEFT_MASK == QUADRANT_4_MASK\nTrue  # quadrant 4 is the dorsal part of the left lung\n</code></pre></p>"},{"location":"api/roi/roi/#eitprocessing.roi.PixelMask.values","title":"values  <code>property</code>","text":"<pre><code>values: ndarray\n</code></pre> <p>Alias for <code>mask</code>.</p>"},{"location":"api/roi/roi/#eitprocessing.roi.PixelMask.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Shape of the mask.</p>"},{"location":"api/roi/roi/#eitprocessing.roi.PixelMask.plotting","title":"plotting  <code>property</code>","text":"<pre><code>plotting: PixelMapPlotting\n</code></pre> <p>Get the plotting configuration for this mask.</p> RETURNS DESCRIPTION <code>PixelMapPlotting</code> <p>The plotting configuration for this mask.</p> <p> TYPE: <code>PixelMapPlotting</code> </p>"},{"location":"api/roi/roi/#eitprocessing.roi.PixelMask.is_weighted","title":"is_weighted  <code>property</code>","text":"<pre><code>is_weighted: bool\n</code></pre> <p>Whether the mask multiplies any pixels with a number other than NaN or 1.</p>"},{"location":"api/roi/roi/#eitprocessing.roi.PixelMask.apply","title":"apply","text":"<pre><code>apply(data: ndarray) -&gt; ndarray\n</code></pre><pre><code>apply(data: EITData, **kwargs) -&gt; EITData\n</code></pre><pre><code>apply(data: PixelMap, **kwargs) -&gt; PixelMap\n</code></pre> <pre><code>apply(data, **kwargs)\n</code></pre> <p>Apply pixel mask to data, returning a copy of the object with pixel values masked.</p> <p>Data can be a numpy array, an EITData object or PixelMap object. In case of an EITData object, the mask will be applied to the <code>pixel_impedance</code> attribute. In case of a PixelMap, the mask will be applied to the <code>values</code> attribute.</p> <p>The input data can have any dimension. The mask is applied to the last two dimensions. The size of the last two dimensions must match the size of the dimensions of the mask, and will generally (but do not have to) have the length 32.</p> <p>The function returns the same data type as <code>data</code>. In case of <code>EITData</code> or <code>PixelMap</code> data, the object will have the provided label, or the original data label if none is provided.</p>"},{"location":"api/roi/tivlungspace/","title":"TIV and amplitude lung space","text":""},{"location":"api/roi/tivlungspace/#eitprocessing.roi.tiv.TIVLungspace","title":"eitprocessing.roi.tiv.TIVLungspace  <code>dataclass</code>","text":"<pre><code>TIVLungspace(*, threshold: float = 0.15)\n</code></pre> <p>Create a pixel mask by thresholding the mean TIV.</p> <p>This defines the functional lung space as all pixels with a tidal impedance variation (TIV) of at least the provided fractional threshold of the maximum TIV.</p> <p>Example usage: <pre><code>&gt;&gt;&gt; mask = TIVLungspace(threshold=0.15).apply(eit_data)\n&gt;&gt;&gt; masked_eit_data = mask.apply(eit_data)\n</code></pre></p> PARAMETER DESCRIPTION <code>threshold</code> <p>The fraction of the maximum TIV that is used as threshold. Defaults to 0.15 (15%).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.15</code> </p>"},{"location":"api/roi/tivlungspace/#eitprocessing.roi.tiv.TIVLungspace.apply","title":"apply","text":"<pre><code>apply(\n    eit_data: EITData,\n    *,\n    timing_data: ContinuousData | None = None,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Apply the TIV thresholding to the EIT data.</p> <p><code>BreathDetection</code> is used to find breaths in timing data. By default, the timing data is the summed pixel impedance. Alternative timing data, e.g., pressure data, can be provided.</p> <p>Then, <code>TIV</code> is used to compute the TIV for each breath. The mean TIV over all breaths is computed, and pixels with a mean TIV above the threshold are included in the mask.</p> PARAMETER DESCRIPTION <code>eit_data</code> <p>The EIT data to process.</p> <p> TYPE: <code>EITData</code> </p> <code>timing_data</code> <p>Optionally, alternative continuous data to use for breath detection. If <code>None</code>, the summed pixel impedance is used. Defaults to <code>None</code>.</p> <p> TYPE: <code>ContinuousData | None</code> DEFAULT: <code>None</code> </p> <code>captures</code> <p>A dictionary to store intermediate results. If <code>None</code>, no intermediate results are stored. Defaults to <code>None</code>.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/roi/tivlungspace/#eitprocessing.roi.amplitude.AmplitudeLungspace","title":"eitprocessing.roi.amplitude.AmplitudeLungspace  <code>dataclass</code>","text":"<pre><code>AmplitudeLungspace(*, threshold: float = 0.15)\n</code></pre> <p>Create a pixel mask by thresholding the mean amplitude.</p> <p>This defines the functional lung space as all pixels with an amplitude of at least the provided fractional threshold of the maximum amplitude.</p> Warning <p>A lung space based on amplitude is not recommended, as it potentially includes reconstruction artifacts. The option is provided for completeness and use in other algorithms, namely WatershedLungspace.</p> <p>Example usage: <pre><code>&gt;&gt;&gt; mask = AmplitudeLungspace(threshold=0.15).apply(eit_data)\n&gt;&gt;&gt; masked_eit_data = mask.apply(eit_data)\n</code></pre></p> PARAMETER DESCRIPTION <code>threshold</code> <p>The fraction of the maximum amplitude that is used as threshold. Defaults to 0.15 (15%).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.15</code> </p>"},{"location":"api/roi/tivlungspace/#eitprocessing.roi.amplitude.AmplitudeLungspace.apply","title":"apply","text":"<pre><code>apply(\n    eit_data: EITData,\n    *,\n    timing_data: ContinuousData | None = None,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Apply the amplitude thresholding to the EIT data.</p> <p><code>BreathDetection</code> is used to find breaths in timing data. By default, the timing data is the summed pixel impedance. Alternative timing data, e.g., pressure data, can be provided.</p> <p>Then, <code>TIV</code> is used to compute the amplitude for each breath. The mean amplitude over all breaths is computed, and pixels with a mean amplitude above the threshold are included in the mask.</p> PARAMETER DESCRIPTION <code>eit_data</code> <p>The EIT data to process.</p> <p> TYPE: <code>EITData</code> </p> <code>timing_data</code> <p>Optionally, alternative continuous data to use for breath detection. If <code>None</code>, the summed pixel impedance is used. Defaults to <code>None</code>.</p> <p> TYPE: <code>ContinuousData | None</code> DEFAULT: <code>None</code> </p> <code>captures</code> <p>A dictionary to store intermediate results. If <code>None</code>, no intermediate results are stored. Defaults to <code>None</code>.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/roi/watershed/","title":"Watershed lung space","text":""},{"location":"api/roi/watershed/#eitprocessing.roi.watershed.WatershedLungspace","title":"eitprocessing.roi.watershed.WatershedLungspace  <code>dataclass</code>","text":"<pre><code>WatershedLungspace(*, threshold_fraction: float = 0.15)\n</code></pre> <p>Create a pixel mask based on the watershed method.</p> <p>This method was designed to improve functional lung space detection in the presence of pendelluft. Functional lung space defined as pixels with a tidal impedance variation (TIV) at or above a percentage of the maximum TIV can result in underdetection of pixels that have reduced TIV due to the pendelluft phenomenon. An alternate approach using the pixel amplitude instead of the TIV results in overinclusion of pixels.</p> Watershed regions <p>The concept of watershed regions come from geography, where it refers to the area of land that drains into a single river or body of water. In the context of image processing, it refers to the region of an image that is associated with a particular local minimum. The region borders are defined by the 'ridges' between the local minima. The inverse watershed method uses maxima and 'valleys' between them. This algorithm uses the inverse method, where local maximum impedance values form the centers of watershed regions.</p> <p>In this method, the inverse watershed method is applied to the pixel amplitude. This results in distinct regions with high values for more central pixels, and low values for edge pixels. Regions where the highest value falls within the TIV-based functional lung space definition are included in the mask. Other regions are excluded. Pixels that fall outside the amplitude based functional lung space definition are excluded.</p> <p>Example usage: <pre><code>&gt;&gt;&gt; mask = WatershedLungspace(threshold_fraction=0.15).apply(eit_data)\n&gt;&gt;&gt; masked_eit_data = mask.apply(eit_data)\n</code></pre></p> PARAMETER DESCRIPTION <code>threshold_fraction</code> <p>The fraction of the maximum TIV for the initial functional lung space definition that is used in the algorithm. Defaults to 0.15 (15%).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.15</code> </p>"},{"location":"api/roi/watershed/#eitprocessing.roi.watershed.WatershedLungspace.apply","title":"apply","text":"<pre><code>apply(\n    eit_data: EITData,\n    *,\n    timing_data: ContinuousData | None = None,\n    captures: dict | None = None\n) -&gt; PixelMask\n</code></pre> <p>Apply the watershed method to the EIT data.</p> <p>TIVLungspace is used to gather the mean TIV and mean amplitude maps, and their associated functional lung space masks. The lung space masks are redefined here to ensure that the same breaths are used for both TIV and amplitude.</p> <p>Local peaks in the amplitude map are found using <code>skimage.feature.peak_local_max</code>. These peaks are used to find the (inverse) watershed regions in the amplitude map. Regions whose peaks fall inside the functional TIV mask are included, the others are excluded. The final watershed mask is the intersection of the functional amplitude mask and the remaining included watershed regions.</p> PARAMETER DESCRIPTION <code>eit_data</code> <p>The EIT data to apply the watershed method to.</p> <p> TYPE: <code>EITData</code> </p> <code>timing_data</code> <p>The timing data to use for the TIV calculation. If None, the summed impedance is used.</p> <p> TYPE: <code>ContinuousData | None</code> DEFAULT: <code>None</code> </p> <code>captures</code> <p>An optional dictionary to capture intermediate results. If None, no captures are made.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PixelMask</code> <p>A mask of the functional lung space based on the watershed method.</p> <p> TYPE: <code>PixelMask</code> </p>"},{"location":"examples/notebooks/minidemo/","title":"Minidemo","text":"In\u00a0[3]: Copied! <pre>from pathlib import Path\n\nfrom matplotlib import pyplot as plt\n\nfrom eitprocessing.datahandling.continuousdata import ContinuousData\nfrom eitprocessing.datahandling.loading import load_eit_data\n\n\ndef plot_gi(data: ContinuousData) -&gt; None:\n    \"\"\"Plot the data over time.\"\"\"\n    t0 = data.time[0]\n    plt.plot(data.time - t0, data)\n</pre> from pathlib import Path  from matplotlib import pyplot as plt  from eitprocessing.datahandling.continuousdata import ContinuousData from eitprocessing.datahandling.loading import load_eit_data   def plot_gi(data: ContinuousData) -&gt; None:     \"\"\"Plot the data over time.\"\"\"     t0 = data.time[0]     plt.plot(data.time - t0, data) In\u00a0[4]: Copied! <pre>data_directory = Path.cwd() / \"tests\" / \"test_data\"\ndata_path = Path(data_directory) / \"Draeger_Test.bin\"\nsequence = load_eit_data(data_path, vendor=\"draeger\")\n</pre> data_directory = Path.cwd() / \"tests\" / \"test_data\" data_path = Path(data_directory) / \"Draeger_Test.bin\" sequence = load_eit_data(data_path, vendor=\"draeger\") In\u00a0[5]: Copied! <pre>gi = sequence.continuous_data[\"global_impedance_(raw)\"]\nplot_gi(gi)\n</pre> gi = sequence.continuous_data[\"global_impedance_(raw)\"] plot_gi(gi) In\u00a0[6]: Copied! <pre>t0 = sequence.time[0]\nshorter_sequence = sequence.select_by_time(t0 + 800, t0 + 830)\nstable_gi = shorter_sequence.continuous_data[\"global_impedance_(raw)\"]\n\nplot_gi(stable_gi)\n</pre> t0 = sequence.time[0] shorter_sequence = sequence.select_by_time(t0 + 800, t0 + 830) stable_gi = shorter_sequence.continuous_data[\"global_impedance_(raw)\"]  plot_gi(stable_gi) In\u00a0[7]: Copied! <pre>from eitprocessing.filters.butterworth_filters import LowPassFilter\n\nMINUTE = 60  # seconds\n\nlp_filter = LowPassFilter(\n    cutoff_frequency=40 / MINUTE,\n    order=10,\n    sample_frequency=sequence.eit_data[\"raw\"].framerate,\n)\n\nfiltered_gi = stable_gi.derive(\n    label=\"global_impedance_(lowpass)\",\n    function=lp_filter.apply_filter,\n    func_args={},\n)\n\nshorter_sequence.continuous_data.add(filtered_gi)\n\nplot_gi(stable_gi)\nplot_gi(filtered_gi)\n</pre> from eitprocessing.filters.butterworth_filters import LowPassFilter  MINUTE = 60  # seconds  lp_filter = LowPassFilter(     cutoff_frequency=40 / MINUTE,     order=10,     sample_frequency=sequence.eit_data[\"raw\"].framerate, )  filtered_gi = stable_gi.derive(     label=\"global_impedance_(lowpass)\",     function=lp_filter.apply_filter,     func_args={}, )  shorter_sequence.continuous_data.add(filtered_gi)  plot_gi(stable_gi) plot_gi(filtered_gi) In\u00a0[8]: Copied! <pre>filtered_gi.derived_from\n</pre> filtered_gi.derived_from Out[8]: <pre>[EITData(label='raw'),\n ContinuousData(label='global_impedance_(raw)'),\n ContinuousData(label='global_impedance_(raw)')]</pre>"},{"location":"examples/notebooks/minidemo/#initialize","title":"Initialize\u00b6","text":"<p>Import libraries and create a basic plotting function</p>"},{"location":"examples/notebooks/minidemo/#load-data","title":"Load data\u00b6","text":""},{"location":"examples/notebooks/minidemo/#visualize-the-global-impedance","title":"Visualize the global impedance\u00b6","text":""},{"location":"examples/notebooks/minidemo/#select-a-stable-period","title":"Select a stable period\u00b6","text":""},{"location":"examples/notebooks/minidemo/#perform-lowpass-filtering-on-this-subsequence","title":"Perform lowpass filtering on this subsequence\u00b6","text":""},{"location":"examples/notebooks/minidemo/#visualize-traceback-of-derivation","title":"Visualize traceback of derivation\u00b6","text":""},{"location":"examples/notebooks/test_parameter_eeli_/","title":"Test parameter eeli","text":"In\u00a0[1]: Copied! <pre>from matplotlib import pyplot as plt\n\nfrom eitprocessing.datahandling.continuousdata import ContinuousData\nfrom eitprocessing.datahandling.loading import load_eit_data\nfrom eitprocessing.filters.butterworth_filters import LowPassFilter\nfrom eitprocessing.parameters.eeli import EELI\n\nMINUTE = 60\n</pre> from matplotlib import pyplot as plt  from eitprocessing.datahandling.continuousdata import ContinuousData from eitprocessing.datahandling.loading import load_eit_data from eitprocessing.filters.butterworth_filters import LowPassFilter from eitprocessing.parameters.eeli import EELI  MINUTE = 60 <p>Load a sequence from file.</p> In\u00a0[2]: Copied! <pre>sequence = load_eit_data(\"tests/test_data/Draeger_Test.bin\", vendor=\"draeger\")\n</pre> sequence = load_eit_data(\"tests/test_data/Draeger_Test.bin\", vendor=\"draeger\") <p>Filter the global impedance using a low pass filter.</p> In\u00a0[3]: Copied! <pre>gi = sequence.continuous_data[\"global_impedance_(raw)\"]\n\nfilter_params = {\n    \"sample_frequency\": sequence.eit_data[\"raw\"].sample_frequency,\n    \"cutoff_frequency\": 50 / MINUTE,\n    \"order\": 10,\n}\nfilter_ = LowPassFilter(**filter_params)\n\nsequence.continuous_data.add(\n    ContinuousData(\n        \"global_impedance_(lowpass)\",\n        \"Global impedance (low pass filtered)\",\n        \"a.u.\",\n        \"impedance\",\n        derived_from=[*gi.derived_from, gi],\n        parameters={LowPassFilter: filter_params},\n        time=gi.time,\n        values=filter_.apply_filter(gi.values),\n        sample_frequency=gi.sample_frequency,\n    ),\n)\n</pre> gi = sequence.continuous_data[\"global_impedance_(raw)\"]  filter_params = {     \"sample_frequency\": sequence.eit_data[\"raw\"].sample_frequency,     \"cutoff_frequency\": 50 / MINUTE,     \"order\": 10, } filter_ = LowPassFilter(**filter_params)  sequence.continuous_data.add(     ContinuousData(         \"global_impedance_(lowpass)\",         \"Global impedance (low pass filtered)\",         \"a.u.\",         \"impedance\",         derived_from=[*gi.derived_from, gi],         parameters={LowPassFilter: filter_params},         time=gi.time,         values=filter_.apply_filter(gi.values),         sample_frequency=gi.sample_frequency,     ), ) <p>Select a small portion of the sequence.</p> In\u00a0[4]: Copied! <pre>sequence = sequence.t[56600:56650]\n</pre> sequence = sequence.t[56600:56650] <p>Determine the EELI of the selected portion.</p> In\u00a0[5]: Copied! <pre>cd = sequence.continuous_data[\"global_impedance_(raw)\"]\ncd_filtered = sequence.continuous_data[\"global_impedance_(lowpass)\"]\neeli_result = EELI().compute_parameter(cd)\neeli_result_filtered = EELI().compute_parameter(cd_filtered)\n</pre> cd = sequence.continuous_data[\"global_impedance_(raw)\"] cd_filtered = sequence.continuous_data[\"global_impedance_(lowpass)\"] eeli_result = EELI().compute_parameter(cd) eeli_result_filtered = EELI().compute_parameter(cd_filtered) <p>Plot the results.</p> <p>Note that the variation in the EELI has decreased after filtering the global impedance. The filter removed the high frequency noise (mainly cardiac artifacts), which was causing the EELI to fluctuate.</p> In\u00a0[6]: Copied! <pre>import numpy as np\n\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\nfor ax, result, gi in zip(\n    (ax1, ax2),\n    (eeli_result, eeli_result_filtered),\n    (\n        sequence.continuous_data[\"global_impedance_(raw)\"],\n        sequence.continuous_data[\"global_impedance_(lowpass)\"],\n    ),\n    strict=False,\n):\n    sd_upper = np.mean(result.values) + np.std(result.values)\n    sd_lower = np.mean(result.values) - np.std(result.values)\n\n    ax.plot(gi.time, gi.values)\n    ax.axhline(np.mean(result.values), color=\"red\", label=\"Mean\")\n    ax.axhline(np.median(result.values), color=\"green\", label=\"Median\")\n    ax.plot(\n        result.time,\n        result,\n        \"o\",\n        color=\"black\",\n        label=\"EELIs\",\n    )\n\n    xlim = ax.get_xlim()\n    ax.fill_between(\n        xlim,\n        sd_upper,\n        sd_lower,\n        color=\"blue\",\n        alpha=0.3,\n        label=\"Standard deviation\",\n    )\n    ax.set(xlim=xlim, xlabel=\"Time (s)\", ylabel=\"Global impedance (a.u.)\")\n    ax.legend()\n</pre> import numpy as np  fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True) for ax, result, gi in zip(     (ax1, ax2),     (eeli_result, eeli_result_filtered),     (         sequence.continuous_data[\"global_impedance_(raw)\"],         sequence.continuous_data[\"global_impedance_(lowpass)\"],     ),     strict=False, ):     sd_upper = np.mean(result.values) + np.std(result.values)     sd_lower = np.mean(result.values) - np.std(result.values)      ax.plot(gi.time, gi.values)     ax.axhline(np.mean(result.values), color=\"red\", label=\"Mean\")     ax.axhline(np.median(result.values), color=\"green\", label=\"Median\")     ax.plot(         result.time,         result,         \"o\",         color=\"black\",         label=\"EELIs\",     )      xlim = ax.get_xlim()     ax.fill_between(         xlim,         sd_upper,         sd_lower,         color=\"blue\",         alpha=0.3,         label=\"Standard deviation\",     )     ax.set(xlim=xlim, xlabel=\"Time (s)\", ylabel=\"Global impedance (a.u.)\")     ax.legend()"},{"location":"examples/notebooks/test_parameter_tiv/","title":"Test parameter tiv","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\n\n%matplotlib widget\nfrom eitprocessing.datahandling.loading import load_eit_data\nfrom eitprocessing.features.breath_detection import BreathDetection\nfrom eitprocessing.features.pixel_breath import PixelBreath\nfrom eitprocessing.parameters.tidal_impedance_variation import TIV\n</pre> import numpy as np from matplotlib import pyplot as plt  %matplotlib widget from eitprocessing.datahandling.loading import load_eit_data from eitprocessing.features.breath_detection import BreathDetection from eitprocessing.features.pixel_breath import PixelBreath from eitprocessing.parameters.tidal_impedance_variation import TIV In\u00a0[2]: Copied! <pre>sequence = load_eit_data(\"tests/test_data/Draeger_Test.bin\", vendor=\"draeger\")\n</pre> sequence = load_eit_data(\"tests/test_data/Draeger_Test.bin\", vendor=\"draeger\") In\u00a0[3]: Copied! <pre>ssequence = sequence.t[55940:55970]\n</pre> ssequence = sequence.t[55940:55970] In\u00a0[4]: Copied! <pre>bd = BreathDetection()\nbreaths = bd.find_breaths(ssequence.continuous_data[\"global_impedance_(raw)\"])\n</pre> bd = BreathDetection() breaths = bd.find_breaths(ssequence.continuous_data[\"global_impedance_(raw)\"]) In\u00a0[5]: Copied! <pre>continuous_data = ssequence.continuous_data[\"global_impedance_(raw)\"].values\ncontinuous_time = ssequence.continuous_data[\"global_impedance_(raw)\"].time\ncontinuous_start_indices = [\n    np.argmax(continuous_time == start_time) for start_time in [breath.start_time for breath in breaths.values]\n]\ncontinuous_middle_indices = [\n    np.argmax(continuous_time == middle_time) for middle_time in [breath.middle_time for breath in breaths.values]\n]\ncontinuous_end_indices = [\n    np.argmax(continuous_time == end_time) for end_time in [breath.end_time for breath in breaths.values]\n]\n</pre> continuous_data = ssequence.continuous_data[\"global_impedance_(raw)\"].values continuous_time = ssequence.continuous_data[\"global_impedance_(raw)\"].time continuous_start_indices = [     np.argmax(continuous_time == start_time) for start_time in [breath.start_time for breath in breaths.values] ] continuous_middle_indices = [     np.argmax(continuous_time == middle_time) for middle_time in [breath.middle_time for breath in breaths.values] ] continuous_end_indices = [     np.argmax(continuous_time == end_time) for end_time in [breath.end_time for breath in breaths.values] ] In\u00a0[6]: Copied! <pre>pb = PixelBreath()\npixel_breaths = pb.find_pixel_breaths(\n    eit_data=ssequence.eit_data[\"raw\"],\n    continuous_data=ssequence.continuous_data[\"global_impedance_(raw)\"],\n    sequence=ssequence,\n)\n</pre> pb = PixelBreath() pixel_breaths = pb.find_pixel_breaths(     eit_data=ssequence.eit_data[\"raw\"],     continuous_data=ssequence.continuous_data[\"global_impedance_(raw)\"],     sequence=ssequence, ) In\u00a0[7]: Copied! <pre>pixel_data = ssequence.eit_data[\"raw\"].pixel_impedance\n_, n_rows, n_cols = pixel_data.shape\npixel_breaths = np.stack(pixel_breaths.values)\n\n# Handle None values\npixel_start_indices = [\n    np.argmax(ssequence.eit_data[\"raw\"].time == start_time)\n    for pixel_breath in pixel_breaths[:, 12, 13]\n    if pixel_breath is not None\n    for start_time in [pixel_breath.start_time]\n]\n\npixel_middle_indices = [\n    np.argmax(ssequence.eit_data[\"raw\"].time == middle_time)\n    for pixel_breath in pixel_breaths[:, 12, 13]\n    if pixel_breath is not None\n    for middle_time in [pixel_breath.middle_time]\n]\n\npixel_end_indices = [\n    np.argmax(ssequence.eit_data[\"raw\"].time == end_time)\n    for pixel_breath in pixel_breaths[:, 12, 13]\n    if pixel_breath is not None\n    for end_time in [pixel_breath.end_time]\n]\n</pre> pixel_data = ssequence.eit_data[\"raw\"].pixel_impedance _, n_rows, n_cols = pixel_data.shape pixel_breaths = np.stack(pixel_breaths.values)  # Handle None values pixel_start_indices = [     np.argmax(ssequence.eit_data[\"raw\"].time == start_time)     for pixel_breath in pixel_breaths[:, 12, 13]     if pixel_breath is not None     for start_time in [pixel_breath.start_time] ]  pixel_middle_indices = [     np.argmax(ssequence.eit_data[\"raw\"].time == middle_time)     for pixel_breath in pixel_breaths[:, 12, 13]     if pixel_breath is not None     for middle_time in [pixel_breath.middle_time] ]  pixel_end_indices = [     np.argmax(ssequence.eit_data[\"raw\"].time == end_time)     for pixel_breath in pixel_breaths[:, 12, 13]     if pixel_breath is not None     for end_time in [pixel_breath.end_time] ] In\u00a0[8]: Copied! <pre># TIV of global impedance determined on inspiratory limb of breath\ntiv_result_continuous_inspiratory = TIV().compute_continuous_parameter(\n    ssequence.continuous_data[\"global_impedance_(raw)\"],\n)\ntiv_result_continuous_inspiratory_values = np.stack(tiv_result_continuous_inspiratory.values)\n# TIV of global impedance determined on expiratory limb of breath\ntiv_result_continuous_expiratory = TIV().compute_continuous_parameter(\n    ssequence.continuous_data[\"global_impedance_(raw)\"],\n    tiv_method=\"expiratory\",\n)\ntiv_result_continuous_expiratory_values = np.stack(tiv_result_continuous_expiratory.values)\n# TIV of global impedance determined on mean of inspiratory and expiratory limb of breath\ntiv_result_continuous_mean = TIV().compute_continuous_parameter(\n    ssequence.continuous_data[\"global_impedance_(raw)\"],\n    tiv_method=\"mean\",\n)\ntiv_result_continuous_mean_values = np.stack(tiv_result_continuous_mean.values)\n\n# TIV of pixel impedance determined on inspiratory limb of pixel breath based on pixel breath timings\ntiv_result_pixel_inspiratory = TIV().compute_pixel_parameter(\n    ssequence.eit_data[\"raw\"],\n    ssequence.continuous_data[\"global_impedance_(raw)\"],\n    ssequence,\n    tiv_method=\"inspiratory\",\n    tiv_timing=\"pixel\",\n    result_label=\"inspiratory pixel tiv\",\n)\ntiv_result_pixel_inspiratory_values = (\n    np.empty((0, n_rows, n_cols))\n    if not len(tiv_result_pixel_inspiratory.values)\n    else np.stack(tiv_result_pixel_inspiratory.values)\n)\n# TIV of pixel impedance determined on inspiratory limb of pixel breath based on continuous breath timings\ntiv_result_pixel_inspiratory_continuous_timing = TIV().compute_pixel_parameter(\n    ssequence.eit_data[\"raw\"],\n    ssequence.continuous_data[\"global_impedance_(raw)\"],\n    ssequence,\n    tiv_method=\"inspiratory\",\n    tiv_timing=\"continuous\",\n    result_label=\"inspiratory pixel tiv with continuous timing\",\n)\n\ntiv_result_pixel_inspiratory_continuous_timing_values = (\n    np.empty((0, n_rows, n_cols))\n    if not len(tiv_result_pixel_inspiratory_continuous_timing.values)\n    else np.stack(tiv_result_pixel_inspiratory_continuous_timing.values)\n)\n</pre> # TIV of global impedance determined on inspiratory limb of breath tiv_result_continuous_inspiratory = TIV().compute_continuous_parameter(     ssequence.continuous_data[\"global_impedance_(raw)\"], ) tiv_result_continuous_inspiratory_values = np.stack(tiv_result_continuous_inspiratory.values) # TIV of global impedance determined on expiratory limb of breath tiv_result_continuous_expiratory = TIV().compute_continuous_parameter(     ssequence.continuous_data[\"global_impedance_(raw)\"],     tiv_method=\"expiratory\", ) tiv_result_continuous_expiratory_values = np.stack(tiv_result_continuous_expiratory.values) # TIV of global impedance determined on mean of inspiratory and expiratory limb of breath tiv_result_continuous_mean = TIV().compute_continuous_parameter(     ssequence.continuous_data[\"global_impedance_(raw)\"],     tiv_method=\"mean\", ) tiv_result_continuous_mean_values = np.stack(tiv_result_continuous_mean.values)  # TIV of pixel impedance determined on inspiratory limb of pixel breath based on pixel breath timings tiv_result_pixel_inspiratory = TIV().compute_pixel_parameter(     ssequence.eit_data[\"raw\"],     ssequence.continuous_data[\"global_impedance_(raw)\"],     ssequence,     tiv_method=\"inspiratory\",     tiv_timing=\"pixel\",     result_label=\"inspiratory pixel tiv\", ) tiv_result_pixel_inspiratory_values = (     np.empty((0, n_rows, n_cols))     if not len(tiv_result_pixel_inspiratory.values)     else np.stack(tiv_result_pixel_inspiratory.values) ) # TIV of pixel impedance determined on inspiratory limb of pixel breath based on continuous breath timings tiv_result_pixel_inspiratory_continuous_timing = TIV().compute_pixel_parameter(     ssequence.eit_data[\"raw\"],     ssequence.continuous_data[\"global_impedance_(raw)\"],     ssequence,     tiv_method=\"inspiratory\",     tiv_timing=\"continuous\",     result_label=\"inspiratory pixel tiv with continuous timing\", )  tiv_result_pixel_inspiratory_continuous_timing_values = (     np.empty((0, n_rows, n_cols))     if not len(tiv_result_pixel_inspiratory_continuous_timing.values)     else np.stack(tiv_result_pixel_inspiratory_continuous_timing.values) ) In\u00a0[9]: Copied! <pre>## Plot results of inspiratory TIV method on global impedance\n\nplt.figure()\nplt.plot(continuous_data)\nplt.plot(continuous_start_indices, continuous_data[continuous_start_indices], \"*\")\nplt.plot(continuous_middle_indices, continuous_data[continuous_middle_indices], \"o\")\nplt.plot(continuous_end_indices, continuous_data[continuous_end_indices], \"x\")\n\n# Plot vertical lines at middle_indices\nfor i, mid_idx in enumerate(np.searchsorted(continuous_time, tiv_result_continuous_inspiratory.time)):\n    plt.vlines(\n        x=mid_idx,\n        ymin=continuous_data[mid_idx] - tiv_result_continuous_inspiratory[i].values,\n        ymax=continuous_data[mid_idx],\n        color=\"r\",\n        linestyle=\"-\",\n        label=\"TIV\",\n    )\n</pre> ## Plot results of inspiratory TIV method on global impedance  plt.figure() plt.plot(continuous_data) plt.plot(continuous_start_indices, continuous_data[continuous_start_indices], \"*\") plt.plot(continuous_middle_indices, continuous_data[continuous_middle_indices], \"o\") plt.plot(continuous_end_indices, continuous_data[continuous_end_indices], \"x\")  # Plot vertical lines at middle_indices for i, mid_idx in enumerate(np.searchsorted(continuous_time, tiv_result_continuous_inspiratory.time)):     plt.vlines(         x=mid_idx,         ymin=continuous_data[mid_idx] - tiv_result_continuous_inspiratory[i].values,         ymax=continuous_data[mid_idx],         color=\"r\",         linestyle=\"-\",         label=\"TIV\",     )                      Figure                  In\u00a0[10]: Copied! <pre>## Plot results of inspiratory TIV method of a single pixel\n\npixel_row = 12\npixel_col = 12\n\npixel_data = ssequence.eit_data[\"raw\"].pixel_impedance\n\ntiv_result_pixel_inspiratory_time = (\n    np.empty((0, n_rows, n_cols))\n    if not len(tiv_result_pixel_inspiratory.time)\n    else np.stack(tiv_result_pixel_inspiratory.time)\n)\n\nplt.figure()\nplt.plot(pixel_data[:, pixel_row, pixel_col], label=\"Pixel Impedance\")\nplt.plot(pixel_start_indices, pixel_data[pixel_start_indices, pixel_row, pixel_col], \"*\", label=\"Start Indices\")\nplt.plot(pixel_middle_indices, pixel_data[pixel_middle_indices, pixel_row, pixel_col], \"o\", label=\"Middle Indices\")\nplt.plot(pixel_end_indices, pixel_data[pixel_end_indices, pixel_row, pixel_col], \"x\", label=\"End Indices\")\n\n# Filter out None values from tiv_result_pixel_inspiratory_time for the specific pixel\nvalid_times = [time for time in tiv_result_pixel_inspiratory_time[:, pixel_row, pixel_col] if time is not None]\n\n# Use np.searchsorted on the filtered times\nmid_indices = np.searchsorted(ssequence.eit_data[\"raw\"].time, valid_times)\n\n# Plot vertical lines at middle indices\nfor i, mid_idx in enumerate(mid_indices):\n    plt.vlines(\n        x=mid_idx,\n        ymin=(\n            pixel_data[mid_idx, pixel_row, pixel_col] - tiv_result_pixel_inspiratory_values[i + 1, pixel_row, pixel_col]\n        ),\n        ymax=pixel_data[mid_idx, pixel_row, pixel_col],\n        color=\"r\",\n        linestyle=\"-\",\n        label=\"TIV Line\" if i == 0 else None,  # Label only once\n    )\n\nplt.xlabel(\"Time\")\nplt.ylabel(\"Impedance (a.u.)\")\nplt.legend()\nplt.show()\n</pre> ## Plot results of inspiratory TIV method of a single pixel  pixel_row = 12 pixel_col = 12  pixel_data = ssequence.eit_data[\"raw\"].pixel_impedance  tiv_result_pixel_inspiratory_time = (     np.empty((0, n_rows, n_cols))     if not len(tiv_result_pixel_inspiratory.time)     else np.stack(tiv_result_pixel_inspiratory.time) )  plt.figure() plt.plot(pixel_data[:, pixel_row, pixel_col], label=\"Pixel Impedance\") plt.plot(pixel_start_indices, pixel_data[pixel_start_indices, pixel_row, pixel_col], \"*\", label=\"Start Indices\") plt.plot(pixel_middle_indices, pixel_data[pixel_middle_indices, pixel_row, pixel_col], \"o\", label=\"Middle Indices\") plt.plot(pixel_end_indices, pixel_data[pixel_end_indices, pixel_row, pixel_col], \"x\", label=\"End Indices\")  # Filter out None values from tiv_result_pixel_inspiratory_time for the specific pixel valid_times = [time for time in tiv_result_pixel_inspiratory_time[:, pixel_row, pixel_col] if time is not None]  # Use np.searchsorted on the filtered times mid_indices = np.searchsorted(ssequence.eit_data[\"raw\"].time, valid_times)  # Plot vertical lines at middle indices for i, mid_idx in enumerate(mid_indices):     plt.vlines(         x=mid_idx,         ymin=(             pixel_data[mid_idx, pixel_row, pixel_col] - tiv_result_pixel_inspiratory_values[i + 1, pixel_row, pixel_col]         ),         ymax=pixel_data[mid_idx, pixel_row, pixel_col],         color=\"r\",         linestyle=\"-\",         label=\"TIV Line\" if i == 0 else None,  # Label only once     )  plt.xlabel(\"Time\") plt.ylabel(\"Impedance (a.u.)\") plt.legend() plt.show()                      Figure                  In\u00a0[11]: Copied! <pre>## Plot results of inspiratory TIV method of a single pixel based on the timing of the continuous breaths\n\nplt.figure()\nplt.plot(pixel_data[:, pixel_row, pixel_col], label=\"Pixel Impedance\")\n\n# Prepare the timing array for inspiratory TIV with continuous timing\ntiv_result_pixel_inspiratory_continuous_timing_time = (\n    np.empty((0, n_rows, n_cols))\n    if not len(tiv_result_pixel_inspiratory_continuous_timing.time)\n    else np.stack(tiv_result_pixel_inspiratory_continuous_timing.time)\n)\n\n# Plot vertical lines based on TIV timing values\nvalid_times = [\n    time for time in tiv_result_pixel_inspiratory_continuous_timing_time[:, pixel_row, pixel_col] if time is not None\n]\n\nmid_indices = np.searchsorted(continuous_time, valid_times)\n\nfor i, glo_mid_idx in enumerate(mid_indices):\n    plt.vlines(\n        x=glo_mid_idx,\n        ymin=(\n            pixel_data[glo_mid_idx, pixel_row, pixel_col]\n            - tiv_result_pixel_inspiratory_continuous_timing_values[i, pixel_row, pixel_col]\n        ),\n        ymax=pixel_data[glo_mid_idx, pixel_row, pixel_col],\n        color=\"g\",\n        linestyle=\"-\",\n        label=\"TIV\" if i == 0 else None,  # Label only once\n    )\n\n# Add vertical lines for continuous start and end indices\nfor start_idx, end_idx in zip(continuous_start_indices, continuous_end_indices, strict=True):\n    plt.axvline(start_idx, color=\"red\", ls=\"--\", label=\"Start\" if start_idx == continuous_start_indices[0] else None)\n    plt.axvline(end_idx, color=\"black\", ls=\"-.\", label=\"End\" if end_idx == continuous_end_indices[0] else None)\n\nplt.xlabel(\"Time\")\nplt.ylabel(\"Impedance (a.u.)\")\nplt.legend()\nplt.show()\n</pre> ## Plot results of inspiratory TIV method of a single pixel based on the timing of the continuous breaths  plt.figure() plt.plot(pixel_data[:, pixel_row, pixel_col], label=\"Pixel Impedance\")  # Prepare the timing array for inspiratory TIV with continuous timing tiv_result_pixel_inspiratory_continuous_timing_time = (     np.empty((0, n_rows, n_cols))     if not len(tiv_result_pixel_inspiratory_continuous_timing.time)     else np.stack(tiv_result_pixel_inspiratory_continuous_timing.time) )  # Plot vertical lines based on TIV timing values valid_times = [     time for time in tiv_result_pixel_inspiratory_continuous_timing_time[:, pixel_row, pixel_col] if time is not None ]  mid_indices = np.searchsorted(continuous_time, valid_times)  for i, glo_mid_idx in enumerate(mid_indices):     plt.vlines(         x=glo_mid_idx,         ymin=(             pixel_data[glo_mid_idx, pixel_row, pixel_col]             - tiv_result_pixel_inspiratory_continuous_timing_values[i, pixel_row, pixel_col]         ),         ymax=pixel_data[glo_mid_idx, pixel_row, pixel_col],         color=\"g\",         linestyle=\"-\",         label=\"TIV\" if i == 0 else None,  # Label only once     )  # Add vertical lines for continuous start and end indices for start_idx, end_idx in zip(continuous_start_indices, continuous_end_indices, strict=True):     plt.axvline(start_idx, color=\"red\", ls=\"--\", label=\"Start\" if start_idx == continuous_start_indices[0] else None)     plt.axvline(end_idx, color=\"black\", ls=\"-.\", label=\"End\" if end_idx == continuous_end_indices[0] else None)  plt.xlabel(\"Time\") plt.ylabel(\"Impedance (a.u.)\") plt.legend() plt.show()                      Figure"},{"location":"examples/notebooks/test_parameter_tiv/#load-a-sequence-from-file","title":"Load a sequence from file.\u00b6","text":""},{"location":"examples/notebooks/test_parameter_tiv/#select-a-small-portion-of-the-sequence","title":"Select a small portion of the sequence\u00b6","text":""},{"location":"examples/notebooks/test_parameter_tiv/#perform-breath-detection-on-global-impedance","title":"Perform breath detection on global impedance\u00b6","text":""},{"location":"examples/notebooks/test_parameter_tiv/#perform-pixel-breath-detection-on-pixel-impedance","title":"Perform pixel breath detection on pixel impedance\u00b6","text":""},{"location":"examples/notebooks/test_parameter_tiv/#compute-tiv-with-different-tiv-methods-and-timings","title":"Compute TIV with different TIV methods and timings\u00b6","text":""},{"location":"examples/notebooks/test_parameter_tiv/#visualise-global-impedance-tiv","title":"Visualise global impedance TIV\u00b6","text":""},{"location":"examples/notebooks/test_parameter_tiv/#visualise-pixel-tiv-based-on-pixel-breath-timings","title":"Visualise pixel TIV based on pixel breath timings\u00b6","text":""},{"location":"examples/notebooks/test_parameter_tiv/#visualise-pixel-tiv-based-on-continuous-breaths-timings","title":"Visualise pixel TIV based on continuous breaths timings\u00b6","text":""},{"location":"examples/notebooks/test_pixel_breath/","title":"Test pixel breath","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\n\n%matplotlib widget\nfrom eitprocessing.datahandling.loading import load_eit_data\nfrom eitprocessing.features.pixel_breath import PixelBreath\n</pre> import numpy as np from matplotlib import pyplot as plt  %matplotlib widget from eitprocessing.datahandling.loading import load_eit_data from eitprocessing.features.pixel_breath import PixelBreath In\u00a0[2]: Copied! <pre>sequence = load_eit_data(\"tests/test_data/Draeger_Test.bin\", vendor=\"draeger\")\n</pre> sequence = load_eit_data(\"tests/test_data/Draeger_Test.bin\", vendor=\"draeger\") In\u00a0[3]: Copied! <pre>ssequence = sequence[0:1000]\n</pre> ssequence = sequence[0:1000] In\u00a0[4]: Copied! <pre>eit_data = ssequence.eit_data[\"raw\"]\ncontinuous_data = ssequence.continuous_data[\"global_impedance_(raw)\"]\n\npb = PixelBreath()\nbreath_container = pb.find_pixel_breaths(eit_data, continuous_data, ssequence, result_label=\"pixel breaths\")\n</pre> eit_data = ssequence.eit_data[\"raw\"] continuous_data = ssequence.continuous_data[\"global_impedance_(raw)\"]  pb = PixelBreath() breath_container = pb.find_pixel_breaths(eit_data, continuous_data, ssequence, result_label=\"pixel breaths\") In\u00a0[5]: Copied! <pre>pixel_row = 8\npixel_col = 8\npixel_data = ssequence.eit_data[\"raw\"].pixel_impedance\n\npixel_breaths = np.stack(breath_container.values)\n\nstart_indices = [\n    np.argmax(ssequence.eit_data[\"raw\"].time == start_time)\n    for pixel_breath in pixel_breaths[:, pixel_row, pixel_col]\n    if pixel_breath is not None\n    for start_time in [pixel_breath.start_time]\n]\n\nmiddle_indices = [\n    np.argmax(ssequence.eit_data[\"raw\"].time == middle_time)\n    for pixel_breath in pixel_breaths[:, pixel_row, pixel_col]\n    if pixel_breath is not None\n    for middle_time in [pixel_breath.middle_time]\n]\n\nend_indices = [\n    np.argmax(ssequence.eit_data[\"raw\"].time == end_time)\n    for pixel_breath in pixel_breaths[:, pixel_row, pixel_col]\n    if pixel_breath is not None\n    for end_time in [pixel_breath.end_time]\n]\n\n\nplt.figure()\nplt.plot(pixel_data[:, pixel_row, pixel_col])\nplt.plot(start_indices, pixel_data[start_indices, pixel_row, pixel_col], \"*\")\nplt.plot(middle_indices, pixel_data[middle_indices, pixel_row, pixel_col], \"o\")\nplt.plot(end_indices, pixel_data[end_indices, pixel_row, pixel_col], \"x\")\n</pre> pixel_row = 8 pixel_col = 8 pixel_data = ssequence.eit_data[\"raw\"].pixel_impedance  pixel_breaths = np.stack(breath_container.values)  start_indices = [     np.argmax(ssequence.eit_data[\"raw\"].time == start_time)     for pixel_breath in pixel_breaths[:, pixel_row, pixel_col]     if pixel_breath is not None     for start_time in [pixel_breath.start_time] ]  middle_indices = [     np.argmax(ssequence.eit_data[\"raw\"].time == middle_time)     for pixel_breath in pixel_breaths[:, pixel_row, pixel_col]     if pixel_breath is not None     for middle_time in [pixel_breath.middle_time] ]  end_indices = [     np.argmax(ssequence.eit_data[\"raw\"].time == end_time)     for pixel_breath in pixel_breaths[:, pixel_row, pixel_col]     if pixel_breath is not None     for end_time in [pixel_breath.end_time] ]   plt.figure() plt.plot(pixel_data[:, pixel_row, pixel_col]) plt.plot(start_indices, pixel_data[start_indices, pixel_row, pixel_col], \"*\") plt.plot(middle_indices, pixel_data[middle_indices, pixel_row, pixel_col], \"o\") plt.plot(end_indices, pixel_data[end_indices, pixel_row, pixel_col], \"x\") Out[5]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fbea82b1c90&gt;]</pre>                      Figure"},{"location":"examples/notebooks/test_pixel_breath/#load-a-sequence-from-file","title":"Load a sequence from file.\u00b6","text":""},{"location":"examples/notebooks/test_pixel_breath/#select-a-small-portion-of-the-sequence","title":"Select a small portion of the sequence\u00b6","text":""},{"location":"examples/notebooks/test_pixel_breath/#perform-pixel-breath-detection-on-pixel-impedance","title":"Perform pixel breath detection on pixel impedance\u00b6","text":""},{"location":"examples/notebooks/test_pixel_breath/#visualise-startend-of-inspiration-and-expiration-based-on-the-pixel-breath-detection","title":"Visualise start/end of inspiration and expiration based on the pixel breath detection\u00b6","text":""},{"location":"examples/notebooks/test_pixelmap_imshow/","title":"Test pixelmap imshow","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom eitprocessing.datahandling.pixelmap import (\n    DifferenceMap,\n    IntegerMap,\n    ODCLMap,\n    PendelluftMap,\n    PerfusionMap,\n    PixelMap,\n    SignedPendelluftMap,\n    TIVMap,\n)\nfrom eitprocessing.roi.amplitude import AmplitudeMap\n\n\ndef value_gradient(start: float, end: float, n: int = 32) -&gt; np.ndarray:\n    \"\"\"Create a gradient of values from start to end in an n x n grid.\"\"\"\n    return np.reshape(np.linspace(start, end, n * n), (n, n))\n\n\nfig, axes = plt.subplots(5, 3, figsize=(9, 10))\nvalues = value_gradient(-1.5, 1.5)\nvalues[:, 20] = np.nan\n\n## PixelMap example\n\n\n# Plot with NaN values and custom facecolor\nPixelMap(values, plot_config={\"facecolor\": \"red\"}).plotting.imshow(ax=axes[0, 0])\n\n# Plot with NaN values, facecolor set in imshow, and custom cmap\nPixelMap(values * 100, plot_config={\"cmap\": \"Reds\", \"facecolor\": \"red\"}).plotting.imshow(\n    ax=axes[0, 1], absolute=True, facecolor=\"blue\"\n)\n\n# Plot with NaN values, cmap set in imshow\nPixelMap(values).plotting.imshow(ax=axes[0, 2], colorbar=False, cmap=\"hsv\")\n\n## ODCLMap example\nodcl_map = ODCLMap(values)\n\n# ODCL plot with hidding axes\nodcl_map.plotting.imshow(hide_axes=False, ax=axes[1, 0])\n\n# ODCL plot overriding absolute, and with extra kwargs for imshow and colorbar\nodcl_map.plotting.imshow(\n    hide_axes=False, ax=axes[1, 1], interpolation=\"blackman\", colorbar_kwargs={\"extend\": \"both\"}, absolute=False\n)\n\n# ODCL plot overriding percentage\nodcl_map.plotting.imshow(hide_axes=False, ax=axes[1, 2], percentage=False, cmap=\"berlin\")\n\n\n## TIVMap example\ntiv = value_gradient(-250, 1000)\ntiv_map = TIVMap(tiv, suppress_negative_warning=True)\n\n# Plot on current axes\nplt.sca(axes[2, 0])\ntiv_map.plotting.imshow()\n\n# Thresholded and normalized plot\nplt.sca(axes[2, 1])\nmask = tiv_map.create_mask_from_threshold(50)\nmask.apply(tiv_map).plotting.imshow(normalize=True, colorbar_kwargs={\"extendrect\": True})\n\n# Don't extend colorbar, threshold absolute values, convert to AmplitudeMap\nplt.sca(axes[2, 2])\nmask.apply(tiv_map).convert_to(AmplitudeMap).plotting.imshow(colorbar_kwargs={\"extend\": None})\n\n# Plot difference maps\ndiff_map = DifferenceMap(values, label=\"differences\")\n\ndiff_map.plotting.imshow(ax=axes[3, 0])\ndiff_map.plotting.imshow(ax=axes[3, 1], cmap=\"coolwarm\")\n\n# Plot integer map\ninteger_map = IntegerMap([[0, 0, 0, 1], [2, 1, 1, 1], [2, 2, 3, 3], [2, 3, 3, 3]])\ninteger_map.plotting.imshow(ax=axes[3, 2])\n\n# Perfusion map\nperfusion_map = PerfusionMap(value_gradient(0, 45), plot_config={\"hide_axes\": False, \"extra_kwargs\": {\"aspect\": 0.75}})\nperfusion_map.plotting.imshow(ax=axes[4, 0])\n\n# Pendelluft map\npendelluft_map = PendelluftMap(value_gradient(0, 0.4))\npendelluft_map.plotting.imshow(ax=axes[4, 1], percentage=True)\n\n# Replace plot parameters\n\nsigned_pendelluft_map = SignedPendelluftMap(value_gradient(-1.5, 2))\nsigned_pendelluft_map.plotting.imshow(ax=axes[4, 2])\n\n\nfig.tight_layout()\n</pre> import numpy as np from matplotlib import pyplot as plt  from eitprocessing.datahandling.pixelmap import (     DifferenceMap,     IntegerMap,     ODCLMap,     PendelluftMap,     PerfusionMap,     PixelMap,     SignedPendelluftMap,     TIVMap, ) from eitprocessing.roi.amplitude import AmplitudeMap   def value_gradient(start: float, end: float, n: int = 32) -&gt; np.ndarray:     \"\"\"Create a gradient of values from start to end in an n x n grid.\"\"\"     return np.reshape(np.linspace(start, end, n * n), (n, n))   fig, axes = plt.subplots(5, 3, figsize=(9, 10)) values = value_gradient(-1.5, 1.5) values[:, 20] = np.nan  ## PixelMap example   # Plot with NaN values and custom facecolor PixelMap(values, plot_config={\"facecolor\": \"red\"}).plotting.imshow(ax=axes[0, 0])  # Plot with NaN values, facecolor set in imshow, and custom cmap PixelMap(values * 100, plot_config={\"cmap\": \"Reds\", \"facecolor\": \"red\"}).plotting.imshow(     ax=axes[0, 1], absolute=True, facecolor=\"blue\" )  # Plot with NaN values, cmap set in imshow PixelMap(values).plotting.imshow(ax=axes[0, 2], colorbar=False, cmap=\"hsv\")  ## ODCLMap example odcl_map = ODCLMap(values)  # ODCL plot with hidding axes odcl_map.plotting.imshow(hide_axes=False, ax=axes[1, 0])  # ODCL plot overriding absolute, and with extra kwargs for imshow and colorbar odcl_map.plotting.imshow(     hide_axes=False, ax=axes[1, 1], interpolation=\"blackman\", colorbar_kwargs={\"extend\": \"both\"}, absolute=False )  # ODCL plot overriding percentage odcl_map.plotting.imshow(hide_axes=False, ax=axes[1, 2], percentage=False, cmap=\"berlin\")   ## TIVMap example tiv = value_gradient(-250, 1000) tiv_map = TIVMap(tiv, suppress_negative_warning=True)  # Plot on current axes plt.sca(axes[2, 0]) tiv_map.plotting.imshow()  # Thresholded and normalized plot plt.sca(axes[2, 1]) mask = tiv_map.create_mask_from_threshold(50) mask.apply(tiv_map).plotting.imshow(normalize=True, colorbar_kwargs={\"extendrect\": True})  # Don't extend colorbar, threshold absolute values, convert to AmplitudeMap plt.sca(axes[2, 2]) mask.apply(tiv_map).convert_to(AmplitudeMap).plotting.imshow(colorbar_kwargs={\"extend\": None})  # Plot difference maps diff_map = DifferenceMap(values, label=\"differences\")  diff_map.plotting.imshow(ax=axes[3, 0]) diff_map.plotting.imshow(ax=axes[3, 1], cmap=\"coolwarm\")  # Plot integer map integer_map = IntegerMap([[0, 0, 0, 1], [2, 1, 1, 1], [2, 2, 3, 3], [2, 3, 3, 3]]) integer_map.plotting.imshow(ax=axes[3, 2])  # Perfusion map perfusion_map = PerfusionMap(value_gradient(0, 45), plot_config={\"hide_axes\": False, \"extra_kwargs\": {\"aspect\": 0.75}}) perfusion_map.plotting.imshow(ax=axes[4, 0])  # Pendelluft map pendelluft_map = PendelluftMap(value_gradient(0, 0.4)) pendelluft_map.plotting.imshow(ax=axes[4, 1], percentage=True)  # Replace plot parameters  signed_pendelluft_map = SignedPendelluftMap(value_gradient(-1.5, 2)) signed_pendelluft_map.plotting.imshow(ax=axes[4, 2])   fig.tight_layout()"},{"location":"examples/notebooks/test_plot_filter_results/","title":"Plotting Filter Results","text":"In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\n\nfrom eitprocessing.datahandling.loading import load_eit_data\nfrom eitprocessing.features.rate_detection import RateDetection\nfrom eitprocessing.filters.butterworth_filters import BandPassFilter, BandStopFilter\nfrom eitprocessing.filters.mdn import MDNFilter\nfrom eitprocessing.plotting.filter import FilterPlotting\n\nMINUTE = 60\n\nsequence = load_eit_data(\"example_data.bin\", vendor=\"draeger\", sample_frequency=20, label=\"draeger1\")\neit_data = sequence.eit_data[\"raw\"]\nglobal_impedance = sequence.continuous_data[\"global_impedance_(raw)\"]\nsample_frequency = eit_data.sample_frequency\n\nrd = RateDetection(\"adult\", refine_estimated_frequency=False)\nestimated_respiratory_rate, estimated_heart_rate = rd.apply(eit_data, captures=(captures := {}))\n</pre> from matplotlib import pyplot as plt  from eitprocessing.datahandling.loading import load_eit_data from eitprocessing.features.rate_detection import RateDetection from eitprocessing.filters.butterworth_filters import BandPassFilter, BandStopFilter from eitprocessing.filters.mdn import MDNFilter from eitprocessing.plotting.filter import FilterPlotting  MINUTE = 60  sequence = load_eit_data(\"example_data.bin\", vendor=\"draeger\", sample_frequency=20, label=\"draeger1\") eit_data = sequence.eit_data[\"raw\"] global_impedance = sequence.continuous_data[\"global_impedance_(raw)\"] sample_frequency = eit_data.sample_frequency  rd = RateDetection(\"adult\", refine_estimated_frequency=False) estimated_respiratory_rate, estimated_heart_rate = rd.apply(eit_data, captures=(captures := {})) In\u00a0[2]: Copied! <pre>mdn = MDNFilter(respiratory_rate=estimated_respiratory_rate, heart_rate=estimated_heart_rate)\nfiltered_data = mdn.apply(global_impedance, captures=(captures := {}))\n\nax = mdn.plotting.plot_results(**captures)\n</pre> mdn = MDNFilter(respiratory_rate=estimated_respiratory_rate, heart_rate=estimated_heart_rate) filtered_data = mdn.apply(global_impedance, captures=(captures := {}))  ax = mdn.plotting.plot_results(**captures) In\u00a0[3]: Copied! <pre>bp = BandPassFilter(cutoff_frequency=(0.1, 1), sample_frequency=20, order=10)\nbp_filtered_data = bp.apply(global_impedance.values, captures=(captures_bp := {}))\n\nbs = BandStopFilter(cutoff_frequency=(0.1, 1), sample_frequency=20, order=10)\nbs_filtered_data = bs.apply(global_impedance.values, captures=(captures_bs := {}))\n\nfig, axes = plt.subplots(2, 1)\n\nFilterPlotting.plot_results(**captures_bp, ax=axes[0])\n_ = FilterPlotting.plot_results(**captures_bs, ax=axes[1])\n</pre> bp = BandPassFilter(cutoff_frequency=(0.1, 1), sample_frequency=20, order=10) bp_filtered_data = bp.apply(global_impedance.values, captures=(captures_bp := {}))  bs = BandStopFilter(cutoff_frequency=(0.1, 1), sample_frequency=20, order=10) bs_filtered_data = bs.apply(global_impedance.values, captures=(captures_bs := {}))  fig, axes = plt.subplots(2, 1)  FilterPlotting.plot_results(**captures_bp, ax=axes[0]) _ = FilterPlotting.plot_results(**captures_bs, ax=axes[1])"},{"location":"examples/notebooks/test_plot_filter_results/#plotting-filter-results","title":"Plotting Filter Results\u00b6","text":"<p>This Notebook exemplifies how to plot the results of a filter applied to data using the <code>FilterResultsPlotting</code> class.</p>"},{"location":"examples/notebooks/test_plot_filter_results/#mdn-filter","title":"MDN Filter\u00b6","text":""},{"location":"examples/notebooks/test_plot_filter_results/#band-stop-and-band-pass-filters","title":"Band stop and band pass filters\u00b6","text":""},{"location":"examples/notebooks/test_ratedetection/","title":"Rate Detection","text":"In\u00a0[1]: Copied! <pre>from matplotlib import pyplot as plt\n\nfrom eitprocessing.datahandling.loading import load_eit_data\nfrom eitprocessing.features.rate_detection import RateDetection\n\nMINUTE = 60\n</pre> from matplotlib import pyplot as plt  from eitprocessing.datahandling.loading import load_eit_data from eitprocessing.features.rate_detection import RateDetection  MINUTE = 60 In\u00a0[\u00a0]: Copied! <pre>sequence = load_eit_data(\"example_data.bin\", vendor=\"draeger\", sample_frequency=20, label=\"draeger1\")\neit_data = sequence.eit_data[\"raw\"]\nglobal_impedance = eit_data.calculate_global_impedance()\n\nfig, axis = plt.subplots(1, figsize=(6, 3))\naxis.plot(eit_data.time, global_impedance, label=\"Global Impedance\")\naxis.set(xlabel=\"Time (s)\", ylabel=\"Impedance (a.u.)\", title=\"Global impedance\")\nfig.tight_layout()\n</pre> sequence = load_eit_data(\"example_data.bin\", vendor=\"draeger\", sample_frequency=20, label=\"draeger1\") eit_data = sequence.eit_data[\"raw\"] global_impedance = eit_data.calculate_global_impedance()  fig, axis = plt.subplots(1, figsize=(6, 3)) axis.plot(eit_data.time, global_impedance, label=\"Global Impedance\") axis.set(xlabel=\"Time (s)\", ylabel=\"Impedance (a.u.)\", title=\"Global impedance\") fig.tight_layout() In\u00a0[3]: Copied! <pre>rd = RateDetection(\"adult\", refine_estimated_frequency=False)\nestimated_respiratory_rate, estimated_heart_rate = rd.apply(eit_data, captures=(captures := {}))\nfig = rd.plotting.plot(**captures)\n\nrd = RateDetection(\"adult\")\nestimated_respiratory_rate_refined, estimated_heart_rate_refined = rd.apply(eit_data, captures=(captures := {}))\nfig = rd.plotting.plot(**captures)\nfig.suptitle(\"Refined Rate Detection (refined)\")\n\nprint(\n    f\"\"\"\nRespiratory rate:\n  estimated (peak):    {estimated_respiratory_rate * MINUTE:.1f} bpm\n  estimated (refined): {estimated_respiratory_rate_refined * MINUTE:.1f} bpm\n\nHeart rate:\n  estimated (peak):    {estimated_heart_rate * MINUTE:.1f} bpm\n  estimated (refined): {estimated_heart_rate_refined * MINUTE:.1f} bpm\n\"\"\"\n)\n</pre> rd = RateDetection(\"adult\", refine_estimated_frequency=False) estimated_respiratory_rate, estimated_heart_rate = rd.apply(eit_data, captures=(captures := {})) fig = rd.plotting.plot(**captures)  rd = RateDetection(\"adult\") estimated_respiratory_rate_refined, estimated_heart_rate_refined = rd.apply(eit_data, captures=(captures := {})) fig = rd.plotting.plot(**captures) fig.suptitle(\"Refined Rate Detection (refined)\")  print(     f\"\"\" Respiratory rate:   estimated (peak):    {estimated_respiratory_rate * MINUTE:.1f} bpm   estimated (refined): {estimated_respiratory_rate_refined * MINUTE:.1f} bpm  Heart rate:   estimated (peak):    {estimated_heart_rate * MINUTE:.1f} bpm   estimated (refined): {estimated_heart_rate_refined * MINUTE:.1f} bpm \"\"\" ) <pre>\nRespiratory rate:\n  estimated (peak):    8.0 bpm\n  estimated (refined): 7.6 bpm\n\nHeart rate:\n  estimated (peak):    68.0 bpm\n  estimated (refined): 67.6 bpm\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>from tests.test_rate_detection import generate_signal\n\nbase_respiratory_rate = 12 / MINUTE\nbase_heart_rate = 54 / MINUTE\n\n# This function generates an EIT impedance-like signal for 32x32 pixels over time with a high power signal\n# component (respiration), a low power signal component (cardiovascular action), and noise. The respiratory and heart\n# rate slightly vary over time, starting at the base rate and increasing by 10% (respiratory rate) or decreasing by 5%\n# (heart rate).\nsignal = generate_signal(\n    high_power_frequencies=(base_respiratory_rate,),\n    low_power_frequencies=(base_heart_rate,),\n    duration=60,  # seconds\n    sample_frequency=50.0,\n    low_power_amplitude=0.01,  # approximate relative amplitude of the heart rate signal\n    noise_amplitude=0.1,  # relative amplitude of the noise\n    captures=(captures := {}),\n    high_frequency_scale_factor=1.1,\n    low_frequency_scale_factor=0.95,\n)\nfig, axis = plt.subplots(1, figsize=(6, 3))\naxis.plot(captures[\"time\"], captures[\"summed_impedance\"], label=\"Summed Simulated Impedance Signal\")\naxis.set(xlabel=\"Time (s)\", ylabel=\"Impedance (a.u.)\", title=\"Summed Simulated Impedance Signal\")\nfig.tight_layout()\n</pre> from tests.test_rate_detection import generate_signal  base_respiratory_rate = 12 / MINUTE base_heart_rate = 54 / MINUTE  # This function generates an EIT impedance-like signal for 32x32 pixels over time with a high power signal # component (respiration), a low power signal component (cardiovascular action), and noise. The respiratory and heart # rate slightly vary over time, starting at the base rate and increasing by 10% (respiratory rate) or decreasing by 5% # (heart rate). signal = generate_signal(     high_power_frequencies=(base_respiratory_rate,),     low_power_frequencies=(base_heart_rate,),     duration=60,  # seconds     sample_frequency=50.0,     low_power_amplitude=0.01,  # approximate relative amplitude of the heart rate signal     noise_amplitude=0.1,  # relative amplitude of the noise     captures=(captures := {}),     high_frequency_scale_factor=1.1,     low_frequency_scale_factor=0.95, ) fig, axis = plt.subplots(1, figsize=(6, 3)) axis.plot(captures[\"time\"], captures[\"summed_impedance\"], label=\"Summed Simulated Impedance Signal\") axis.set(xlabel=\"Time (s)\", ylabel=\"Impedance (a.u.)\", title=\"Summed Simulated Impedance Signal\") fig.tight_layout() In\u00a0[5]: Copied! <pre>rd = RateDetection(\"adult\", refine_estimated_frequency=False)\nestimated_respiratory_rate, estimated_heart_rate = rd.apply(signal, captures=(captures := {}))\nfig = rd.plotting.plot(**captures)\n\nrd_refined = RateDetection(\"adult\")\nestimated_respiratory_rate_refined, estimated_heart_rate_refined = rd_refined.apply(\n    signal, captures=(captures_refined := {})\n)\nfig = rd.plotting.plot(**captures_refined)\nfig.suptitle(\"Refined Rate Detection (refined)\")\n\n\nprint(\n    f\"\"\"\nRespiratory rate:\n  base:                {base_respiratory_rate * MINUTE:.1f} bpm\n  final:               {base_respiratory_rate * 1.1 * MINUTE:.1f} bpm\n  estimated (peak):    {estimated_respiratory_rate * MINUTE:.1f} bpm\n  estimated (refined): {estimated_respiratory_rate_refined * MINUTE:.1f} bpm\n\nHeart rate:\n  base:                {base_heart_rate * MINUTE:.1f} bpm\n  final:               {base_heart_rate * 0.95 * MINUTE:.1f} bpm\n  estimated (peak):    {estimated_heart_rate * MINUTE:.1f} bpm\n  estimated (refined): {estimated_heart_rate_refined * MINUTE:.1f} bpm\n\"\"\"\n)\n</pre> rd = RateDetection(\"adult\", refine_estimated_frequency=False) estimated_respiratory_rate, estimated_heart_rate = rd.apply(signal, captures=(captures := {})) fig = rd.plotting.plot(**captures)  rd_refined = RateDetection(\"adult\") estimated_respiratory_rate_refined, estimated_heart_rate_refined = rd_refined.apply(     signal, captures=(captures_refined := {}) ) fig = rd.plotting.plot(**captures_refined) fig.suptitle(\"Refined Rate Detection (refined)\")   print(     f\"\"\" Respiratory rate:   base:                {base_respiratory_rate * MINUTE:.1f} bpm   final:               {base_respiratory_rate * 1.1 * MINUTE:.1f} bpm   estimated (peak):    {estimated_respiratory_rate * MINUTE:.1f} bpm   estimated (refined): {estimated_respiratory_rate_refined * MINUTE:.1f} bpm  Heart rate:   base:                {base_heart_rate * MINUTE:.1f} bpm   final:               {base_heart_rate * 0.95 * MINUTE:.1f} bpm   estimated (peak):    {estimated_heart_rate * MINUTE:.1f} bpm   estimated (refined): {estimated_heart_rate_refined * MINUTE:.1f} bpm \"\"\" ) <pre>\nRespiratory rate:\n  base:                12.0 bpm\n  final:               13.2 bpm\n  estimated (peak):    12.0 bpm\n  estimated (refined): 12.4 bpm\n\nHeart rate:\n  base:                54.0 bpm\n  final:               51.3 bpm\n  estimated (peak):    52.0 bpm\n  estimated (refined): 52.4 bpm\n\n</pre>"},{"location":"examples/notebooks/test_ratedetection/#rate-detection","title":"Rate Detection\u00b6","text":"<p>This Notebook exemplifies the rate detection algorithm. There are two examples: one with a short section of real EIT data, and one with simulated data. Both examples are demonstrated with and without the refinement of the estimated rates.</p>"},{"location":"examples/notebooks/test_ratedetection/#real-eit-data","title":"Real EIT Data\u00b6","text":""},{"location":"examples/notebooks/test_ratedetection/#simulated-eit-data","title":"Simulated EIT Data\u00b6","text":""},{"location":"examples/notebooks/test_watershed/","title":"Watershed lung space masking","text":"In\u00a0[\u00a0]: Copied! <pre>from tests.test_watershed import simulated_eit_data\n\nbreath_duration = 4.5\nsample_frequency = 20\n\neit_data = simulated_eit_data.__pytest_wrapped__.obj()(\n    breath_duration=breath_duration, sample_frequency=sample_frequency\n)\n</pre> from tests.test_watershed import simulated_eit_data  breath_duration = 4.5 sample_frequency = 20  eit_data = simulated_eit_data.__pytest_wrapped__.obj()(     breath_duration=breath_duration, sample_frequency=sample_frequency ) In\u00a0[2]: Copied! <pre>import numpy as np\nfrom IPython.display import HTML\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.artist import Artist\n\nfrom eitprocessing.datahandling.pixelmap import TIVMap\n\nvmin = np.nanmin(eit_data.pixel_impedance)\nvmax = np.nanmax(eit_data.pixel_impedance)\n\nfig = plt.figure(figsize=(8, 3))\nax = fig.add_subplot(121, projection=\"3d\")\nax2 = fig.add_subplot(122)\n\ny, x = np.indices(eit_data.pixel_impedance.shape[1:])\n\n\ndef _update_figure(num: int) -&gt; list[Artist]:\n    data = eit_data.pixel_impedance[num]\n    ax.clear()\n    surf = ax.plot_surface(x, y, data, cmap=\"viridis\", edgecolor=\"none\", vmin=vmin, vmax=vmax)\n    ax.set_zlim3d([vmin, vmax])\n    ax.view_init(elev=45, azim=10)\n    ax.set(xticklabels=[], yticklabels=[], zticklabels=[], title=\"3D representation of Z\")\n\n    ax2.clear()\n    im = TIVMap(data, suppress_negative_warning=True).plotting.imshow(\n        ax=ax2, vmin=0, vmax=vmax, colorbar=False, percentage=True\n    )\n    ax2.set(title=\"Impedance movie\")\n    return [surf, im]\n\n\n_update_figure(0)\n\n\n# Creating the Animation object. Only animated a single breath, but can be looped.\nanim = animation.FuncAnimation(\n    fig,\n    _update_figure,\n    int(breath_duration * sample_frequency),\n    interval=1000 / sample_frequency,\n    blit=False,\n)\ndisplay(HTML(anim.to_jshtml()))\nplt.close(fig)\n</pre> import numpy as np from IPython.display import HTML from matplotlib import animation from matplotlib import pyplot as plt from matplotlib.artist import Artist  from eitprocessing.datahandling.pixelmap import TIVMap  vmin = np.nanmin(eit_data.pixel_impedance) vmax = np.nanmax(eit_data.pixel_impedance)  fig = plt.figure(figsize=(8, 3)) ax = fig.add_subplot(121, projection=\"3d\") ax2 = fig.add_subplot(122)  y, x = np.indices(eit_data.pixel_impedance.shape[1:])   def _update_figure(num: int) -&gt; list[Artist]:     data = eit_data.pixel_impedance[num]     ax.clear()     surf = ax.plot_surface(x, y, data, cmap=\"viridis\", edgecolor=\"none\", vmin=vmin, vmax=vmax)     ax.set_zlim3d([vmin, vmax])     ax.view_init(elev=45, azim=10)     ax.set(xticklabels=[], yticklabels=[], zticklabels=[], title=\"3D representation of Z\")      ax2.clear()     im = TIVMap(data, suppress_negative_warning=True).plotting.imshow(         ax=ax2, vmin=0, vmax=vmax, colorbar=False, percentage=True     )     ax2.set(title=\"Impedance movie\")     return [surf, im]   _update_figure(0)   # Creating the Animation object. Only animated a single breath, but can be looped. anim = animation.FuncAnimation(     fig,     _update_figure,     int(breath_duration * sample_frequency),     interval=1000 / sample_frequency,     blit=False, ) display(HTML(anim.to_jshtml())) plt.close(fig) Once Loop Reflect In\u00a0[3]: Copied! <pre>from eitprocessing.roi.watershed import WatershedLungspace\n\nwatershed_mask = WatershedLungspace().apply(eit_data, captures=(captures := {}))\n</pre> from eitprocessing.roi.watershed import WatershedLungspace  watershed_mask = WatershedLungspace().apply(eit_data, captures=(captures := {})) In\u00a0[4]: Copied! <pre>fig, axes = plt.subplots(1, 3, figsize=(8, 2.3))\n\ncaptures[\"mean tiv\"].plotting.imshow(ax=axes[0])\naxes[0].set(title=\"Mean TIV\")\n\ncaptures[\"mean amplitude\"].plotting.imshow(ax=axes[1])\npeaks = captures[\"local peaks\"]\naxes[1].axes.scatter(peaks[:, 1], peaks[:, 0], color=\"red\", s=10, label=\"local peaks\")\naxes[1].set(title=\"Mean amplitude w/ peaks\")\n\nwatershed_map = captures[\"watershed regions\"]\nwatershed_map.plotting.imshow(ax=axes[2], colorbar=False)\nwatershed_map.plotting.add_region_markers(ax=axes[2])\n\naxes[2].set(title=\"Watershed regions\")\n\nfig.tight_layout()\n\nprint(\"Included regions: \", captures[\"included marker indices\"].astype(int))\n</pre> fig, axes = plt.subplots(1, 3, figsize=(8, 2.3))  captures[\"mean tiv\"].plotting.imshow(ax=axes[0]) axes[0].set(title=\"Mean TIV\")  captures[\"mean amplitude\"].plotting.imshow(ax=axes[1]) peaks = captures[\"local peaks\"] axes[1].axes.scatter(peaks[:, 1], peaks[:, 0], color=\"red\", s=10, label=\"local peaks\") axes[1].set(title=\"Mean amplitude w/ peaks\")  watershed_map = captures[\"watershed regions\"] watershed_map.plotting.imshow(ax=axes[2], colorbar=False) watershed_map.plotting.add_region_markers(ax=axes[2])  axes[2].set(title=\"Watershed regions\")  fig.tight_layout()  print(\"Included regions: \", captures[\"included marker indices\"].astype(int)) <pre>Included regions:  [ 4  5  9 10]\n</pre> In\u00a0[7]: Copied! <pre>fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n\nmean_amplitude = captures[\"mean amplitude\"]\nmasked_amplitude = watershed_mask.apply(mean_amplitude)\n\nmean_tiv = captures[\"mean tiv\"]\nfunctional_tiv_mask = captures[\"functional tiv mask\"]\nmasked_tiv = functional_tiv_mask.apply(mean_tiv)\n\nfunctional_tiv_mask.plotting.imshow(ax=axes[0, 0])\naxes[0, 0].set(title=\"Functional TIV mask\")\n\nwatershed_mask.plotting.imshow(ax=axes[0, 1])\naxes[0, 1].set(title=\"Functional amplitude mask\")\n\nmasked_tiv.plotting.imshow(ax=axes[1, 0])\naxes[1, 0].set(title=\"Applied TIV mask\")\n\nmasked_amplitude.plotting.imshow(ax=axes[1, 1])\naxes[1, 1].set(title=\"Applied amplitude mask\")\n\nfig.tight_layout()\n</pre> fig, axes = plt.subplots(2, 2, figsize=(8, 6))  mean_amplitude = captures[\"mean amplitude\"] masked_amplitude = watershed_mask.apply(mean_amplitude)  mean_tiv = captures[\"mean tiv\"] functional_tiv_mask = captures[\"functional tiv mask\"] masked_tiv = functional_tiv_mask.apply(mean_tiv)  functional_tiv_mask.plotting.imshow(ax=axes[0, 0]) axes[0, 0].set(title=\"Functional TIV mask\")  watershed_mask.plotting.imshow(ax=axes[0, 1]) axes[0, 1].set(title=\"Functional amplitude mask\")  masked_tiv.plotting.imshow(ax=axes[1, 0]) axes[1, 0].set(title=\"Applied TIV mask\")  masked_amplitude.plotting.imshow(ax=axes[1, 1]) axes[1, 1].set(title=\"Applied amplitude mask\")  fig.tight_layout()"},{"location":"examples/notebooks/test_watershed/#watershed-lung-space-masking","title":"Watershed lung space masking\u00b6","text":"<p>This Notebook demonstrates the use and working of the <code>WatershedLungspace</code> class for creating a pendelluft-aware mask of the functional lung space based on EIT data. The mask is created using a watershed algorithm that segments the lung space based on the amplitude of the EIT data. Details of the implementation can be found in the documentation of <code>WatershedLungspace</code>.</p>"},{"location":"examples/notebooks/test_watershed/#generate-simulated-eit-data","title":"Generate simulated EIT data\u00b6","text":"<p>We wrote a function to generate simulated EIT data with pendelluft for testing purposes. We use it here to generate data to visualize the workings of the <code>WatershedLungspace</code> class.</p>"},{"location":"examples/notebooks/test_watershed/#visualize-simulated-eit-data","title":"Visualize simulated EIT data\u00b6","text":"<p>This animation shows the generated EIT data. The data consists of four areas of high TIV with areas of negative TIV in between. The phase of the wave pattern changes from ventral right to dorsal left,</p>"},{"location":"examples/notebooks/test_watershed/#create-a-watershed-mask","title":"Create a watershed mask\u00b6","text":"<p>Creating the mask is done by instantiating the <code>WatershedLungspace</code> class and calling the <code>create_mask</code> method. A <code>captures</code> dictionary is passed to the method to capture any intermediate steps. This is not required, but is useful for visualizing the algorithm, as shown below.</p>"},{"location":"examples/notebooks/test_watershed/#apply-the-mask-to-the-tivamplitude-data","title":"Apply the mask to the TIV/amplitude data\u00b6","text":"<p>The mask is applied to the mean TIV and mean amplitude generated in the previous step. The mask can be applied to</p>"}]}