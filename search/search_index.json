{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to EITprocessing","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the documentation of the ALIVE software tool designed to load, analyze, and extract parameters from Electrical Impedance Tomography (EIT) data.  This software was designed by a joined effort of the Rotterdam Advanced Respiratory Care research group (ROTARC) of the Intensive Care of the Erasmus Medical Center and the Netherlands e-Science center.  Grant ID: NLESC.OEC.2022.002</p> <p>EIT is a bedside non-invasive lung imaging tool: it continuously and real-time visualizes changes in lung volume. Our software tool  serves as a comprehensive solution for handling EIT data from multiple leading manufacturers, including Sentec, Dr\u00e4ger, and Timpel.</p> <p>The software tool includes a back-end for researchers that are familair with programming eitprocessing and also a user-friendly dashboard eit_dash for clinical researchers allowing to quickly import datasets from various formats and sources and perform processing and analysis. This documentation page concerns eitprocessing. </p> <p>Our tool offers robust analysis features. From basic filters to advanced signal processing techniques, you can extract meaningful parameters from your EIT data. With our dashboard we aim to provide default analysis pipelines and many opportunities for customization according to the user needs. Visualizations and interactive graphs make it easy to interpret the results and understand the underlying physiological processes.</p> <p>It is important to note that the software tool is a work in progress, so not all fuctionalities are available yet. If you would like to contribute to coding you can reach out to us. </p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To start using our software you can you use the installation guide to set up the software on your system. Once installed, you can load your first dataset and explore the basic features.  We are committed to supporting your journey with EIT data analysis and extraction. If you encounter any issues or have questions you can put a pull request via github or emailadres. </p>"},{"location":"APIindex/","title":"API Documentation","text":""},{"location":"APIindex/#classes","title":"Classes","text":""},{"location":"APIindex/#eitprocessing.datahandling","title":"datahandling","text":""},{"location":"APIindex/#eitprocessing.datahandling.DataContainer","title":"DataContainer  <code>dataclass</code>","text":"<pre><code>DataContainer()\n</code></pre> <p>               Bases: <code>Equivalence</code></p> <p>Base class for data container classes.</p>"},{"location":"APIindex/#eitprocessing.datahandling.DataContainer.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.breath","title":"breath","text":""},{"location":"APIindex/#eitprocessing.datahandling.breath.Breath","title":"Breath  <code>dataclass</code>","text":"<pre><code>Breath(start_time: float, middle_time: float, end_time: float)\n</code></pre> <p>Represents a breath with a start, middle and end time.</p>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata","title":"continuousdata","text":""},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData","title":"ContinuousData  <code>dataclass</code>","text":"<pre><code>ContinuousData(label: str, name: str, unit: str, category: str, description: str = '', parameters: dict[str, Any] = dict(), derived_from: Any | list[Any] = list(), *, time: ndarray, values: ndarray, sample_frequency: float | None = None)\n</code></pre> <p>               Bases: <code>DataContainer</code>, <code>SelectByTime</code></p> <p>Container for data with a continuous time axis.</p> <p>Continuous data is assumed to be sequential (i.e. a single data point at each time point, sorted by time) and continuously measured/created at a fixed sampling frequency. However, a fixed interval between consecutive time points is not enforced to account for floating point arithmetic, devices with imperfect sampling frequencies, and other sources of variation.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Computer readable naming of the instance.</p> required <code>name</code> <code>str</code> <p>Human readable naming of the instance.</p> required <code>unit</code> <code>str</code> <p>Unit of the data, if applicable.</p> required <code>category</code> <code>str</code> <p>Category the data falls into, e.g. 'airway pressure'.</p> required <code>description</code> <code>str</code> <p>Human readable extended description of the data.</p> <code>''</code> <code>parameters</code> <code>dict[str, Any]</code> <p>Parameters used to derive this data.</p> <code>dict()</code> <code>derived_from</code> <code>Any | list[Any]</code> <p>Traceback of intermediates from which the current data was derived.</p> <code>list()</code> <code>values</code> <code>ndarray</code> <p>Data points.</p> required"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.locked","title":"locked  <code>property</code>","text":"<pre><code>locked: bool\n</code></pre> <p>Return whether the values attribute is locked.</p> <p>See lock().</p>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.loaded","title":"loaded  <code>property</code>","text":"<pre><code>loaded: bool\n</code></pre> <p>Return whether the data was loaded from disk, or derived from elsewhere.</p>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(start_time: float | None = None, end_time: float | None = None, start_inclusive: bool = False, end_inclusive: bool = False, label: str | None = None) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float | None</code> <p>first time point to include. Defaults to first frame of sequence.</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>last time point. Defaults to last frame of sequence.</p> <code>None</code> <code>start_inclusive</code> <code>default</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <code>False</code> <code>label</code> <code>str | None</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A shortened copy of the object.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_time(  # noqa: D417\n    self,\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self:\n    \"\"\"Get a shortened copy of the object, starting from start_time and ending at end_time.\n\n    Given a start and end time stamp (i.e. its value, not its index),\n    return a slice of the original object, which must contain a time axis.\n\n    Args:\n        start_time: first time point to include. Defaults to first frame of sequence.\n        end_time: last time point. Defaults to last frame of sequence.\n        start_inclusive (default: `True`), end_inclusive (default `False`):\n            these arguments control the behavior if the given time stamp\n            does not match exactly with an existing time stamp of the input.\n            if `True`: the given time stamp will be inside the sliced object.\n            if `False`: the given time stamp will be outside the sliced object.\n        label: Description. Defaults to None, which will create a label based\n            on the original object label and the frames by which it is sliced.\n\n    Raises:\n        TypeError: if `self` does not contain a `time` attribute.\n        ValueError: if time stamps are not sorted.\n\n    Returns:\n        A shortened copy of the object.\n    \"\"\"\n    if not hasattr(self, \"time\"):\n        msg = f\"Object {self} has no time axis.\"\n        raise TypeError(msg)\n\n    if len(self.time) == 0:\n        # TODO: make proper new instances when not slicing\n        return copy.deepcopy(self)\n\n    if start_time is None and end_time is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    if not np.all(np.sort(self.time) == self.time):\n        msg = f\"Time stamps for {self} are not sorted and therefore data cannot be selected by time.\"\n        raise ValueError(msg)\n\n    if start_time is None or start_time &lt; self.time[0]:\n        start_index = 0\n    elif start_inclusive:\n        start_index = bisect.bisect_right(self.time, start_time) - 1\n    else:\n        start_index = bisect.bisect_left(self.time, start_time)\n\n    if end_time is None:\n        end_index = len(self.time)\n    elif end_inclusive:\n        end_index = bisect.bisect_left(self.time, end_time) + 1\n    else:\n        end_index = bisect.bisect_left(self.time, end_time)\n\n    return self.select_by_index(\n        start=start_index,\n        end=end_index,\n        newlabel=label,\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(start: int | None = None, end: int | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_index(\n    self,\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"De facto implementation of the `__getitem__` function.\n\n    This function can also be called directly to add a label to the sliced\n    object. Otherwise a default label describing the slice and original\n    object is attached.\n    \"\"\"\n    if start is None and end is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    start = start if start is not None else 0\n    end = end if end is not None else len(self)\n    newlabel = newlabel or self.label\n\n    return self._sliced_copy(start_index=start, end_index=end, newlabel=newlabel)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.copy","title":"copy","text":"<pre><code>copy(label: str, *, name: str | None = None, unit: str | None = None, description: str | None = None, parameters: dict | None = None) -&gt; Self\n</code></pre> <p>Create a copy.</p> <p>Whenever data is altered, it should probably be copied first. The alterations should then be made in the copy.</p> Source code in <code>eitprocessing/datahandling/continuousdata.py</code> <pre><code>def copy(\n    self,\n    label: str,\n    *,\n    name: str | None = None,\n    unit: str | None = None,\n    description: str | None = None,\n    parameters: dict | None = None,\n) -&gt; Self:\n    \"\"\"Create a copy.\n\n    Whenever data is altered, it should probably be copied first. The alterations should then be made in the copy.\n    \"\"\"\n    obj = self.__class__(\n        label=label,\n        name=name or label,\n        unit=unit or self.unit,\n        description=description or f\"Derived from {self.name}\",\n        parameters=self.parameters | (parameters or {}),\n        derived_from=[*self.derived_from, self],\n        category=self.category,\n        # copying data can become inefficient with large datasets if the\n        # data is not directly edited afer copying but overridden instead;\n        # consider creating a view and locking it, requiring the user to\n        # make a copy if they want to edit the data directly\n        time=np.copy(self.time),\n        values=np.copy(self.values),\n        sample_frequency=self.sample_frequency,\n    )\n    obj.unlock()\n    return obj\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.derive","title":"derive","text":"<pre><code>derive(label: str, function: Callable, func_args: dict | None = None, **kwargs) -&gt; Self\n</code></pre> <p>Create a copy deriving data from values attribute.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>New label for the derived object.</p> required <code>function</code> <code>Callable</code> <p>Function that takes the values and returns the derived values.</p> required <code>func_args</code> <code>dict | None</code> <p>Arguments to pass to function, if any.</p> <code>None</code> <code>**kwargs</code> <p>Values for changed attributes of derived object.</p> <code>{}</code> <p>Example: <pre><code>def convert_data(x, add=None, subtract=None, multiply=None, divide=None):\n    if add:\n        x += add\n    if subtract:\n        x -= subtract\n    if multiply:\n        x *= multiply\n    if divide:\n        x /= divide\n    return x\n\n\ndata = ContinuousData(\n    name=\"Lung volume (in mL)\", label=\"volume_mL\", unit=\"mL\", category=\"volume\", values=some_loaded_data\n)\nderived = data.derive(\"volume_L\", convert_data, {\"divide\": 1000}, name=\"Lung volume (in L)\", unit=\"L\")\n</code></pre></p> Source code in <code>eitprocessing/datahandling/continuousdata.py</code> <pre><code>def derive(\n    self,\n    label: str,\n    function: Callable,\n    func_args: dict | None = None,\n    **kwargs,\n) -&gt; Self:\n    \"\"\"Create a copy deriving data from values attribute.\n\n    Args:\n        label: New label for the derived object.\n        function: Function that takes the values and returns the derived values.\n        func_args: Arguments to pass to function, if any.\n        **kwargs: Values for changed attributes of derived object.\n\n    Example:\n    ```\n    def convert_data(x, add=None, subtract=None, multiply=None, divide=None):\n        if add:\n            x += add\n        if subtract:\n            x -= subtract\n        if multiply:\n            x *= multiply\n        if divide:\n            x /= divide\n        return x\n\n\n    data = ContinuousData(\n        name=\"Lung volume (in mL)\", label=\"volume_mL\", unit=\"mL\", category=\"volume\", values=some_loaded_data\n    )\n    derived = data.derive(\"volume_L\", convert_data, {\"divide\": 1000}, name=\"Lung volume (in L)\", unit=\"L\")\n    ```\n    \"\"\"\n    if func_args is None:\n        func_args = {}\n    copy = self.copy(label, **kwargs)\n    copy.values = function(copy.values, **func_args)\n    return copy\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.lock","title":"lock","text":"<pre><code>lock(*attr: str) -&gt; None\n</code></pre> <p>Lock attributes, essentially rendering them read-only.</p> <p>Locked attributes cannot be overwritten. Attributes can be unlocked using <code>unlock()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*attr</code> <code>str</code> <p>any number of attributes can be passed here, all of which will be locked. Defaults to \"values\".</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # lock the `values` attribute of `data`\n&gt;&gt;&gt; data.lock()\n&gt;&gt;&gt; data.values = [1, 2, 3] # will result in an AttributeError\n&gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n</code></pre> Source code in <code>eitprocessing/datahandling/continuousdata.py</code> <pre><code>def lock(self, *attr: str) -&gt; None:\n    \"\"\"Lock attributes, essentially rendering them read-only.\n\n    Locked attributes cannot be overwritten. Attributes can be unlocked using `unlock()`.\n\n    Args:\n        *attr: any number of attributes can be passed here, all of which will be locked. Defaults to \"values\".\n\n    Examples:\n        &gt;&gt;&gt; # lock the `values` attribute of `data`\n        &gt;&gt;&gt; data.lock()\n        &gt;&gt;&gt; data.values = [1, 2, 3] # will result in an AttributeError\n        &gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n    \"\"\"\n    if not len(attr):\n        # default values are not allowed when using *attr, so set a default here if none is supplied\n        attr = (\"values\",)\n    for attr_ in attr:\n        getattr(self, attr_).flags[\"WRITEABLE\"] = False\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.continuousdata.ContinuousData.unlock","title":"unlock","text":"<pre><code>unlock(*attr: str) -&gt; None\n</code></pre> <p>Unlock attributes, rendering them editable.</p> <p>Locked attributes cannot be overwritten, but can be unlocked with this function to make them editable.</p> <p>Parameters:</p> Name Type Description Default <code>*attr</code> <code>str</code> <p>any number of attributes can be passed here, all of which will be unlocked. Defaults to \"values\".</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # lock the `values` attribute of `data`\n&gt;&gt;&gt; data.lock()\n&gt;&gt;&gt; data.values = [1, 2, 3] # will result in an AttributeError\n&gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n&gt;&gt;&gt; data.unlock()\n&gt;&gt;&gt; data.values = [1, 2, 3]\n&gt;&gt;&gt; print(data.values)\n[1,2,3]\n&gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n&gt;&gt;&gt; print(data.values)\n1\n</code></pre> Source code in <code>eitprocessing/datahandling/continuousdata.py</code> <pre><code>def unlock(self, *attr: str) -&gt; None:\n    \"\"\"Unlock attributes, rendering them editable.\n\n    Locked attributes cannot be overwritten, but can be unlocked with this function to make them editable.\n\n    Args:\n        *attr: any number of attributes can be passed here, all of which will be unlocked. Defaults to \"values\".\n\n    Examples:\n        &gt;&gt;&gt; # lock the `values` attribute of `data`\n        &gt;&gt;&gt; data.lock()\n        &gt;&gt;&gt; data.values = [1, 2, 3] # will result in an AttributeError\n        &gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n        &gt;&gt;&gt; data.unlock()\n        &gt;&gt;&gt; data.values = [1, 2, 3]\n        &gt;&gt;&gt; print(data.values)\n        [1,2,3]\n        &gt;&gt;&gt; data.values[0] = 1      # will result in a RuntimeError\n        &gt;&gt;&gt; print(data.values)\n        1\n    \"\"\"\n    if not len(attr):\n        # default values are not allowed when using *attr, so set a default here if none is supplied\n        attr = (\"values\",)\n    for attr_ in attr:\n        getattr(self, attr_).flags[\"WRITEABLE\"] = True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection","title":"datacollection","text":""},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection","title":"DataCollection","text":"<pre><code>DataCollection(data_type: type[V], *args, **kwargs)\n</code></pre> <p>               Bases: <code>Equivalence</code>, <code>UserDict</code>, <code>HasTimeIndexer</code>, <code>Generic[V]</code></p> <p>A collection of a single type of data with unique labels.</p> <p>A DataCollection functions largely as a dictionary, but requires a data_type argument, which must be one of the data containers existing in this package. When adding an item to the collection, the type of the value must match the data_type of the collection. Furthermore, the key has to match the attribute 'label' attached to the value.</p> <p>The convenience method <code>add()</code> adds an item by setting the key to <code>value.label</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>type[V]</code> <p>the data container stored in this collection.</p> required Source code in <code>eitprocessing/datahandling/datacollection.py</code> <pre><code>def __init__(self, data_type: type[V], *args, **kwargs):\n    if not any(issubclass(data_type, cls) for cls in V_classes):\n        msg = f\"Type {data_type} not expected to be stored in a DataCollection.\"\n        raise ValueError(msg)\n    self.data_type = data_type\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.add","title":"add","text":"<pre><code>add(*item: V, overwrite: bool = False) -&gt; None\n</code></pre> <p>Add one or multiple item(s) to the collection using the item label as the key.</p> Source code in <code>eitprocessing/datahandling/datacollection.py</code> <pre><code>def add(self, *item: V, overwrite: bool = False) -&gt; None:\n    \"\"\"Add one or multiple item(s) to the collection using the item label as the key.\"\"\"\n    for item_ in item:\n        self._check_item(item_, overwrite=overwrite)\n        super().__setitem__(item_.label, item_)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.get_loaded_data","title":"get_loaded_data","text":"<pre><code>get_loaded_data() -&gt; dict[str, V]\n</code></pre> <p>Return all data that was directly loaded from disk.</p> Source code in <code>eitprocessing/datahandling/datacollection.py</code> <pre><code>def get_loaded_data(self) -&gt; dict[str, V]:\n    \"\"\"Return all data that was directly loaded from disk.\"\"\"\n    return {k: v for k, v in self.items() if v.loaded}\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.get_data_derived_from","title":"get_data_derived_from","text":"<pre><code>get_data_derived_from(obj: V) -&gt; dict[str, V]\n</code></pre> <p>Return all data that was derived from a specific source.</p> Source code in <code>eitprocessing/datahandling/datacollection.py</code> <pre><code>def get_data_derived_from(self, obj: V) -&gt; dict[str, V]:\n    \"\"\"Return all data that was derived from a specific source.\"\"\"\n    return {k: v for k, v in self.items() if obj in v.derived_from}\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.get_derived_data","title":"get_derived_data","text":"<pre><code>get_derived_data() -&gt; dict[str, V]\n</code></pre> <p>Return all data that was derived from any source.</p> Source code in <code>eitprocessing/datahandling/datacollection.py</code> <pre><code>def get_derived_data(self) -&gt; dict[str, V]:\n    \"\"\"Return all data that was derived from any source.\"\"\"\n    return {k: v for k, v in self.items() if v.derived_from}\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.concatenate","title":"concatenate","text":"<pre><code>concatenate(other: Self) -&gt; Self\n</code></pre> <p>Concatenate this collection with an equivalent collection.</p> <p>Each item of self of concatenated with the item of other with the same key.</p> Source code in <code>eitprocessing/datahandling/datacollection.py</code> <pre><code>def concatenate(self: Self, other: Self) -&gt; Self:\n    \"\"\"Concatenate this collection with an equivalent collection.\n\n    Each item of self of concatenated with the item of other with the same key.\n    \"\"\"\n    self.isequivalent(other, raise_=True)\n\n    concatenated = self.__class__(self.data_type)\n    for key in self:\n        concatenated.add(self[key].concatenate(other[key]))\n\n    return concatenated\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.datacollection.DataCollection.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(start_time: float | None, end_time: float | None, start_inclusive: bool = True, end_inclusive: bool = False) -&gt; DataCollection\n</code></pre> <p>Return a DataCollection containing sliced copies of the items.</p> Source code in <code>eitprocessing/datahandling/datacollection.py</code> <pre><code>def select_by_time(\n    self,\n    start_time: float | None,\n    end_time: float | None,\n    start_inclusive: bool = True,\n    end_inclusive: bool = False,\n) -&gt; DataCollection:\n    \"\"\"Return a DataCollection containing sliced copies of the items.\"\"\"\n    if self.data_type is IntervalData:\n        return DataCollection(\n            self.data_type,\n            **{\n                k: v.select_by_time(\n                    start_time=start_time,\n                    end_time=end_time,\n                )\n                for k, v in self.items()\n            },\n        )\n\n    return DataCollection(\n        self.data_type,\n        **{\n            k: v.select_by_time(\n                start_time=start_time,\n                end_time=end_time,\n                start_inclusive=start_inclusive,\n                end_inclusive=end_inclusive,\n            )\n            for k, v in self.items()\n        },\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata","title":"eitdata","text":""},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData","title":"EITData  <code>dataclass</code>","text":"<pre><code>EITData(path: str | Path | list[Path | str], nframes: int, time: ndarray, sample_frequency: float, vendor: Vendor, label: str | None = None, name: str | None = None, *, pixel_impedance: ndarray)\n</code></pre> <p>               Bases: <code>DataContainer</code>, <code>SelectByTime</code></p> <p>Container for EIT impedance data.</p> <p>This class holds the pixel impedance from an EIT measurement, as well as metadata describing the measurement. The class is meant to hold data from (part of) a singular continuous measurement.</p> <p>This class can't be initialized directly. Instead, use <code>load_eit_data(&lt;path&gt;, vendor=&lt;vendor&gt;)</code> to load data from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | list[Path | str]</code> <p>The path of list of paths of the source from which data was derived.</p> required <code>nframes</code> <code>int</code> <p>Number of frames.</p> required <code>time</code> <code>ndarray</code> <p>The time of each frame (since start measurement).</p> required <code>sample_frequency</code> <code>float</code> <p>The (average) frequency at which the frames are collected, in Hz.</p> required <code>vendor</code> <code>Vendor</code> <p>The vendor of the device the data was collected with.</p> required <code>label</code> <code>str | None</code> <p>Computer readable label identifying this dataset.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Human readable name for the data.</p> <code>None</code> <code>pixel_impedance</code> <code>ndarray</code> <p>Impedance values for each pixel at each frame.</p> required"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData.framerate","title":"framerate  <code>property</code>","text":"<pre><code>framerate: float\n</code></pre> <p>Deprecated alias to <code>sample_frequency</code>.</p>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(start_time: float | None = None, end_time: float | None = None, start_inclusive: bool = False, end_inclusive: bool = False, label: str | None = None) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float | None</code> <p>first time point to include. Defaults to first frame of sequence.</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>last time point. Defaults to last frame of sequence.</p> <code>None</code> <code>start_inclusive</code> <code>default</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <code>False</code> <code>label</code> <code>str | None</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A shortened copy of the object.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_time(  # noqa: D417\n    self,\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self:\n    \"\"\"Get a shortened copy of the object, starting from start_time and ending at end_time.\n\n    Given a start and end time stamp (i.e. its value, not its index),\n    return a slice of the original object, which must contain a time axis.\n\n    Args:\n        start_time: first time point to include. Defaults to first frame of sequence.\n        end_time: last time point. Defaults to last frame of sequence.\n        start_inclusive (default: `True`), end_inclusive (default `False`):\n            these arguments control the behavior if the given time stamp\n            does not match exactly with an existing time stamp of the input.\n            if `True`: the given time stamp will be inside the sliced object.\n            if `False`: the given time stamp will be outside the sliced object.\n        label: Description. Defaults to None, which will create a label based\n            on the original object label and the frames by which it is sliced.\n\n    Raises:\n        TypeError: if `self` does not contain a `time` attribute.\n        ValueError: if time stamps are not sorted.\n\n    Returns:\n        A shortened copy of the object.\n    \"\"\"\n    if not hasattr(self, \"time\"):\n        msg = f\"Object {self} has no time axis.\"\n        raise TypeError(msg)\n\n    if len(self.time) == 0:\n        # TODO: make proper new instances when not slicing\n        return copy.deepcopy(self)\n\n    if start_time is None and end_time is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    if not np.all(np.sort(self.time) == self.time):\n        msg = f\"Time stamps for {self} are not sorted and therefore data cannot be selected by time.\"\n        raise ValueError(msg)\n\n    if start_time is None or start_time &lt; self.time[0]:\n        start_index = 0\n    elif start_inclusive:\n        start_index = bisect.bisect_right(self.time, start_time) - 1\n    else:\n        start_index = bisect.bisect_left(self.time, start_time)\n\n    if end_time is None:\n        end_index = len(self.time)\n    elif end_inclusive:\n        end_index = bisect.bisect_left(self.time, end_time) + 1\n    else:\n        end_index = bisect.bisect_left(self.time, end_time)\n\n    return self.select_by_index(\n        start=start_index,\n        end=end_index,\n        newlabel=label,\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(start: int | None = None, end: int | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_index(\n    self,\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"De facto implementation of the `__getitem__` function.\n\n    This function can also be called directly to add a label to the sliced\n    object. Otherwise a default label describing the slice and original\n    object is attached.\n    \"\"\"\n    if start is None and end is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    start = start if start is not None else 0\n    end = end if end is not None else len(self)\n    newlabel = newlabel or self.label\n\n    return self._sliced_copy(start_index=start, end_index=end, newlabel=newlabel)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData.ensure_path_list","title":"ensure_path_list  <code>staticmethod</code>","text":"<pre><code>ensure_path_list(path: str | Path | list[str | Path]) -&gt; list[Path]\n</code></pre> <p>Return the path or paths as a list.</p> <p>The path of any EITData object can be a single str/Path or a list of str/Path objects. This method returns a list of Path objects given either a str/Path or list of str/Paths.</p> Source code in <code>eitprocessing/datahandling/eitdata.py</code> <pre><code>@staticmethod\ndef ensure_path_list(\n    path: str | Path | list[str | Path],\n) -&gt; list[Path]:\n    \"\"\"Return the path or paths as a list.\n\n    The path of any EITData object can be a single str/Path or a list of str/Path objects. This method returns a\n    list of Path objects given either a str/Path or list of str/Paths.\n    \"\"\"\n    if isinstance(path, list):\n        return [Path(p) for p in path]\n    return [Path(path)]\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.EITData.calculate_global_impedance","title":"calculate_global_impedance","text":"<pre><code>calculate_global_impedance() -&gt; ndarray\n</code></pre> <p>Return the global impedance, i.e. the sum of all included pixels at each frame.</p> Source code in <code>eitprocessing/datahandling/eitdata.py</code> <pre><code>def calculate_global_impedance(self) -&gt; np.ndarray:\n    \"\"\"Return the global impedance, i.e. the sum of all included pixels at each frame.\"\"\"\n    return np.nansum(self.pixel_impedance, axis=(1, 2))\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.eitdata.Vendor","title":"Vendor","text":"<p>               Bases: <code>LowercaseStrEnum</code></p> <p>Enum indicating the vendor (manufacturer) of the source EIT device.</p>"},{"location":"APIindex/#eitprocessing.datahandling.event","title":"event","text":""},{"location":"APIindex/#eitprocessing.datahandling.event.Event","title":"Event  <code>dataclass</code>","text":"<pre><code>Event(marker: int, text: str)\n</code></pre> <p>Single time point event registered during an EIT measurement.</p>"},{"location":"APIindex/#eitprocessing.datahandling.intervaldata","title":"intervaldata","text":""},{"location":"APIindex/#eitprocessing.datahandling.intervaldata.Interval","title":"Interval","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A tuple containing the start time and end time of an interval.</p>"},{"location":"APIindex/#eitprocessing.datahandling.intervaldata.IntervalData","title":"IntervalData  <code>dataclass</code>","text":"<pre><code>IntervalData(label: str, name: str, unit: str | None, category: str, intervals: list[Interval | tuple[float, float]], values: list[Any] | None = None, parameters: dict[str, Any] = dict(), derived_from: list[Any] = list(), description: str = '', default_partial_inclusion: bool = False)\n</code></pre> <p>               Bases: <code>DataContainer</code>, <code>SelectByIndex</code>, <code>HasTimeIndexer</code></p> <p>Container for interval data existing over a period of time.</p> <p>Interval data is data that consists for a given time interval. Examples are a ventilator setting (e.g. end-expiratory pressure), the position of a patient, a maneuver (end-expiratory hold) being performed, detected periods in the data, etc.</p> <p>Interval data consists of a number of intervals that may or may not have values associated with them.</p> <p>Examples of IntervalData with associated values are certain ventilator settings (e.g. end-expiratory pressure) and the position of a patient. Examples of IntervalData without associated values are indicators of maneouvres (e.g. a breath hold) or detected occurences (e.g. a breath).</p> <p>Interval data can be selected by time through the <code>select_by_time(start_time, end_time)</code> method. Alternatively, <code>t[start_time:end_time]</code> can be used.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Computer readable label identifying this dataset.</p> required <code>name</code> <code>str</code> <p>Human readable name for the data.</p> required <code>unit</code> <code>str | None</code> <p>The unit of the data, if applicable.</p> required <code>category</code> <code>str</code> <p>Category the data falls into, e.g. 'breath'.</p> required <code>intervals</code> <code>list[Interval | tuple[float, float]]</code> <p>A list of intervals (tuples containing a start time and end time).</p> required <code>values</code> <code>list[Any] | None</code> <p>An optional list of values associated with each interval.</p> <code>None</code> <code>parameters</code> <code>dict[str, Any]</code> <p>Parameters used to derive the data.</p> <code>dict()</code> <code>derived_from</code> <code>list[Any]</code> <p>Traceback of intermediates from which the current data was derived.</p> <code>list()</code> <code>description</code> <code>str</code> <p>Extended human readible description of the data.</p> <code>''</code> <code>default_partial_inclusion</code> <code>bool</code> <p>Whether to include a trimmed version of an interval when selecting data</p> <code>False</code>"},{"location":"APIindex/#eitprocessing.datahandling.intervaldata.IntervalData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.intervaldata.IntervalData.has_values","title":"has_values  <code>property</code>","text":"<pre><code>has_values: bool\n</code></pre> <p>True if the IntervalData has values, False otherwise.</p>"},{"location":"APIindex/#eitprocessing.datahandling.intervaldata.IntervalData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(start: int | None = None, end: int | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_index(\n    self,\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"De facto implementation of the `__getitem__` function.\n\n    This function can also be called directly to add a label to the sliced\n    object. Otherwise a default label describing the slice and original\n    object is attached.\n    \"\"\"\n    if start is None and end is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    start = start if start is not None else 0\n    end = end if end is not None else len(self)\n    newlabel = newlabel or self.label\n\n    return self._sliced_copy(start_index=start, end_index=end, newlabel=newlabel)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.intervaldata.IntervalData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.intervaldata.IntervalData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(start_time: float | None = None, end_time: float | None = None, partial_inclusion: bool | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>Create a new copy of the object, selecting data between start_time and end_time.</p> <p>This function returns a shortened copy of the object, containing data from the specified start_time to end_time.</p> <p>If <code>partial_inclusion</code> is set to <code>True</code>, any intervals that overlap with the start_time or end_time are included in the selection, and their times are adjusted to fit within the specified range. If <code>partial_inclusion</code> is <code>False</code>, intervals that overlap the start or end times are excluded from the selection.</p> <p>For example: - Set <code>partial_inclusion</code> to <code>True</code> for cases like \"set_driving_pressure\" where you want to include settings that were active before the start_time. - Set <code>partial_inclusion</code> to <code>False</code> for cases like \"detected_breaths\" where you want to exclude partial data that doesn't fully fit within the time range.</p> <p>Note that the end_time is always included in the selection if it is present in the original object.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float | None</code> <p>The earliest time point to include in the copy.</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>The latest time point to include in the copy.</p> <code>None</code> <code>partial_inclusion</code> <code>bool | None</code> <p>Whether to include intervals that overlap with the start_time or end_time.</p> <code>None</code> <code>newlabel</code> <code>str | None</code> <p>A new label for the copied object.</p> <code>None</code> Source code in <code>eitprocessing/datahandling/intervaldata.py</code> <pre><code>def select_by_time(\n    self,\n    start_time: float | None = None,\n    end_time: float | None = None,\n    partial_inclusion: bool | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"Create a new copy of the object, selecting data between start_time and end_time.\n\n    This function returns a shortened copy of the object, containing data from the specified start_time to end_time.\n\n    If `partial_inclusion` is set to `True`, any intervals that overlap with the start_time or end_time are included\n    in the selection, and their times are adjusted to fit within the specified range. If `partial_inclusion` is\n    `False`, intervals that overlap the start or end times are excluded from the selection.\n\n    For example:\n    - Set `partial_inclusion` to `True` for cases like \"set_driving_pressure\" where you want to include settings\n    that were active before the start_time.\n    - Set `partial_inclusion` to `False` for cases like \"detected_breaths\" where you want to exclude partial data\n    that doesn't fully fit within the time range.\n\n    Note that the end_time is always included in the selection if it is present in the original object.\n\n    Args:\n        start_time: The earliest time point to include in the copy.\n        end_time: The latest time point to include in the copy.\n        partial_inclusion: Whether to include intervals that overlap with the start_time or end_time.\n        newlabel: A new label for the copied object.\n    \"\"\"\n    newlabel = newlabel or self.label\n\n    if start_time is None and end_time is None:\n        copy_ = copy.deepcopy(self)\n        copy_.derived_from.append(self)\n        copy_.label = newlabel\n        return copy_\n\n    partial_inclusion = partial_inclusion or self.default_partial_inclusion\n\n    selection_start = start_time or self.intervals[0].start_time\n    selection_end = end_time or self.intervals[-1].end_time\n\n    numbered_filtered_intervals = [\n        (i, self._replace_start_end_time(interval, selection_start, selection_end))\n        for i, interval in enumerate(self.intervals)\n        if self._keep_overlapping(interval, selection_start, selection_end, partial_inclusion)\n    ]\n\n    try:\n        indices, filtered_intervals = zip(*numbered_filtered_intervals, strict=True)\n        values = [self.values[i] for i in indices] if self.has_values else None\n    except ValueError:\n        filtered_intervals = []\n        values = [] if self.has_values else None\n\n    return type(self)(\n        label=newlabel,\n        name=self.name,\n        unit=self.unit,\n        category=self.category,\n        derived_from=[*self.derived_from, self],\n        intervals=list(filtered_intervals),\n        values=values,\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading","title":"loading","text":""},{"location":"APIindex/#eitprocessing.datahandling.loading.load_eit_data","title":"load_eit_data","text":"<pre><code>load_eit_data(path: str | Path | list[str | Path], vendor: Vendor | str, label: str | None = None, name: str | None = None, description: str = '', sample_frequency: float | None = None, first_frame: int = 0, max_frames: int | None = None) -&gt; Sequence\n</code></pre> <p>Load EIT data from path(s).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | list[str | Path]</code> <p>relative or absolute path(s) to data file.</p> required <code>vendor</code> <code>Vendor | str</code> <p>vendor indicating the device used. Note: for load functions of specific vendors (e.g. <code>load_draeger_data</code>), this argument is defaulted to the correct vendor.</p> required <code>label</code> <code>str | None</code> <p>short description of sequence for computer interpretation. Defaults to \"Sequence_\". <code>None</code> <code>name</code> <code>str | None</code> <p>short description of sequence for human interpretation. Defaults to the same value as label.</p> <code>None</code> <code>description</code> <code>str</code> <p>long description of sequence for human interpretation.</p> <code>''</code> <code>sample_frequency</code> <code>float | None</code> <p>sample frequency at which the data was recorded. Default for Draeger: 20 Default for Timpel: 50 Default for Sentec: 50.2</p> <code>None</code> <code>first_frame</code> <code>int</code> <p>index of first frame to load. Defaults to 0.</p> <code>0</code> <code>max_frames</code> <code>int | None</code> <p>maximum number of frames to load. The actual number of frames can be lower than this if this would surpass the final frame.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>is raised when there is no loading method for</p> <p>Returns:</p> Name Type Description <code>Sequence</code> <code>Sequence</code> <p>a Sequence with the given label, name and description, containing the loaded data.</p> <p>Example: <pre><code>sequence = load_data([\"path/to/file1\", \"path/to/file2\"], vendor=\"sentec\", label=\"initial_measurement\")\npixel_impedance = sequence.eit_data[\"raw\"].pixel_impedance\n</code></pre></p> Source code in <code>eitprocessing/datahandling/loading/__init__.py</code> <pre><code>def load_eit_data(\n    path: str | Path | list[str | Path],\n    vendor: Vendor | str,\n    label: str | None = None,\n    name: str | None = None,\n    description: str = \"\",\n    sample_frequency: float | None = None,\n    first_frame: int = 0,\n    max_frames: int | None = None,\n) -&gt; Sequence:\n    \"\"\"Load EIT data from path(s).\n\n    Args:\n        path: relative or absolute path(s) to data file.\n        vendor: vendor indicating the device used.\n            Note: for load functions of specific vendors (e.g. `load_draeger_data`), this argument is defaulted to the\n            correct vendor.\n        label: short description of sequence for computer interpretation.\n            Defaults to \"Sequence_&lt;unique_id&gt;\".\n        name: short description of sequence for human interpretation.\n            Defaults to the same value as label.\n        description: long description of sequence for human interpretation.\n        sample_frequency: sample frequency at which the data was recorded.\n            Default for Draeger: 20\n            Default for Timpel: 50\n            Default for Sentec: 50.2\n        first_frame: index of first frame to load.\n            Defaults to 0.\n        max_frames: maximum number of frames to load.\n            The actual number of frames can be lower than this if this\n            would surpass the final frame.\n\n    Raises:\n        NotImplementedError: is raised when there is no loading method for\n        the given vendor.\n\n    Returns:\n        Sequence: a Sequence with the given label, name and description, containing the loaded data.\n\n    Example:\n    ```\n    sequence = load_data([\"path/to/file1\", \"path/to/file2\"], vendor=\"sentec\", label=\"initial_measurement\")\n    pixel_impedance = sequence.eit_data[\"raw\"].pixel_impedance\n    ```\n    \"\"\"\n    from eitprocessing.datahandling.loading import draeger, sentec, timpel  # not in top level to avoid circular import\n\n    vendor = _ensure_vendor(vendor)\n    load_from_single_path = {\n        Vendor.DRAEGER: draeger.load_from_single_path,\n        Vendor.TIMPEL: timpel.load_from_single_path,\n        Vendor.SENTEC: sentec.load_from_single_path,\n    }[vendor]\n\n    first_frame = _check_first_frame(first_frame)\n\n    paths = EITData.ensure_path_list(path)\n\n    eit_datasets: list[DataCollection] = []\n    continuous_datasets: list[DataCollection] = []\n    sparse_datasets: list[DataCollection] = []\n    interval_datasets: list[DataCollection] = []\n\n    for single_path in paths:\n        single_path.resolve(strict=True)  # raise error if any file does not exist\n\n    for single_path in paths:\n        loaded_data = load_from_single_path(\n            path=single_path,\n            sample_frequency=sample_frequency,\n            first_frame=first_frame,\n            max_frames=max_frames,\n        )\n\n        eit_datasets.append(loaded_data[\"eitdata_collection\"])\n        continuous_datasets.append(loaded_data[\"continuousdata_collection\"])\n        sparse_datasets.append(loaded_data[\"sparsedata_collection\"])\n        interval_datasets.append(loaded_data[\"intervaldata_collection\"])\n\n    return Sequence(\n        label=label,\n        name=name,\n        description=description,\n        eit_data=reduce(DataCollection.concatenate, eit_datasets),\n        continuous_data=reduce(DataCollection.concatenate, continuous_datasets),\n        sparse_data=reduce(DataCollection.concatenate, sparse_datasets),\n        interval_data=reduce(DataCollection.concatenate, interval_datasets),\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader","title":"binreader","text":""},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader","title":"BinReader  <code>dataclass</code>","text":"<pre><code>BinReader(file_handle: BufferedReader | mmap, endian: Literal['little', 'big'] | None = None)\n</code></pre> <p>Helper class for reading binary files from disk.</p> <p>Parameters:</p> Name Type Description Default <code>file_handle</code> <code>BufferedReader | mmap</code> <p>a buffered reader handle, e.g. the result of the <code>open()</code> function.</p> required <code>endian</code> <code>Literal['little', 'big'] | None</code> <p>the endianness of the binary data. Either 'little' or 'big', or None.</p> <code>None</code>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.read_single","title":"read_single","text":"<pre><code>read_single(type_code: str, cast: type[T]) -&gt; T\n</code></pre> <p>Read and return a single unit of the given type code.</p> <p>The type of data to be read should be provided as a single typ code. See https://docs.python.org/3.10/library/struct.html#byte-order-size-and-alignment for a list of available type codes.</p> <p>A unit returns a single value, and can be one or more bytes of data. E.g. requesting a signed 32-bit integer ('q') will result in reading 8 bytes of data.</p> <p><code>cast</code> should be a type, e.g. <code>int</code> or <code>float</code> used to cast the value to the proper type.</p> <p>Parameters:</p> Name Type Description Default <code>type_code</code> <code>str</code> <p>singular type code.</p> required <code>cast</code> <code>type[T]</code> <p>the associated type.</p> required Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def read_single(self, type_code: str, cast: type[T]) -&gt; T:\n    \"\"\"Read and return a single unit of the given type code.\n\n    The type of data to be read should be provided as a single typ code. See\n    https://docs.python.org/3.10/library/struct.html#byte-order-size-and-alignment for a list of available type\n    codes.\n\n    A unit returns a single value, and can be one or more bytes of data. E.g. requesting a signed 32-bit integer\n    ('q') will result in reading 8 bytes of data.\n\n    `cast` should be a type, e.g. `int` or `float` used to cast the value to the proper type.\n\n    Args:\n        type_code: singular type code.\n        cast: the associated type.\n    \"\"\"\n    data = self._read_full_type_code(type_code)\n    return cast(data[0])\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.read_list","title":"read_list","text":"<pre><code>read_list(type_code: str, cast: type[T], length: int) -&gt; list[T]\n</code></pre> <p>Read multiple values of the same type and return as list.</p> <p>See <code>read_single()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>type_code</code> <code>str</code> <p>singular type code.</p> required <code>cast</code> <code>type[T]</code> <p>the associated type.</p> required <code>length</code> <code>int</code> <p>number of values to be read.</p> required Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def read_list(self, type_code: str, cast: type[T], length: int) -&gt; list[T]:\n    \"\"\"Read multiple values of the same type and return as list.\n\n    See `read_single()`.\n\n    Args:\n        type_code: singular type code.\n        cast: the associated type.\n        length: number of values to be read.\n    \"\"\"\n    full_type_code = f\"{length}{type_code}\"\n    data = self._read_full_type_code(full_type_code)\n    return [cast(d) for d in data]\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.read_array","title":"read_array","text":"<pre><code>read_array(type_code: str, cast: type[N], length: int) -&gt; NDArray[N]\n</code></pre> <p>Read multiple values of the same type and return as NumPy array.</p> <p>See <code>read_list()</code>.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def read_array(\n    self,\n    type_code: str,\n    cast: type[N],\n    length: int,\n) -&gt; NDArray[N]:\n    \"\"\"Read multiple values of the same type and return as NumPy array.\n\n    See `read_list()`.\n    \"\"\"\n    full_type_code = f\"{length}{type_code}\"\n    data = self._read_full_type_code(full_type_code)\n    return np.array(data, dtype=cast)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.read_string","title":"read_string","text":"<pre><code>read_string(length: int = 1) -&gt; str\n</code></pre> <p>Read and return a string with a given length.</p> <p>Reads <code>length</code> characters of type code 's' and returns as a string. When length is not provided, a single character is returned.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>number of characters.</p> <code>1</code> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def read_string(self, length: int = 1) -&gt; str:\n    \"\"\"Read and return a string with a given length.\n\n    Reads `length` characters of type code 's' and returns as a string. When length is not provided, a single\n    character is returned.\n\n    Args:\n        length: number of characters.\n    \"\"\"\n    full_type_code = f\"{length}s\"\n    data = self._read_full_type_code(full_type_code)\n    return data[0].decode().rstrip()\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.float32","title":"float32","text":"<pre><code>float32() -&gt; float\n</code></pre> <p>Read and return a single signed 32-bit floating point value.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def float32(self) -&gt; float:\n    \"\"\"Read and return a single signed 32-bit floating point value.\"\"\"\n    return self.read_single(type_code=\"f\", cast=float)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.float64","title":"float64","text":"<pre><code>float64() -&gt; float\n</code></pre> <p>Read and return a single signed 64-bit floating point value.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def float64(self) -&gt; float:\n    \"\"\"Read and return a single signed 64-bit floating point value.\"\"\"\n    return self.read_single(type_code=\"d\", cast=float)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.npfloat32","title":"npfloat32","text":"<pre><code>npfloat32(length: int = 1) -&gt; NDArray[float32]\n</code></pre> <p>Read and return an array of signed 32-bit floating point values.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def npfloat32(self, length: int = 1) -&gt; NDArray[np.float32]:\n    \"\"\"Read and return an array of signed 32-bit floating point values.\"\"\"\n    return self.read_array(type_code=\"f\", cast=np.float32, length=length)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.npfloat64","title":"npfloat64","text":"<pre><code>npfloat64(length: int = 1) -&gt; NDArray[float64]\n</code></pre> <p>Read and return an array of signed 64-bit floating point values.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def npfloat64(self, length: int = 1) -&gt; NDArray[np.float64]:\n    \"\"\"Read and return an array of signed 64-bit floating point values.\"\"\"\n    return self.read_array(type_code=\"d\", cast=np.float64, length=length)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.int32","title":"int32","text":"<pre><code>int32() -&gt; int\n</code></pre> <p>Read and return a single signed 32-bit integer value.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def int32(self) -&gt; int:\n    \"\"\"Read and return a single signed 32-bit integer value.\"\"\"\n    return self.read_single(type_code=\"i\", cast=int)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.npint32","title":"npint32","text":"<pre><code>npint32(length: int = 1) -&gt; NDArray[int32]\n</code></pre> <p>Read and return an array of signed 32-bit integer values.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def npint32(self, length: int = 1) -&gt; NDArray[np.int32]:\n    \"\"\"Read and return an array of signed 32-bit integer values.\"\"\"\n    return self.read_array(type_code=\"i\", cast=np.int32, length=length)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.uint8","title":"uint8","text":"<pre><code>uint8() -&gt; int\n</code></pre> <p>Read and return a single unsigned 8-bit integer value.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def uint8(self) -&gt; int:\n    \"\"\"Read and return a single unsigned 8-bit integer value.\"\"\"\n    return self.read_single(type_code=\"B\", cast=int)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.uint16","title":"uint16","text":"<pre><code>uint16() -&gt; int\n</code></pre> <p>Read and return a single unsigned 16-bit integer value.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def uint16(self) -&gt; int:\n    \"\"\"Read and return a single unsigned 16-bit integer value.\"\"\"\n    return self.read_single(type_code=\"H\", cast=int)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.uint32","title":"uint32","text":"<pre><code>uint32() -&gt; int\n</code></pre> <p>Read and return a single unsigned 32-bit integer value.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def uint32(self) -&gt; int:\n    \"\"\"Read and return a single unsigned 32-bit integer value.\"\"\"\n    return self.read_single(type_code=\"I\", cast=int)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.binreader.BinReader.uint64","title":"uint64","text":"<pre><code>uint64() -&gt; int\n</code></pre> <p>Read and return a single unsigned 64-bit integer value.</p> Source code in <code>eitprocessing/datahandling/loading/binreader.py</code> <pre><code>def uint64(self) -&gt; int:\n    \"\"\"Read and return a single unsigned 64-bit integer value.\"\"\"\n    return self.read_single(type_code=\"Q\", cast=int)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.draeger","title":"draeger","text":""},{"location":"APIindex/#eitprocessing.datahandling.loading.draeger.load_from_single_path","title":"load_from_single_path","text":"<pre><code>load_from_single_path(path: Path, sample_frequency: float | None = 20, first_frame: int = 0, max_frames: int | None = None) -&gt; dict[str, DataCollection]\n</code></pre> <p>Load Dr\u00e4ger EIT data from path.</p> Source code in <code>eitprocessing/datahandling/loading/draeger.py</code> <pre><code>def load_from_single_path(\n    path: Path,\n    sample_frequency: float | None = 20,\n    first_frame: int = 0,\n    max_frames: int | None = None,\n) -&gt; dict[str, DataCollection]:\n    \"\"\"Load Dr\u00e4ger EIT data from path.\"\"\"\n    file_size = path.stat().st_size\n    if file_size % _FRAME_SIZE_BYTES:\n        msg = (\n            f\"File size {file_size} of file {path!s} not divisible by {_FRAME_SIZE_BYTES}.\\n\"\n            f\"Make sure this is a valid and uncorrupted Dr\u00e4ger data file.\"\n        )\n        raise OSError(msg)\n    total_frames = file_size // _FRAME_SIZE_BYTES\n\n    if (f0 := first_frame) &gt; (fn := total_frames):\n        msg = f\"Invalid input: `first_frame` ({f0}) is larger than the total number of frames in the file ({fn}).\"\n        raise ValueError(msg)\n\n    n_frames = min(total_frames - first_frame, max_frames or sys.maxsize)\n\n    if max_frames and max_frames != n_frames:\n        msg = (\n            f\"The number of frames requested ({max_frames}) is larger \"\n            f\"than the available number ({n_frames}) of frames after \"\n            f\"the first frame selected ({first_frame}, total frames: \"\n            f\"{total_frames}).\\n {n_frames} frames will be loaded.\"\n        )\n        warnings.warn(msg)\n\n    # We need to load 1 frame before first actual frame to check if there is an event marker. Data for the pre-first\n    # (dummy) frame will be removed from self at the end of this function.\n    load_dummy_frame = first_frame &gt; 0\n    first_frame_to_load = first_frame - 1 if load_dummy_frame else 0\n\n    pixel_impedance = np.zeros((n_frames, 32, 32))\n    time = np.zeros((n_frames,))\n    events: list[tuple[float, Event]] = []\n    phases: list[tuple[float, int]] = []\n    medibus_data = np.zeros((52, n_frames))\n\n    with path.open(\"br\") as fo, mmap.mmap(fo.fileno(), length=0, access=mmap.ACCESS_READ) as fh:\n        fh.seek(first_frame_to_load * _FRAME_SIZE_BYTES)\n        reader = BinReader(fh)\n        previous_marker = None\n\n        first_index = -1 if load_dummy_frame else 0\n        for index in range(first_index, n_frames):\n            previous_marker = _read_frame(\n                reader,\n                index,\n                time,\n                pixel_impedance,\n                medibus_data,\n                events,\n                phases,\n                previous_marker,\n            )\n\n    if not sample_frequency:\n        sample_frequency = DRAEGER_SAMPLE_FREQUENCY\n\n    eit_data = EITData(\n        vendor=Vendor.DRAEGER,\n        path=path,\n        sample_frequency=sample_frequency,\n        nframes=n_frames,\n        time=time,\n        label=\"raw\",\n        pixel_impedance=pixel_impedance,\n    )\n    eitdata_collection = DataCollection(EITData, raw=eit_data)\n\n    (\n        continuousdata_collection,\n        sparsedata_collection,\n    ) = _convert_medibus_data(medibus_data, time, sample_frequency)\n    intervaldata_collection = DataCollection(IntervalData)\n    # TODO: move some medibus data to sparse / interval\n    # TODO: move phases and events to sparse / interval\n\n    continuousdata_collection.add(\n        ContinuousData(\n            label=\"global_impedance_(raw)\",\n            name=\"Global impedance (raw)\",\n            unit=\"a.u.\",\n            category=\"impedance\",\n            derived_from=[eit_data],\n            time=eit_data.time,\n            values=eit_data.calculate_global_impedance(),\n            sample_frequency=sample_frequency,\n        ),\n    )\n    sparsedata_collection.add(\n        SparseData(\n            label=\"minvalues_(draeger)\",\n            name=\"Minimum values detected by Draeger device.\",\n            unit=None,\n            category=\"minvalue\",\n            derived_from=[eit_data],\n            time=np.array([t for t, d in phases if d == -1]),\n        ),\n    )\n    sparsedata_collection.add(\n        SparseData(\n            label=\"maxvalues_(draeger)\",\n            name=\"Maximum values detected by Draeger device.\",\n            unit=None,\n            category=\"maxvalue\",\n            derived_from=[eit_data],\n            time=np.array([t for t, d in phases if d == 1]),\n        ),\n    )\n    if len(events):\n        time_, events_ = zip(*events, strict=True)\n        time = np.array(time_)\n        events = list(events_)\n    else:\n        time, events = np.array([]), []\n    sparsedata_collection.add(\n        SparseData(\n            label=\"events_(draeger)\",\n            name=\"Events loaded from Draeger data\",\n            unit=None,\n            category=\"event\",\n            derived_from=[eit_data],\n            time=time,\n            values=events,\n        ),\n    )\n\n    return {\n        \"eitdata_collection\": eitdata_collection,\n        \"continuousdata_collection\": continuousdata_collection,\n        \"sparsedata_collection\": sparsedata_collection,\n        \"intervaldata_collection\": intervaldata_collection,\n    }\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.sentec","title":"sentec","text":""},{"location":"APIindex/#eitprocessing.datahandling.loading.sentec.Domain","title":"Domain","text":"<p>               Bases: <code>IntEnum</code></p> <p>Domain loaded data falls in.</p>"},{"location":"APIindex/#eitprocessing.datahandling.loading.sentec.MeasurementDataID","title":"MeasurementDataID","text":"<p>               Bases: <code>IntEnum</code></p> <p>ID of measured data.</p>"},{"location":"APIindex/#eitprocessing.datahandling.loading.sentec.ConfigurationDataID","title":"ConfigurationDataID","text":"<p>               Bases: <code>IntEnum</code></p> <p>ID of configuration data.</p>"},{"location":"APIindex/#eitprocessing.datahandling.loading.sentec.load_from_single_path","title":"load_from_single_path","text":"<pre><code>load_from_single_path(path: Path, sample_frequency: float | None = 50.2, first_frame: int = 0, max_frames: int | None = None) -&gt; dict[str, DataCollection]\n</code></pre> <p>Load Sentec EIT data from path.</p> Source code in <code>eitprocessing/datahandling/loading/sentec.py</code> <pre><code>def load_from_single_path(  # noqa: C901, PLR0912\n    path: Path,\n    sample_frequency: float | None = 50.2,\n    first_frame: int = 0,\n    max_frames: int | None = None,\n) -&gt; dict[str, DataCollection]:\n    \"\"\"Load Sentec EIT data from path.\"\"\"\n    with path.open(\"br\") as fo, mmap.mmap(fo.fileno(), length=0, access=mmap.ACCESS_READ) as fh:\n        file_length = os.fstat(fo.fileno()).st_size\n        reader = BinReader(fh, endian=\"little\")\n        version = reader.uint8()\n\n        time: list[float] = []\n        max_n_images = int(file_length / 32 / 32 / 4)\n        image = np.full(shape=(max_n_images, 32, 32), fill_value=np.nan)\n        index = 0\n        n_images_added = 0\n\n        # while there are still data to be read and the number of read data points is higher\n        # than the maximum specified, keep reading\n        while fh.tell() &lt; file_length and (max_frames is None or len(time) &lt; max_frames):\n            _ = reader.uint64()  # skip timestamp reading\n            domain_id = reader.uint8()\n            number_data_fields = reader.uint8()\n\n            for _ in range(number_data_fields):\n                data_id = reader.uint8()\n                payload_size = reader.uint16()\n\n                if payload_size == 0:\n                    continue\n\n                if domain_id == Domain.MEASUREMENT:\n                    if data_id == MeasurementDataID.TIMESTAMP:\n                        time_of_caption = reader.uint32()\n                        time.append(time_of_caption)\n\n                    elif data_id == MeasurementDataID.ZERO_REF_IMAGE:\n                        index += 1\n\n                        ref = _read_frame(\n                            fh,\n                            version,\n                            index,\n                            payload_size,\n                            reader,\n                            first_frame,\n                        )\n\n                        if ref is not None:\n                            image[n_images_added, :, :] = ref\n                            n_images_added += 1\n                    else:\n                        fh.seek(payload_size, os.SEEK_CUR)\n\n                # read the sample frequency from the file, if present\n                # (domain 64 = configuration, data 5 = sample frequency)\n                elif domain_id == Domain.CONFIGURATION and data_id == ConfigurationDataID.SAMPLE_FREQUENCY:\n                    sample_frequency = np.round(reader.float32(), 4)\n                    msg = (\n                        \"Sample frequency value found in file. \"\n                        f\"The sample frequency value will be set to {sample_frequency:.2f}\"\n                    )\n                    warnings.warn(msg)\n\n                else:\n                    fh.seek(payload_size, os.SEEK_CUR)\n    image = image[:n_images_added, :, :]\n    n_frames = len(image) if image is not None else 0\n\n    if (f0 := first_frame) &gt; (fn := index):\n        msg = f\"Invalid input: `first_frame` ({f0}) is larger than the total number of frames in the file ({fn}).\"\n        raise ValueError(msg)\n\n    if max_frames and n_frames != max_frames:\n        msg = (\n            f\"The number of frames requested ({max_frames}) is larger \"\n            f\"than the available number ({n_frames}) of frames after \"\n            f\"the first frame selected ({first_frame}, total frames: \"\n            f\"{index}).\\n {n_frames} frames will be loaded.\"\n        )\n        warnings.warn(msg)\n\n    if not sample_frequency:\n        sample_frequency = SENTEC_SAMPLE_FREQUENCY\n\n    eitdata_collection = DataCollection(EITData)\n    eitdata_collection.add(\n        EITData(\n            vendor=Vendor.SENTEC,\n            path=path,\n            sample_frequency=sample_frequency,\n            nframes=n_frames,\n            time=np.unwrap(np.array(time), period=np.iinfo(np.uint32).max) / 1000000,\n            label=\"raw\",\n            pixel_impedance=image,\n        ),\n    )\n\n    return {\n        \"eitdata_collection\": eitdata_collection,\n        \"continuousdata_collection\": DataCollection(ContinuousData),\n        \"sparsedata_collection\": DataCollection(SparseData),\n        \"intervaldata_collection\": DataCollection(IntervalData),\n    }\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.loading.timpel","title":"timpel","text":""},{"location":"APIindex/#eitprocessing.datahandling.loading.timpel.load_from_single_path","title":"load_from_single_path","text":"<pre><code>load_from_single_path(path: Path, sample_frequency: float | None = 20, first_frame: int = 0, max_frames: int | None = None) -&gt; dict[str, DataCollection]\n</code></pre> <p>Load Timpel EIT data from path.</p> Source code in <code>eitprocessing/datahandling/loading/timpel.py</code> <pre><code>def load_from_single_path(\n    path: Path,\n    sample_frequency: float | None = 20,\n    first_frame: int = 0,\n    max_frames: int | None = None,\n) -&gt; dict[str, DataCollection]:\n    \"\"\"Load Timpel EIT data from path.\"\"\"\n    if not sample_frequency:\n        sample_frequency = TIMPEL_SAMPLE_FREQUENCY\n\n    try:\n        data: NDArray = np.loadtxt(\n            str(path),\n            dtype=float,\n            delimiter=\",\",\n            skiprows=first_frame,\n            max_rows=max_frames,\n        )\n    except UnicodeDecodeError as e:\n        msg = (\n            f\"File {path} could not be read as Timpel data.\\n\"\n            \"Make sure this is a valid and uncorrupted Timpel data file.\\n\"\n            f\"Original error message: {e}\"\n        )\n        raise OSError(msg) from e\n\n    if data.shape[1] != _COLUMN_WIDTH:\n        msg = (\n            f\"Input does not have a width of {_COLUMN_WIDTH} columns.\\n\"\n            \"Make sure this is a valid and uncorrupted Timpel data file.\"\n        )\n        raise OSError(msg)\n    if data.shape[0] == 0:\n        msg = f\"Invalid input: `first_frame` {first_frame} is larger than the total number of frames in the file.\"\n        raise ValueError(msg)\n\n    if max_frames and data.shape[0] == max_frames:\n        nframes = max_frames\n    else:\n        if max_frames:\n            warnings.warn(\n                f\"The number of frames requested ({max_frames}) is larger \"\n                f\"than the available number ({data.shape[0]}) of frames after \"\n                f\"the first frame selected ({first_frame}).\\n\"\n                f\"{data.shape[0]} frames have been loaded.\",\n            )\n        nframes = data.shape[0]\n\n    # TODO (#80): QUESTION: check whether below issue was only a Drager problem or also\n    # applicable to Timpel.\n    # The implemented method seems convoluted: it's easier to create an array\n    # with nframes and add a time_offset. However, this results in floating\n    # point errors, creating issues with comparing times later on.\n    time = np.arange(nframes + first_frame) / sample_frequency\n    time = time[first_frame:]\n\n    pixel_impedance = data[:, :1024]\n    pixel_impedance = np.reshape(pixel_impedance, newshape=(-1, 32, 32), order=\"C\")\n\n    pixel_impedance = np.where(pixel_impedance == _NAN_VALUE, np.nan, pixel_impedance)\n\n    eit_data = EITData(\n        vendor=Vendor.TIMPEL,\n        label=\"raw\",\n        path=path,\n        nframes=nframes,\n        time=time,\n        sample_frequency=sample_frequency,\n        pixel_impedance=pixel_impedance,\n    )\n    eitdata_collection = DataCollection(EITData, raw=eit_data)\n\n    # extract waveform data\n    # TODO: properly export waveform data\n\n    continuousdata_collection = DataCollection(ContinuousData)\n    continuousdata_collection.add(\n        ContinuousData(\n            \"global_impedance_(raw)\",\n            \"Global impedance\",\n            \"a.u.\",\n            \"global_impedance\",\n            \"Global impedance calculated from raw EIT data\",\n            time=time,\n            values=eit_data.calculate_global_impedance(),\n            sample_frequency=sample_frequency,\n        ),\n    )\n    continuousdata_collection.add(\n        ContinuousData(\n            label=\"airway_pressure_(timpel)\",\n            name=\"Airway pressure\",\n            unit=\"cmH2O\",\n            category=\"pressure\",\n            description=\"Airway pressure measured by Timpel device\",\n            time=time,\n            values=data[:, 1024],\n            sample_frequency=sample_frequency,\n        ),\n    )\n\n    continuousdata_collection.add(\n        ContinuousData(\n            label=\"flow_(timpel)\",\n            name=\"Flow\",\n            unit=\"L/s\",\n            category=\"flow\",\n            description=\"Flow measures by Timpel device\",\n            time=time,\n            values=data[:, 1025],\n            sample_frequency=sample_frequency,\n        ),\n    )\n\n    continuousdata_collection.add(\n        ContinuousData(\n            label=\"volume_(timpel)\",\n            name=\"Volume\",\n            unit=\"L\",\n            category=\"volume\",\n            description=\"Volume measured by Timpel device\",\n            time=time,\n            values=data[:, 1026],\n            sample_frequency=sample_frequency,\n        ),\n    )\n\n    # extract sparse data\n    sparsedata_collection = DataCollection(SparseData)\n\n    min_indices = np.nonzero(data[:, 1027] == 1)[0]\n    sparsedata_collection.add(\n        SparseData(\n            label=\"minvalues_(timpel)\",\n            name=\"Minimum values detected by Timpel device.\",\n            unit=None,\n            category=\"minvalue\",\n            derived_from=[eit_data],\n            time=time[min_indices],\n        ),\n    )\n\n    max_indices = np.nonzero(data[:, 1028] == 1)[0]\n    sparsedata_collection.add(\n        SparseData(\n            label=\"maxvalues_(timpel)\",\n            name=\"Maximum values detected by Timpel device.\",\n            unit=None,\n            category=\"maxvalue\",\n            derived_from=[eit_data],\n            time=time[max_indices],\n        ),\n    )\n\n    gi = continuousdata_collection[\"global_impedance_(raw)\"].values\n\n    time_ranges, breaths = _make_breaths(time, min_indices, max_indices, gi)\n    intervaldata_collection = DataCollection(IntervalData)\n    intervaldata_collection.add(\n        IntervalData(\n            label=\"breaths_(timpel)\",\n            name=\"Breaths (Timpel)\",\n            unit=None,\n            category=\"breaths\",\n            intervals=time_ranges,\n            values=breaths,\n            default_partial_inclusion=False,\n        ),\n    )\n\n    qrs_indices = np.nonzero(data[:, 1029] == 1)[0]\n    sparsedata_collection.add(\n        SparseData(\n            label=\"qrscomplexes_(timpel)\",\n            name=\"QRS complexes detected by Timpel device\",\n            unit=None,\n            category=\"qrs_complex\",\n            derived_from=[eit_data],\n            time=time[qrs_indices],\n        ),\n    )\n\n    return {\n        \"eitdata_collection\": eitdata_collection,\n        \"continuousdata_collection\": continuousdata_collection,\n        \"sparsedata_collection\": sparsedata_collection,\n        \"intervaldata_collection\": intervaldata_collection,\n    }\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.mixins","title":"mixins","text":""},{"location":"APIindex/#eitprocessing.datahandling.mixins.equality","title":"equality","text":""},{"location":"APIindex/#eitprocessing.datahandling.mixins.equality.Equivalence","title":"Equivalence","text":"<p>Mixin class that adds an equality and equivalence check.</p>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.equality.Equivalence.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.equality.EquivalenceError","title":"EquivalenceError","text":"<p>               Bases: <code>TypeError</code>, <code>ValueError</code></p> <p>Raised if objects are not equivalent.</p>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing","title":"slicing","text":""},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.SelectByIndex","title":"SelectByIndex","text":"<p>               Bases: <code>ABC</code></p> <p>Adds slicing functionality to subclass by implementing <code>__getitem__</code>.</p> <p>Subclasses must implement a <code>_sliced_copy</code> function that defines what should happen when the object is sliced. This class ensures that when calling a slice between square brackets (as e.g. done for lists) then return the expected sliced object.</p>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.SelectByIndex.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(start: int | None = None, end: int | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_index(\n    self,\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"De facto implementation of the `__getitem__` function.\n\n    This function can also be called directly to add a label to the sliced\n    object. Otherwise a default label describing the slice and original\n    object is attached.\n    \"\"\"\n    if start is None and end is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    start = start if start is not None else 0\n    end = end if end is not None else len(self)\n    newlabel = newlabel or self.label\n\n    return self._sliced_copy(start_index=start, end_index=end, newlabel=newlabel)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.HasTimeIndexer","title":"HasTimeIndexer","text":"<p>Gives access to a TimeIndexer object that can be used to slice by time.</p>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.HasTimeIndexer.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.SelectByTime","title":"SelectByTime","text":"<p>               Bases: <code>SelectByIndex</code>, <code>HasTimeIndexer</code></p> <p>Adds methods for slicing by time rather than index.</p>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.SelectByTime.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.SelectByTime.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(start: int | None = None, end: int | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_index(\n    self,\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"De facto implementation of the `__getitem__` function.\n\n    This function can also be called directly to add a label to the sliced\n    object. Otherwise a default label describing the slice and original\n    object is attached.\n    \"\"\"\n    if start is None and end is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    start = start if start is not None else 0\n    end = end if end is not None else len(self)\n    newlabel = newlabel or self.label\n\n    return self._sliced_copy(start_index=start, end_index=end, newlabel=newlabel)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.SelectByTime.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(start_time: float | None = None, end_time: float | None = None, start_inclusive: bool = False, end_inclusive: bool = False, label: str | None = None) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float | None</code> <p>first time point to include. Defaults to first frame of sequence.</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>last time point. Defaults to last frame of sequence.</p> <code>None</code> <code>start_inclusive</code> <code>default</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <code>False</code> <code>label</code> <code>str | None</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A shortened copy of the object.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_time(  # noqa: D417\n    self,\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self:\n    \"\"\"Get a shortened copy of the object, starting from start_time and ending at end_time.\n\n    Given a start and end time stamp (i.e. its value, not its index),\n    return a slice of the original object, which must contain a time axis.\n\n    Args:\n        start_time: first time point to include. Defaults to first frame of sequence.\n        end_time: last time point. Defaults to last frame of sequence.\n        start_inclusive (default: `True`), end_inclusive (default `False`):\n            these arguments control the behavior if the given time stamp\n            does not match exactly with an existing time stamp of the input.\n            if `True`: the given time stamp will be inside the sliced object.\n            if `False`: the given time stamp will be outside the sliced object.\n        label: Description. Defaults to None, which will create a label based\n            on the original object label and the frames by which it is sliced.\n\n    Raises:\n        TypeError: if `self` does not contain a `time` attribute.\n        ValueError: if time stamps are not sorted.\n\n    Returns:\n        A shortened copy of the object.\n    \"\"\"\n    if not hasattr(self, \"time\"):\n        msg = f\"Object {self} has no time axis.\"\n        raise TypeError(msg)\n\n    if len(self.time) == 0:\n        # TODO: make proper new instances when not slicing\n        return copy.deepcopy(self)\n\n    if start_time is None and end_time is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    if not np.all(np.sort(self.time) == self.time):\n        msg = f\"Time stamps for {self} are not sorted and therefore data cannot be selected by time.\"\n        raise ValueError(msg)\n\n    if start_time is None or start_time &lt; self.time[0]:\n        start_index = 0\n    elif start_inclusive:\n        start_index = bisect.bisect_right(self.time, start_time) - 1\n    else:\n        start_index = bisect.bisect_left(self.time, start_time)\n\n    if end_time is None:\n        end_index = len(self.time)\n    elif end_inclusive:\n        end_index = bisect.bisect_left(self.time, end_time) + 1\n    else:\n        end_index = bisect.bisect_left(self.time, end_time)\n\n    return self.select_by_index(\n        start=start_index,\n        end=end_index,\n        newlabel=label,\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.mixins.slicing.TimeIndexer","title":"TimeIndexer  <code>dataclass</code>","text":"<pre><code>TimeIndexer(obj: HasTimeIndexer)\n</code></pre> <p>Helper class for slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.sequence","title":"sequence","text":""},{"location":"APIindex/#eitprocessing.datahandling.sequence.Sequence","title":"Sequence  <code>dataclass</code>","text":"<pre><code>Sequence(label: str | None = None, name: str | None = None, description: str = '', eit_data: DataCollection = lambda: DataCollection(EITData)(), continuous_data: DataCollection = lambda: DataCollection(ContinuousData)(), sparse_data: DataCollection = lambda: DataCollection(SparseData)(), interval_data: DataCollection = lambda: DataCollection(IntervalData)())\n</code></pre> <p>               Bases: <code>Equivalence</code>, <code>SelectByTime</code></p> <p>Sequence of timepoints containing respiratory data.</p> <p>A Sequence object is a representation of data points over time. These data can consist of any combination of EIT frames (<code>EITData</code>), waveform data (<code>ContinuousData</code>) from different sources, or individual events (<code>SparseData</code>) occurring at any given timepoint. A Sequence can consist of an entire measurement, a section of a measurement, a single breath, or even a portion of a breath. A Sequence can consist of multiple sets of each type of data from the same time-points or can be a single measurement from just one source.</p> <p>A Sequence can be split up into separate sections of a measurement or multiple (similar) Sequence objects can be merged together to form a single Sequence.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str | None</code> <p>Computer readable naming of the instance.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Human readable naming of the instance.</p> <code>None</code> <code>description</code> <code>str</code> <p>Human readable extended description of the data.</p> <code>''</code> <code>eit_data</code> <code>DataCollection</code> <p>Collection of one or more sets of EIT data frames.</p> <code>lambda: DataCollection(EITData)()</code> <code>continuous_data</code> <code>DataCollection</code> <p>Collection of one or more sets of continuous data points.</p> <code>lambda: DataCollection(ContinuousData)()</code> <code>sparse_data</code> <code>DataCollection</code> <p>Collection of one or more sets of individual data points.</p> <code>lambda: DataCollection(SparseData)()</code>"},{"location":"APIindex/#eitprocessing.datahandling.sequence.Sequence.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.sequence.Sequence.time","title":"time  <code>property</code>","text":"<pre><code>time: ndarray\n</code></pre> <p>Time axis from either EITData or ContinuousData.</p>"},{"location":"APIindex/#eitprocessing.datahandling.sequence.Sequence.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(start: int | None = None, end: int | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_index(\n    self,\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"De facto implementation of the `__getitem__` function.\n\n    This function can also be called directly to add a label to the sliced\n    object. Otherwise a default label describing the slice and original\n    object is attached.\n    \"\"\"\n    if start is None and end is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    start = start if start is not None else 0\n    end = end if end is not None else len(self)\n    newlabel = newlabel or self.label\n\n    return self._sliced_copy(start_index=start, end_index=end, newlabel=newlabel)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.sequence.Sequence.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.sequence.Sequence.concatenate","title":"concatenate  <code>classmethod</code>","text":"<pre><code>concatenate(a: Sequence, b: Sequence, newlabel: str | None = None) -&gt; Sequence\n</code></pre> <p>Create a merge of two Sequence objects.</p> Source code in <code>eitprocessing/datahandling/sequence.py</code> <pre><code>@classmethod  # TODO: why is this a class method? In other cases it's instance method\ndef concatenate(\n    cls,\n    a: Sequence,\n    b: Sequence,\n    newlabel: str | None = None,\n) -&gt; Sequence:\n    \"\"\"Create a merge of two Sequence objects.\"\"\"\n    # TODO: rewrite\n\n    concat_eit = a.eit_data.concatenate(b.eit_data)\n    concat_continuous = a.continuous_data.concatenate(b.continuous_data)\n    concat_sparse = a.sparse_data.concatenate(b.sparse_data)\n    concat_interval = a.interval_data.concatenate(b.interval_data)\n\n    newlabel = newlabel or a.label\n    # TODO: add concatenation of other attached objects\n\n    return a.__class__(\n        eit_data=concat_eit,\n        continuous_data=concat_continuous,\n        sparse_data=concat_sparse,\n        interval_data=concat_interval,\n        label=newlabel,\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.sequence.Sequence.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(start_time: float | None = None, end_time: float | None = None, start_inclusive: bool = True, end_inclusive: bool = False, label: str | None = None, name: str | None = None, description: str = '') -&gt; Self\n</code></pre> <p>Return a sliced version of the Sequence.</p> <p>See SelectByTime.select_by_time().</p> Source code in <code>eitprocessing/datahandling/sequence.py</code> <pre><code>def select_by_time(\n    self,\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = True,\n    end_inclusive: bool = False,\n    label: str | None = None,\n    name: str | None = None,\n    description: str = \"\",\n) -&gt; Self:\n    \"\"\"Return a sliced version of the Sequence.\n\n    See SelectByTime.select_by_time().\n    \"\"\"\n    if not label:\n        label = f\"copy_of_&lt;{self.label}&gt;\"\n    if not name:\n        f\"Sliced copy of &lt;{self.name}&gt;\"\n\n    return self.__class__(\n        label=label,\n        name=name,\n        description=description,\n        # perform select_by_time() on all four data types\n        **{\n            key: getattr(self, key).select_by_time(\n                start_time=start_time,\n                end_time=end_time,\n                start_inclusive=start_inclusive,\n                end_inclusive=end_inclusive,\n            )\n            for key in (\"eit_data\", \"continuous_data\", \"sparse_data\", \"interval_data\")\n        },\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.sparsedata","title":"sparsedata","text":""},{"location":"APIindex/#eitprocessing.datahandling.sparsedata.SparseData","title":"SparseData  <code>dataclass</code>","text":"<pre><code>SparseData(label: str, name: str, unit: str | None, category: str, time: ndarray, description: str = '', parameters: dict[str, Any] = dict(), derived_from: list[Any] = list(), values: Any | None = None)\n</code></pre> <p>               Bases: <code>DataContainer</code>, <code>SelectByTime</code></p> <p>Container for data related to individual time points.</p> <p>Sparse data is data for which the time points are not necessarily evenly spaced. Data can consist time-value pairs or only time points.</p> <p>Sparse data differs from interval data in that each data points is associated with a single time point rather than a time range.</p> <p>Examples are data points at end of inspiration/end of expiration (e.g. tidal volume, end-expiratoy lung impedance) or detected time points (e.g. QRS complexes).</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Computer readable name.</p> required <code>name</code> <code>str</code> <p>Human readable name.</p> required <code>unit</code> <code>str | None</code> <p>Unit of the data, if applicable.</p> required <code>category</code> <code>str</code> <p>Category the data falls into, e.g. 'detected r peak'.</p> required <code>description</code> <code>str</code> <p>Human readable extended description of the data.</p> <code>''</code> <code>parameters</code> <code>dict[str, Any]</code> <p>Parameters used to derive the data.</p> <code>dict()</code> <code>derived_from</code> <code>list[Any]</code> <p>Traceback of intermediates from which the current data was derived.</p> <code>list()</code> <code>values</code> <code>Any | None</code> <p>List or array of values. These van be numeric data, text or Python objects.</p> <code>None</code>"},{"location":"APIindex/#eitprocessing.datahandling.sparsedata.SparseData.t","title":"t  <code>property</code>","text":"<pre><code>t: TimeIndexer\n</code></pre> <p>Slicing an object using the time axis instead of indices.</p> <p>Example: <pre><code>&gt;&gt;&gt; sequence = load_eit_data(&lt;path&gt;, ...)\n&gt;&gt;&gt; time_slice1 = sequence.t[tp_start:tp_end]\n&gt;&gt;&gt; time_slice2 = sequence.select_by_time(tp_start, tp_end)\n&gt;&gt;&gt; time_slice1 == time_slice2\nTrue\n</code></pre></p>"},{"location":"APIindex/#eitprocessing.datahandling.sparsedata.SparseData.has_values","title":"has_values  <code>property</code>","text":"<pre><code>has_values: bool\n</code></pre> <p>True if the SparseData has values, False otherwise.</p>"},{"location":"APIindex/#eitprocessing.datahandling.sparsedata.SparseData.select_by_time","title":"select_by_time","text":"<pre><code>select_by_time(start_time: float | None = None, end_time: float | None = None, start_inclusive: bool = False, end_inclusive: bool = False, label: str | None = None) -&gt; Self\n</code></pre> <p>Get a shortened copy of the object, starting from start_time and ending at end_time.</p> <p>Given a start and end time stamp (i.e. its value, not its index), return a slice of the original object, which must contain a time axis.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float | None</code> <p>first time point to include. Defaults to first frame of sequence.</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>last time point. Defaults to last frame of sequence.</p> <code>None</code> <code>start_inclusive</code> <code>default</code> <p><code>True</code>), end_inclusive (default <code>False</code>): these arguments control the behavior if the given time stamp does not match exactly with an existing time stamp of the input. if <code>True</code>: the given time stamp will be inside the sliced object. if <code>False</code>: the given time stamp will be outside the sliced object.</p> <code>False</code> <code>label</code> <code>str | None</code> <p>Description. Defaults to None, which will create a label based on the original object label and the frames by which it is sliced.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if <code>self</code> does not contain a <code>time</code> attribute.</p> <code>ValueError</code> <p>if time stamps are not sorted.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A shortened copy of the object.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_time(  # noqa: D417\n    self,\n    start_time: float | None = None,\n    end_time: float | None = None,\n    start_inclusive: bool = False,\n    end_inclusive: bool = False,\n    label: str | None = None,\n) -&gt; Self:\n    \"\"\"Get a shortened copy of the object, starting from start_time and ending at end_time.\n\n    Given a start and end time stamp (i.e. its value, not its index),\n    return a slice of the original object, which must contain a time axis.\n\n    Args:\n        start_time: first time point to include. Defaults to first frame of sequence.\n        end_time: last time point. Defaults to last frame of sequence.\n        start_inclusive (default: `True`), end_inclusive (default `False`):\n            these arguments control the behavior if the given time stamp\n            does not match exactly with an existing time stamp of the input.\n            if `True`: the given time stamp will be inside the sliced object.\n            if `False`: the given time stamp will be outside the sliced object.\n        label: Description. Defaults to None, which will create a label based\n            on the original object label and the frames by which it is sliced.\n\n    Raises:\n        TypeError: if `self` does not contain a `time` attribute.\n        ValueError: if time stamps are not sorted.\n\n    Returns:\n        A shortened copy of the object.\n    \"\"\"\n    if not hasattr(self, \"time\"):\n        msg = f\"Object {self} has no time axis.\"\n        raise TypeError(msg)\n\n    if len(self.time) == 0:\n        # TODO: make proper new instances when not slicing\n        return copy.deepcopy(self)\n\n    if start_time is None and end_time is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    if not np.all(np.sort(self.time) == self.time):\n        msg = f\"Time stamps for {self} are not sorted and therefore data cannot be selected by time.\"\n        raise ValueError(msg)\n\n    if start_time is None or start_time &lt; self.time[0]:\n        start_index = 0\n    elif start_inclusive:\n        start_index = bisect.bisect_right(self.time, start_time) - 1\n    else:\n        start_index = bisect.bisect_left(self.time, start_time)\n\n    if end_time is None:\n        end_index = len(self.time)\n    elif end_inclusive:\n        end_index = bisect.bisect_left(self.time, end_time) + 1\n    else:\n        end_index = bisect.bisect_left(self.time, end_time)\n\n    return self.select_by_index(\n        start=start_index,\n        end=end_index,\n        newlabel=label,\n    )\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.sparsedata.SparseData.select_by_index","title":"select_by_index","text":"<pre><code>select_by_index(start: int | None = None, end: int | None = None, newlabel: str | None = None) -&gt; Self\n</code></pre> <p>De facto implementation of the <code>__getitem__</code> function.</p> <p>This function can also be called directly to add a label to the sliced object. Otherwise a default label describing the slice and original object is attached.</p> Source code in <code>eitprocessing/datahandling/mixins/slicing.py</code> <pre><code>def select_by_index(\n    self,\n    start: int | None = None,\n    end: int | None = None,\n    newlabel: str | None = None,\n) -&gt; Self:\n    \"\"\"De facto implementation of the `__getitem__` function.\n\n    This function can also be called directly to add a label to the sliced\n    object. Otherwise a default label describing the slice and original\n    object is attached.\n    \"\"\"\n    if start is None and end is None:\n        warnings.warn(\"No starting or end timepoint was selected.\")\n        return self\n\n    start = start if start is not None else 0\n    end = end if end is not None else len(self)\n    newlabel = newlabel or self.label\n\n    return self._sliced_copy(start_index=start, end_index=end, newlabel=newlabel)\n</code></pre>"},{"location":"APIindex/#eitprocessing.datahandling.sparsedata.SparseData.isequivalent","title":"isequivalent","text":"<pre><code>isequivalent(other: Self, raise_: bool = False) -&gt; bool\n</code></pre> <p>Test whether the data structure between two objects are equivalent.</p> <p>Equivalence, in this case means that objects are compatible e.g. to be merged. Data content can vary, but e.g. the category of data (e.g. airway pressure, flow, tidal volume) and unit, etc., must match.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>object that will be compared to self.</p> required <code>raise_</code> <code>bool</code> <p>sets this method's behavior in case of non-equivalence. If True, an <code>EquivalenceError</code> is raised, otherwise <code>False</code> is returned.</p> <code>False</code> <p>Raises:</p> Type Description <code>EquivalenceError</code> <p>if <code>raise_ == True</code> and the objects are not</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool describing result of equivalence comparison.</p> Source code in <code>eitprocessing/datahandling/mixins/equality.py</code> <pre><code>def isequivalent(self, other: Self, raise_: bool = False) -&gt; bool:  # noqa: C901, PLR0912\n    \"\"\"Test whether the data structure between two objects are equivalent.\n\n    Equivalence, in this case means that objects are compatible e.g. to be\n    merged. Data content can vary, but e.g. the category of data (e.g.\n    airway pressure, flow, tidal volume) and unit, etc., must match.\n\n    Args:\n        other: object that will be compared to self.\n        raise_: sets this method's behavior in case of non-equivalence. If\n            True, an `EquivalenceError` is raised, otherwise `False` is\n            returned.\n\n    Raises:\n        EquivalenceError: if `raise_ == True` and the objects are not\n        equivalent.\n\n    Returns:\n        bool describing result of equivalence comparison.\n    \"\"\"\n    if self == other:\n        return True\n\n    try:\n        # check whether types match\n        if type(self) is not type(other):\n            msg = f\"Types don't match: {type(self)}, {type(other)}\"\n            raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check keys in collection\n        # TODO: check whether this is still necessary for dicts #185\n        if isinstance(self, dict | UserDict):\n            if set(self.keys()) != set(other.keys()):\n                msg = f\"Keys don't match:\\n\\t{self.keys()},\\n\\t{other.keys()}\"\n                raise EquivalenceError(msg)  # noqa: TRY301\n\n            for key in self:\n                if not self[key].isequivalent(other[key], False):\n                    msg = f\"Data in {key} doesn't match: {self[key]}, {other[key]}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n        # check attributes of data\n        else:\n            if is_dataclass(self):\n                check_equivalence_fields = filter(\n                    lambda x: \"check_equivalence\" in x.metadata and x.metadata[\"check_equivalence\"],\n                    fields(self),\n                )\n                attrs = [field.name for field in check_equivalence_fields]\n\n            else:\n                self._check_equivalence: list[str]\n                attrs = self._check_equivalence\n\n            for attr in attrs:\n                if (s := getattr(self, attr)) != (o := getattr(other, attr)):\n                    msg = f\"Attribute {attr} doesn't match: {s}, {o}\"\n                    raise EquivalenceError(msg)  # noqa: TRY301\n\n    # raise or return if a check fails\n    except EquivalenceError:\n        if raise_:\n            raise\n        return False\n\n    # if all checks pass\n    return True\n</code></pre>"},{"location":"basic_example/","title":"Basic example","text":"<p>Under construction</p>"},{"location":"code_of_conduct_doc/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct_doc/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct_doc/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct_doc/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct_doc/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct_doc/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at d.bodor@esciencecenter.nl. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct_doc/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"contributing_doc/","title":"Contributing guidelines","text":"<p>We welcome any kind of contribution to our software, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation);</li> <li>you want to make a new release of the code base.</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"contributing_doc/#you-have-a-question","title":"You have a question","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"contributing_doc/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:<ul> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> </ul> </li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"contributing_doc/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<p>We welcome all contributions to this open-source project, as long as they follow our code of conduct.</p> <p>Please read out developers documentation if you are interested in contributing to the code base</p>"},{"location":"installation/","title":"Badges","text":"Badges Registry License Citation Fairness GitHub Actions Python"},{"location":"installation/#contents","title":"Contents","text":"<ul> <li>Installation</li> <li>Virtual environment</li> <li>Install using <code>pip</code></li> <li>Documentation</li> <li>Contributing</li> <li>Credits</li> </ul> <p>The project setup is documented in project_setup. Feel free to remove this document (and/or the link to this document) if you don't need it.</p>"},{"location":"installation/#installation","title":"Installation","text":"<p>Electrical Impedance Tomography (EIT) is a noninvasive and radiation-free continuous imaging tool for monitoring respiratory mechanics. eitprocessing aims to provide a versatile, user-friendly, reproducible and reliable workflow for the processing and analysis of EIT data and related waveform data, like pressures and flow.</p> <p>eitprocessing includes tools to load data exported from EIT-devices from several manufacturers, including Dr\u00e4ger, SenTec and Timpel, as well as data from other sources. Several pre-processing tools and analysis tools are provided.</p> <p>eit_dash provides an accompanying GUI.</p> <p>We welcome any contributions or suggestions</p>"},{"location":"installation/#installation_1","title":"Installation","text":""},{"location":"installation/#virtual-environment","title":"Virtual environment","text":"<p>It is advised to install eitprocessing in a dedicated virtual environment. See e.g. Install packages in a virtual environment using pip and venv or Getting started with conda.</p> <p>For conda (using 'eit-alive' as example environment name; you can choose your own): <pre><code>conda create -n eit-alive python=3.10\nconda activate eit-alive\n</code></pre></p>"},{"location":"installation/#install-using-pip","title":"Install using <code>pip</code>","text":"<p>eitprocessing can be installed from PyPi as follows:</p> <ul> <li>For basic use: <code>pip install eitprocessing</code></li> <li>For full developer options (testing, etc): </li> <li><code>git clone git@github.com:EIT-ALIVE/eitprocessing.git</code></li> <li><code>cd eitprocessing</code></li> <li><code>pip install -e \".[dev]\"</code></li> </ul>"},{"location":"installation/#documentation","title":"Documentation","text":"<p>Please see our usage documentation for a detailed explanation of the package.</p>"},{"location":"installation/#contributing","title":"Contributing","text":"<p>If you want to contribute to the development of eitprocessing, have a look at the contribution guidelines and the developer documentation.</p>"},{"location":"installation/#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the NLeSC/python-template.</p>"},{"location":"license_doc/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/</p> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li>Definitions.</li> </ol> <p>\"License\" shall mean the terms and conditions for use, reproduction,    and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by    the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all    other entities that control, are controlled by, or are under common    control with that entity. For the purposes of this definition,    \"control\" means (i) the power, direct or indirect, to cause the    direction or management of such entity, whether by contract or    otherwise, or (ii) ownership of fifty percent (50%) or more of the    outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity    exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,    including but not limited to software source code, documentation    source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical    transformation or translation of a Source form, including but    not limited to compiled object code, generated documentation,    and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or    Object form, made available under the License, as indicated by a    copyright notice that is included in or attached to the work    (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object    form, that is based on (or derived from) the Work and for which the    editorial revisions, annotations, elaborations, or other modifications    represent, as a whole, an original work of authorship. For the purposes    of this License, Derivative Works shall not include works that remain    separable from, or merely link (or bind by name) to the interfaces of,    the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including    the original version of the Work and any modifications or additions    to that Work or Derivative Works thereof, that is intentionally    submitted to Licensor for inclusion in the Work by the copyright owner    or by an individual or Legal Entity authorized to submit on behalf of    the copyright owner. For the purposes of this definition, \"submitted\"    means any form of electronic, verbal, or written communication sent    to the Licensor or its representatives, including but not limited to    communication on electronic mailing lists, source code control systems,    and issue tracking systems that are managed by, or on behalf of, the    Licensor for the purpose of discussing and improving the Work, but    excluding communication that is conspicuously marked or otherwise    designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity    on behalf of whom a Contribution has been received by Licensor and    subsequently incorporated within the Work.</p> <ol> <li> <p>Grant of Copyright License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    copyright license to reproduce, prepare Derivative Works of,    publicly display, publicly perform, sublicense, and distribute the    Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    (except as stated in this section) patent license to make, have made,    use, offer to sell, sell, import, and otherwise transfer the Work,    where such license applies only to those patent claims licensable    by such Contributor that are necessarily infringed by their    Contribution(s) alone or by combination of their Contribution(s)    with the Work to which such Contribution(s) was submitted. If You    institute patent litigation against any entity (including a    cross-claim or counterclaim in a lawsuit) alleging that the Work    or a Contribution incorporated within the Work constitutes direct    or contributory patent infringement, then any patent licenses    granted to You under this License for that Work shall terminate    as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the    Work or Derivative Works thereof in any medium, with or without    modifications, and in Source or Object form, provided that You    meet the following conditions:</p> </li> </ol> <p>(a) You must give any other recipients of the Work or          Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices          stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works          that You distribute, all copyright, patent, trademark, and          attribution notices from the Source form of the Work,          excluding those notices that do not pertain to any part of          the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its          distribution, then any Derivative Works that You distribute must          include a readable copy of the attribution notices contained          within such NOTICE file, excluding those notices that do not          pertain to any part of the Derivative Works, in at least one          of the following places: within a NOTICE text file distributed          as part of the Derivative Works; within the Source form or          documentation, if provided along with the Derivative Works; or,          within a display generated by the Derivative Works, if and          wherever such third-party notices normally appear. The contents          of the NOTICE file are for informational purposes only and          do not modify the License. You may add Your own attribution          notices within Derivative Works that You distribute, alongside          or as an addendum to the NOTICE text from the Work, provided          that such additional attribution notices cannot be construed          as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and    may provide additional or different license terms and conditions    for use, reproduction, or distribution of Your modifications, or    for any such Derivative Works as a whole, provided Your use,    reproduction, and distribution of the Work otherwise complies with    the conditions stated in this License.</p> <ol> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,    any Contribution intentionally submitted for inclusion in the Work    by You to the Licensor shall be under the terms and conditions of    this License, without any additional terms or conditions.    Notwithstanding the above, nothing herein shall supersede or modify    the terms of any separate license agreement you may have executed    with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade    names, trademarks, service marks, or product names of the Licensor,    except as required for reasonable and customary use in describing the    origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or    agreed to in writing, Licensor provides the Work (and each    Contributor provides its Contributions) on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied, including, without limitation, any warranties or conditions    of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A    PARTICULAR PURPOSE. You are solely responsible for determining the    appropriateness of using or redistributing the Work and assume any    risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,    whether in tort (including negligence), contract, or otherwise,    unless required by applicable law (such as deliberate and grossly    negligent acts) or agreed to in writing, shall any Contributor be    liable to You for damages, including any direct, indirect, special,    incidental, or consequential damages of any character arising as a    result of this License or out of the use or inability to use the    Work (including but not limited to damages for loss of goodwill,    work stoppage, computer failure or malfunction, or any and all    other commercial damages or losses), even if such Contributor    has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing    the Work or Derivative Works thereof, You may choose to offer,    and charge a fee for, acceptance of support, warranty, indemnity,    or other liability obligations and/or rights consistent with this    License. However, in accepting such obligations, You may act only    on Your own behalf and on Your sole responsibility, not on behalf    of any other Contributor, and only if You agree to indemnify,    defend, and hold each Contributor harmless for any liability    incurred by, or claims asserted against, such Contributor by reason    of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!)  The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright yyy</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"team/","title":"Our team","text":""},{"location":"team/#our-team","title":"Our team:","text":""},{"location":"team/#escience-center","title":"eScience Center","text":"<ul> <li>Dani Bodor, Lead RSE</li> <li>Walter Baccinelli, RSE</li> <li>Pablo Lopez-Tarifa, Programme Manager </li> </ul>"},{"location":"team/#erasmus-medical-center","title":"Erasmus Medical Center","text":"<ul> <li>Annemijn H. Jonkman, Lead Applicant</li> <li>Peter Somhorst </li> <li>Juliette Francovich </li> <li>Jantine Wisse </li> </ul>"}]}